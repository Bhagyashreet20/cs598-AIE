STARTTIME Tue Dec 3 23:28:56 CST 2024
Lmod Warning:
-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc-runtime/8.5.0
(required by: python/3.10.13)
-------------------------------------------------------------------------------




The following have been reloaded with a version change:
  1) cuda/11.8.0 => cuda/12.4.0     2) gcc-runtime/8.5.0 => gcc-runtime/11.4.0

=== Environment Settings ===
MASTER_ADDR: 172.28.23.110
MASTER_PORT: 12355
NODE_ROLE: master
RANK: 1
==========================
=== Testing connectivity to 172.28.23.110 ===
Current host info:
gpub010.delta.ncsa.illinois.edu
172.28.23.110 172.28.86.110 141.142.254.110 
âœ“ Can reach 172.28.23.110
local rank: 0 172.28.23.110 12355, init distributed
Initialized rank 0 of 2 on cuda:0
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Map:   0%|          | 0/4 [00:00<?, ? examples/s]/projects/bdof/leatherman/leatherman_env_llama3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.06 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.20 examples/s]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-12-03 23:29:58,643] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /u/leatherman/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jl180. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /projects/bdof/leatherman/wandb/run-20241203_233007-6s6y304e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /projects/bdof/leatherman/t5_checkpoints
wandb: â­ï¸ View project at https://wandb.ai/jl180/huggingface
wandb: ðŸš€ View run at https://wandb.ai/jl180/huggingface/runs/6s6y304e
  0%|          | 0/2 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
[rank0]:[W1203 23:30:14.288072427 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.04s/it]device: 0, Time: 2024-12-03 23:30:16.801, naybelogsaveevaluate start
2024-12-03 23:30:16.801 rank: 0,start receiving
2024-12-03 23:30:17.302 Rank 0 done receiving-------------------------
device: 0, Time: 2024-12-03 23:30:17.305, naybelogsaveevaluate end
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.15s/it]device: 0, Time: 2024-12-03 23:30:17.527, naybelogsaveevaluate start
2024-12-03 23:30:17.528 rank: 0,start receiving
2024-12-03 23:30:17.852 Rank 0 done receiving-------------------------
device: 0, Time: 2024-12-03 23:30:17, _save_checkpoint count: 0, _save_checkpoint start
device: 0, Time: 2024-12-03 23:30:18, _save_checkpoint count: 0, _save_checkpoint end
device: 0, Time: 2024-12-03 23:30:18.450, naybelogsaveevaluate end
                                             {'train_runtime': 11.6104, 'train_samples_per_second': 0.689, 'train_steps_per_second': 0.172, 'train_loss': 15.463075637817383, 'epoch': 2.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.15s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.34s/it]
training finished
[1;34mwandb[0m: ðŸš€ View run [33m/projects/bdof/leatherman/t5_checkpoints[0m at: [34mhttps://wandb.ai/jl180/huggingface/runs/6s6y304e[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241203_233007-6s6y304e/logs[0m
[rank0]:[W1203 23:30:19.542636840 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ENDTIME Tue Dec 3 23:30:21 CST 2024
