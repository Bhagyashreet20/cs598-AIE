STARTTIME Tue Dec 3 23:29:13 CST 2024
Lmod Warning:
-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc-runtime/8.5.0
(required by: python/3.10.13)
-------------------------------------------------------------------------------




The following have been reloaded with a version change:
  1) cuda/11.8.0 => cuda/12.4.0     2) gcc-runtime/8.5.0 => gcc-runtime/11.4.0

=== Environment Settings ===
MASTER_ADDR: 172.28.23.110
MASTER_PORT: 12355
NODE_ROLE: worker
RANK: 1
==========================
=== Testing connectivity to 172.28.23.110 ===
Current host info:
gpua045.delta.ncsa.illinois.edu
172.28.23.45 172.28.86.45 141.142.254.45 
✓ Can reach 172.28.23.110
local rank: 0 172.28.23.110 12355, init distributed
Initialized rank 1 of 2 on cuda:0
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Map:   0%|          | 0/4 [00:00<?, ? examples/s]/projects/bdof/leatherman/leatherman_env_llama3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|██████████| 4/4 [00:00<00:00, 11.05 examples/s]Map: 100%|██████████| 4/4 [00:00<00:00, 10.19 examples/s]
[2024-12-03 23:29:58,643] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /u/leatherman/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
[rank1]:[W1203 23:30:14.320626758 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
device: 1, Time: 2024-12-03 23:30:16.800, naybelogsaveevaluate start
2024-12-03 23:30:16.802 Rank 1: start sending!! model_state_dict: type <class 'dict'>, length: 134
2024-12-03 23:30:17.213 Rank 1 done sending-------------------------
device: 1, Time: 2024-12-03 23:30:17.213, naybelogsaveevaluate end
device: 1, Time: 2024-12-03 23:30:17.527, naybelogsaveevaluate start
2024-12-03 23:30:17.528 Rank 1: start sending!! model_state_dict: type <class 'dict'>, length: 134
2024-12-03 23:30:17.764 Rank 1 done sending-------------------------
device: 1, Time: 2024-12-03 23:30:17.764, naybelogsaveevaluate end
training finished
[rank1]:[W1203 23:30:18.483546388 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ENDTIME Tue Dec 3 23:30:21 CST 2024
