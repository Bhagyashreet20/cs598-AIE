Tue Dec  3 17:27:38 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:46:00.0 Off |                    0 |
| N/A   29C    P0             55W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:85:00.0 Off |                    0 |
| N/A   29C    P0             53W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:C7:00.0 Off |                    0 |
| N/A   29C    P0             54W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[2024-12-03 17:28:42,109] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-03 17:29:04,703] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2: setting --include=localhost:0,1,2
[2024-12-03 17:29:04,704] [INFO] [runner.py:607:main] cmd = /projects/bdof/code/cs598-AIE/myenv/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train-deepspeed.py
[2024-12-03 17:29:07,428] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-03 17:29:12,174] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2]}
[2024-12-03 17:29:12,174] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=3, node_rank=0
[2024-12-03 17:29:12,174] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2]})
[2024-12-03 17:29:12,174] [INFO] [launch.py:164:main] dist_world_size=3
[2024-12-03 17:29:12,174] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2
[2024-12-03 17:29:12,175] [INFO] [launch.py:256:main] process 3197434 spawned with command: ['/projects/bdof/code/cs598-AIE/myenv/bin/python3', '-u', 'train-deepspeed.py', '--local_rank=0']
[2024-12-03 17:29:12,176] [INFO] [launch.py:256:main] process 3197435 spawned with command: ['/projects/bdof/code/cs598-AIE/myenv/bin/python3', '-u', 'train-deepspeed.py', '--local_rank=1']
[2024-12-03 17:29:12,177] [INFO] [launch.py:256:main] process 3197436 spawned with command: ['/projects/bdof/code/cs598-AIE/myenv/bin/python3', '-u', 'train-deepspeed.py', '--local_rank=2']
[2024-12-03 17:29:43,897] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-03 17:29:43,936] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-03 17:29:43,937] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-03 17:30:04,190] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-12-03 17:30:04,190] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-12-03 17:30:04,296] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-12-03 17:30:04,411] [INFO] [comm.py:652:init_distributed] cdb=None
Start of inner train loop, device rank: 2, time: 2024-12-03 17:30:05
Start of inner train loop, device rank: 1, time: 2024-12-03 17:30:05
Start of inner train loop, device rank: 0, time: 2024-12-03 17:30:06
[2024-12-03 17:30:06,513] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown
[2024-12-03 17:30:06,513] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 3
[2024-12-03 17:30:44,559] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-12-03 17:30:44,561] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-12-03 17:30:44,561] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-12-03 17:30:44,571] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-12-03 17:30:44,571] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-12-03 17:30:44,571] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-12-03 17:30:44,571] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2024-12-03 17:30:44,571] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 1000000000
[2024-12-03 17:30:44,571] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-12-03 17:30:44,571] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-12-03 17:30:54,457] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-12-03 17:30:54,458] [INFO] [utils.py:782:see_memory_usage] MA 9.98 GB         Max_MA 9.98 GB         CA 9.98 GB         Max_CA 10 GB 
[2024-12-03 17:30:54,458] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.13 GB, percent = 16.7%
[2024-12-03 17:30:54,698] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-12-03 17:30:54,699] [INFO] [utils.py:782:see_memory_usage] MA 9.98 GB         Max_MA 13.97 GB         CA 13.97 GB         Max_CA 14 GB 
[2024-12-03 17:30:54,699] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.05 GB, percent = 14.3%
[2024-12-03 17:30:54,699] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized
[2024-12-03 17:30:54,833] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-12-03 17:30:54,834] [INFO] [utils.py:782:see_memory_usage] MA 9.98 GB         Max_MA 9.98 GB         CA 13.97 GB         Max_CA 14 GB 
[2024-12-03 17:30:54,834] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.05 GB, percent = 14.3%
[2024-12-03 17:30:54,836] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-12-03 17:30:54,836] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2024-12-03 17:30:54,836] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-12-03 17:30:54,836] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-12-03 17:30:54,837] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": true, 
    "contiguous_memory_optimization": true, 
    "cpu_checkpointing": true, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  True
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-12-03 17:30:54,838] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9a82317be0>
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-12-03 17:30:54,839] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   fp16_enabled ................. True
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   loss_scale ................... 0
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-12-03 17:30:54,840] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   train_batch_size ............. 24
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  8
[2024-12-03 17:30:54,841] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   use_node_local_storage ....... True
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... True
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   world_size ................... 3
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-12-03 17:30:54,842] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2
[2024-12-03 17:30:54,842] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 24, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "reduce_scatter": true, 
        "allgather_bucket_size": 1.000000e+09, 
        "overlap_comm": true, 
        "contiguous_gradients": true
    }, 
    "activation_checkpointing": {
        "partition_activations": true, 
        "contiguous_memory_optimization": true, 
        "cpu_checkpointing": true
    }, 
    "checkpoint": {
        "tag_validation": "Warn", 
        "use_node_local_storage": true, 
        "parallel_write": {
            "pipeline_stage": true
        }
    }, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": true, 
    "memory_optimization": {
        "enable_memory_logging": true
    }, 
    "bf16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Training started.
[2024-12-03 17:31:11,853] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-12-03 17:31:11,854] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 10696.20 | bwd_microstep: 3923.71 | bwd_inner_microstep: 3904.36 | bwd_allreduce_microstep: 19.22 | step_microstep: 1113.16
[2024-12-03 17:31:11,854] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 10696.21 | bwd: 3923.68 | bwd_inner: 3904.34 | bwd_allreduce: 19.24 | step: 1113.16
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:31:11
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:31:11
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:31:11
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:31:31,834] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1 is about to be saved!
[2024-12-03 17:31:36,067] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-1/global_step1/mp_rank_00_model_states.pt
[2024-12-03 17:31:36,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-1/global_step1/mp_rank_00_model_states.pt...
[2024-12-03 17:31:54,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-1/global_step1/mp_rank_00_model_states.pt.
[2024-12-03 17:31:54,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-1/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:32:01,368] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-1/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:32:01,372] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-1/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:32:01,372] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 17:32:01
End of save checkpoint, device rank: 2, time: 2024-12-03 17:32:01
End of save checkpoint, device rank: 0, time: 2024-12-03 17:32:32
[2024-12-03 17:32:33,632] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-12-03 17:32:33,632] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.51 | bwd_microstep: 581.95 | bwd_inner_microstep: 562.94 | bwd_allreduce_microstep: 18.95 | step_microstep: 34.14
[2024-12-03 17:32:33,633] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.49 | bwd: 581.96 | bwd_inner: 562.95 | bwd_allreduce: 18.97 | step: 34.15
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:32:33
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:32:33
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:32:33
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:32:47,822] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2 is about to be saved!
[2024-12-03 17:32:49,902] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-2/global_step2/mp_rank_00_model_states.pt
[2024-12-03 17:32:49,903] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-2/global_step2/mp_rank_00_model_states.pt...
[2024-12-03 17:33:03,803] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-2/global_step2/mp_rank_00_model_states.pt.
[2024-12-03 17:33:03,806] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-2/global_step2/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:33:11,379] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-2/global_step2/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:33:11,383] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-2/global_step2/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:33:11,384] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 17:33:11
End of save checkpoint, device rank: 1, time: 2024-12-03 17:33:11
End of save checkpoint, device rank: 0, time: 2024-12-03 17:33:15
[2024-12-03 17:33:16,942] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-12-03 17:33:16,943] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 203.68 | bwd_microstep: 580.11 | bwd_inner_microstep: 561.06 | bwd_allreduce_microstep: 18.99 | step_microstep: 34.59
[2024-12-03 17:33:16,943] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 203.66 | bwd: 580.12 | bwd_inner: 561.06 | bwd_allreduce: 19.01 | step: 34.59
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:33:16
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:33:16
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:33:16
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:33:32,022] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3 is about to be saved!
[2024-12-03 17:33:32,751] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-3/global_step3/mp_rank_00_model_states.pt
[2024-12-03 17:33:32,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-3/global_step3/mp_rank_00_model_states.pt...
[2024-12-03 17:33:43,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-3/global_step3/mp_rank_00_model_states.pt.
[2024-12-03 17:33:43,114] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:33:50,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:33:50,498] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:33:50,502] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 17:33:50End of save checkpoint, device rank: 1, time: 2024-12-03 17:33:50

End of save checkpoint, device rank: 0, time: 2024-12-03 17:33:53
[2024-12-03 17:33:54,615] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-12-03 17:33:54,616] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.30 | bwd_microstep: 579.08 | bwd_inner_microstep: 559.86 | bwd_allreduce_microstep: 19.16 | step_microstep: 58.09
[2024-12-03 17:33:54,616] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.27 | bwd: 579.09 | bwd_inner: 559.86 | bwd_allreduce: 19.18 | step: 58.08
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:33:54
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:33:54
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:33:54
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:34:07,766] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4 is about to be saved!
[2024-12-03 17:34:07,777] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-4/global_step4/mp_rank_00_model_states.pt
[2024-12-03 17:34:07,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-4/global_step4/mp_rank_00_model_states.pt...
[2024-12-03 17:34:24,137] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-4/global_step4/mp_rank_00_model_states.pt.
[2024-12-03 17:34:24,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-4/global_step4/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:34:31,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-4/global_step4/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:34:31,690] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-4/global_step4/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:34:31,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 17:34:31
End of save checkpoint, device rank: 1, time: 2024-12-03 17:34:31
End of save checkpoint, device rank: 0, time: 2024-12-03 17:34:35
[2024-12-03 17:34:36,813] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-12-03 17:34:36,814] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.27 | bwd_microstep: 581.97 | bwd_inner_microstep: 562.94 | bwd_allreduce_microstep: 18.97 | step_microstep: 33.34
[2024-12-03 17:34:36,814] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.25 | bwd: 581.98 | bwd_inner: 562.94 | bwd_allreduce: 18.99 | step: 33.34
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:34:36
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:34:36
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:34:36
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:34:48,338] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5 is about to be saved!
[2024-12-03 17:34:50,722] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-5/global_step5/mp_rank_00_model_states.pt
[2024-12-03 17:34:50,722] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-5/global_step5/mp_rank_00_model_states.pt...
[2024-12-03 17:35:01,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-5/global_step5/mp_rank_00_model_states.pt.
[2024-12-03 17:35:01,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:35:08,843] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:35:08,848] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:35:08,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 17:35:09End of save checkpoint, device rank: 2, time: 2024-12-03 17:35:09

End of save checkpoint, device rank: 0, time: 2024-12-03 17:35:12
[2024-12-03 17:35:13,923] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
[2024-12-03 17:35:13,923] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.07 | bwd_microstep: 579.36 | bwd_inner_microstep: 560.26 | bwd_allreduce_microstep: 19.04 | step_microstep: 34.54
[2024-12-03 17:35:13,923] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.05 | bwd: 579.38 | bwd_inner: 560.26 | bwd_allreduce: 19.06 | step: 34.55
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:35:13Start of save checkpoint, device rank: 2, time: 2024-12-03 17:35:13

Start of save checkpoint, device rank: 0, time: 2024-12-03 17:35:13
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:35:26,677] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6 is about to be saved!
[2024-12-03 17:35:26,687] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-6/global_step6/mp_rank_00_model_states.pt
[2024-12-03 17:35:26,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-6/global_step6/mp_rank_00_model_states.pt...
[2024-12-03 17:35:37,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-6/global_step6/mp_rank_00_model_states.pt.
[2024-12-03 17:35:37,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:35:45,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:35:45,377] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:35:45,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 17:35:45End of save checkpoint, device rank: 2, time: 2024-12-03 17:35:45

End of save checkpoint, device rank: 0, time: 2024-12-03 17:35:49
[2024-12-03 17:35:50,542] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-12-03 17:35:50,543] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 204.59 | bwd_microstep: 578.76 | bwd_inner_microstep: 559.59 | bwd_allreduce_microstep: 19.11 | step_microstep: 39.04
[2024-12-03 17:35:50,543] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 204.57 | bwd: 578.78 | bwd_inner: 559.59 | bwd_allreduce: 19.14 | step: 39.04
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:35:50
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:35:50
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:35:50
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:36:05,063] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7 is about to be saved!
[2024-12-03 17:36:05,075] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-7/global_step7/mp_rank_00_model_states.pt
[2024-12-03 17:36:05,075] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-7/global_step7/mp_rank_00_model_states.pt...
[2024-12-03 17:36:22,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-7/global_step7/mp_rank_00_model_states.pt.
[2024-12-03 17:36:22,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-7/global_step7/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:36:29,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-7/global_step7/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:36:29,515] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-7/global_step7/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:36:29,515] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:36:29
End of save checkpoint, device rank: 1, time: 2024-12-03 17:36:29
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:36:32
[2024-12-03 17:36:33,921] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-12-03 17:36:33,922] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.61 | bwd_microstep: 579.57 | bwd_inner_microstep: 560.53 | bwd_allreduce_microstep: 18.98 | step_microstep: 33.28
[2024-12-03 17:36:33,922] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.59 | bwd: 579.58 | bwd_inner: 560.53 | bwd_allreduce: 18.99 | step: 33.29
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:36:33
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:36:33
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:36:33
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:36:48,183] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8 is about to be saved!
[2024-12-03 17:36:48,192] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-8/global_step8/mp_rank_00_model_states.pt
[2024-12-03 17:36:48,192] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-8/global_step8/mp_rank_00_model_states.pt...
[2024-12-03 17:36:58,514] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-8/global_step8/mp_rank_00_model_states.pt.
[2024-12-03 17:36:58,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-8/global_step8/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:37:05,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-8/global_step8/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:37:05,412] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-8/global_step8/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:37:05,412] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:37:05
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 17:37:05
End of save checkpoint, device rank: 0, time: 2024-12-03 17:37:09
[2024-12-03 17:37:10,753] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
[2024-12-03 17:37:10,754] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.22 | bwd_microstep: 593.16 | bwd_inner_microstep: 573.85 | bwd_allreduce_microstep: 19.25 | step_microstep: 33.03
[2024-12-03 17:37:10,754] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.20 | bwd: 593.17 | bwd_inner: 573.85 | bwd_allreduce: 19.27 | step: 33.03
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:37:10
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:37:10
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:37:10
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:37:23,778] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9 is about to be saved!
[2024-12-03 17:37:25,084] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-9/global_step9/mp_rank_00_model_states.pt
[2024-12-03 17:37:25,084] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-9/global_step9/mp_rank_00_model_states.pt...
[2024-12-03 17:37:35,755] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-9/global_step9/mp_rank_00_model_states.pt.
[2024-12-03 17:37:35,757] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:37:42,649] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:37:42,659] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:37:42,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:37:42
End of save checkpoint, device rank: 1, time: 2024-12-03 17:37:42
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:37:45
[2024-12-03 17:37:47,756] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
[2024-12-03 17:37:47,757] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 785.55 | bwd_microstep: 645.52 | bwd_inner_microstep: 626.37 | bwd_allreduce_microstep: 19.09 | step_microstep: 33.23
[2024-12-03 17:37:47,757] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 785.53 | bwd: 645.54 | bwd_inner: 626.37 | bwd_allreduce: 19.11 | step: 33.24
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:37:47
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:37:47
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:37:47
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:37:58,826] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10 is about to be saved!
[2024-12-03 17:37:58,837] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-10/global_step10/mp_rank_00_model_states.pt
[2024-12-03 17:37:58,837] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-10/global_step10/mp_rank_00_model_states.pt...
[2024-12-03 17:38:09,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-10/global_step10/mp_rank_00_model_states.pt.
[2024-12-03 17:38:09,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:38:16,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:38:16,038] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:38:16,038] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:38:16
End of save checkpoint, device rank: 1, time: 2024-12-03 17:38:16
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:38:19
[2024-12-03 17:38:20,793] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
[2024-12-03 17:38:20,794] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.78 | bwd_microstep: 581.94 | bwd_inner_microstep: 563.88 | bwd_allreduce_microstep: 18.00 | step_microstep: 33.30
[2024-12-03 17:38:20,794] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.76 | bwd: 581.95 | bwd_inner: 563.88 | bwd_allreduce: 18.02 | step: 33.31
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:38:20
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:38:20
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:38:20
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:38:31,781] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11 is about to be saved!
[2024-12-03 17:38:31,794] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-11/global_step11/mp_rank_00_model_states.pt
[2024-12-03 17:38:31,794] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-11/global_step11/mp_rank_00_model_states.pt...
[2024-12-03 17:38:41,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-11/global_step11/mp_rank_00_model_states.pt.
[2024-12-03 17:38:41,968] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-11/global_step11/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:38:49,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-11/global_step11/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:38:49,468] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-11/global_step11/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:38:49,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:38:49
End of save checkpoint, device rank: 2, time: 2024-12-03 17:38:49
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:38:53
[2024-12-03 17:38:54,293] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
[2024-12-03 17:38:54,293] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.79 | bwd_microstep: 583.60 | bwd_inner_microstep: 564.51 | bwd_allreduce_microstep: 19.03 | step_microstep: 33.13
[2024-12-03 17:38:54,294] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.77 | bwd: 583.61 | bwd_inner: 564.51 | bwd_allreduce: 19.05 | step: 33.13
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:38:54
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:38:54
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:38:54
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:39:08,226] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12 is about to be saved!
[2024-12-03 17:39:13,259] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-12/global_step12/mp_rank_00_model_states.pt
[2024-12-03 17:39:13,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-12/global_step12/mp_rank_00_model_states.pt...
[2024-12-03 17:39:23,612] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-12/global_step12/mp_rank_00_model_states.pt.
[2024-12-03 17:39:23,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:39:30,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:39:30,602] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:39:30,602] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:39:30
End of save checkpoint, device rank: 1, time: 2024-12-03 17:39:30
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:39:33
[2024-12-03 17:39:34,741] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
[2024-12-03 17:39:34,742] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 211.43 | bwd_microstep: 578.02 | bwd_inner_microstep: 558.91 | bwd_allreduce_microstep: 19.06 | step_microstep: 33.01
[2024-12-03 17:39:34,742] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 211.41 | bwd: 578.04 | bwd_inner: 558.91 | bwd_allreduce: 19.08 | step: 33.01
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:39:34Start of save checkpoint, device rank: 2, time: 2024-12-03 17:39:34

Start of save checkpoint, device rank: 0, time: 2024-12-03 17:39:34
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:39:46,042] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13 is about to be saved!
[2024-12-03 17:39:46,054] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-13/global_step13/mp_rank_00_model_states.pt
[2024-12-03 17:39:46,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-13/global_step13/mp_rank_00_model_states.pt...
[2024-12-03 17:39:56,545] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-13/global_step13/mp_rank_00_model_states.pt.
[2024-12-03 17:39:56,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-13/global_step13/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:40:03,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-13/global_step13/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:40:03,602] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-13/global_step13/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:40:03,602] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:40:03
End of save checkpoint, device rank: 1, time: 2024-12-03 17:40:03
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:40:07
[2024-12-03 17:40:08,849] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
[2024-12-03 17:40:08,850] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.14 | bwd_microstep: 583.40 | bwd_inner_microstep: 565.25 | bwd_allreduce_microstep: 18.10 | step_microstep: 33.25
[2024-12-03 17:40:08,850] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.12 | bwd: 583.41 | bwd_inner: 565.25 | bwd_allreduce: 18.12 | step: 33.25
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:40:08
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:40:08
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:40:08
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:40:23,476] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14 is about to be saved!
[2024-12-03 17:40:25,349] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-14/global_step14/mp_rank_00_model_states.pt
[2024-12-03 17:40:25,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-14/global_step14/mp_rank_00_model_states.pt...
[2024-12-03 17:40:36,090] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-14/global_step14/mp_rank_00_model_states.pt.
[2024-12-03 17:40:36,092] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-14/global_step14/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:40:42,692] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-14/global_step14/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:40:42,696] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-14/global_step14/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:40:42,696] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:40:42
End of save checkpoint, device rank: 2, time: 2024-12-03 17:40:42
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:40:47
[2024-12-03 17:40:48,848] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
[2024-12-03 17:40:48,849] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.20 | bwd_microstep: 581.35 | bwd_inner_microstep: 562.20 | bwd_allreduce_microstep: 19.10 | step_microstep: 33.28
[2024-12-03 17:40:48,849] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.18 | bwd: 581.37 | bwd_inner: 562.20 | bwd_allreduce: 19.12 | step: 33.28
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:40:48
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:40:48
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:40:48
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:40:59,134] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15 is about to be saved!
[2024-12-03 17:41:07,502] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-15/global_step15/mp_rank_00_model_states.pt
[2024-12-03 17:41:07,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-15/global_step15/mp_rank_00_model_states.pt...
[2024-12-03 17:41:18,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-15/global_step15/mp_rank_00_model_states.pt.
[2024-12-03 17:41:18,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:41:25,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:41:25,015] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:41:25,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:41:25
End of save checkpoint, device rank: 1, time: 2024-12-03 17:41:25
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:41:27
[2024-12-03 17:41:29,154] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536
[2024-12-03 17:41:29,154] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.28 | bwd_microstep: 560.89 | bwd_inner_microstep: 541.77 | bwd_allreduce_microstep: 19.06 | step_microstep: 33.09
[2024-12-03 17:41:29,155] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.26 | bwd: 560.90 | bwd_inner: 541.77 | bwd_allreduce: 19.08 | step: 33.09
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:41:29
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:41:29
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:41:29
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:41:40,724] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16 is about to be saved!
[2024-12-03 17:41:40,733] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-16/global_step16/mp_rank_00_model_states.pt
[2024-12-03 17:41:40,734] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-16/global_step16/mp_rank_00_model_states.pt...
[2024-12-03 17:41:51,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-16/global_step16/mp_rank_00_model_states.pt.
[2024-12-03 17:41:51,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-16/global_step16/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:41:57,886] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-16/global_step16/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:41:57,889] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-16/global_step16/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:41:57,890] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 17:41:57End of save checkpoint, device rank: 1, time: 2024-12-03 17:41:57

End of save checkpoint, device rank: 0, time: 2024-12-03 17:42:01
[2024-12-03 17:42:03,297] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
[2024-12-03 17:42:03,297] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 215.42 | bwd_microstep: 585.17 | bwd_inner_microstep: 565.98 | bwd_allreduce_microstep: 19.13 | step_microstep: 37.41
[2024-12-03 17:42:03,298] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 215.41 | bwd: 585.19 | bwd_inner: 565.98 | bwd_allreduce: 19.15 | step: 37.42
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:42:03Start of save checkpoint, device rank: 2, time: 2024-12-03 17:42:03

Start of save checkpoint, device rank: 0, time: 2024-12-03 17:42:03
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:42:14,056] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17 is about to be saved!
[2024-12-03 17:42:19,052] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-17/global_step17/mp_rank_00_model_states.pt
[2024-12-03 17:42:19,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-17/global_step17/mp_rank_00_model_states.pt...
[2024-12-03 17:42:29,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-17/global_step17/mp_rank_00_model_states.pt.
[2024-12-03 17:42:29,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-17/global_step17/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:42:36,301] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-17/global_step17/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:42:36,304] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-17/global_step17/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:42:36,304] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:42:36
End of save checkpoint, device rank: 1, time: 2024-12-03 17:42:36
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:42:40
[2024-12-03 17:42:41,847] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
[2024-12-03 17:42:41,848] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 215.66 | bwd_microstep: 585.70 | bwd_inner_microstep: 567.53 | bwd_allreduce_microstep: 18.12 | step_microstep: 33.02
[2024-12-03 17:42:41,848] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 215.64 | bwd: 585.71 | bwd_inner: 567.53 | bwd_allreduce: 18.14 | step: 33.02
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:42:41Start of save checkpoint, device rank: 1, time: 2024-12-03 17:42:41

Start of save checkpoint, device rank: 0, time: 2024-12-03 17:42:41
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:42:52,846] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18 is about to be saved!
[2024-12-03 17:42:52,859] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-18/global_step18/mp_rank_00_model_states.pt
[2024-12-03 17:42:52,859] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-18/global_step18/mp_rank_00_model_states.pt...
[2024-12-03 17:43:03,199] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-18/global_step18/mp_rank_00_model_states.pt.
[2024-12-03 17:43:03,201] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:43:10,627] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:43:10,631] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:43:10,631] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:43:10
End of save checkpoint, device rank: 2, time: 2024-12-03 17:43:10
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:43:14
[2024-12-03 17:43:15,503] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
[2024-12-03 17:43:15,503] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.36 | bwd_microstep: 588.72 | bwd_inner_microstep: 569.66 | bwd_allreduce_microstep: 19.01 | step_microstep: 33.08
[2024-12-03 17:43:15,504] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.34 | bwd: 588.73 | bwd_inner: 569.66 | bwd_allreduce: 19.02 | step: 33.08
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:43:15
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:43:15
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:43:15
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:43:26,361] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19 is about to be saved!
[2024-12-03 17:43:27,203] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-19/global_step19/mp_rank_00_model_states.pt
[2024-12-03 17:43:27,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-19/global_step19/mp_rank_00_model_states.pt...
[2024-12-03 17:43:37,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-19/global_step19/mp_rank_00_model_states.pt.
[2024-12-03 17:43:37,895] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-19/global_step19/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:43:44,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-19/global_step19/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:43:44,199] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-19/global_step19/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:43:44,200] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:43:44
End of save checkpoint, device rank: 2, time: 2024-12-03 17:43:44
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:43:47
[2024-12-03 17:43:48,682] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
[2024-12-03 17:43:48,683] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 193.60 | bwd_microstep: 557.17 | bwd_inner_microstep: 538.16 | bwd_allreduce_microstep: 18.95 | step_microstep: 33.12
[2024-12-03 17:43:48,683] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 193.58 | bwd: 557.19 | bwd_inner: 538.17 | bwd_allreduce: 18.97 | step: 33.12
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:43:48
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:43:48
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:43:48
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:43:59,898] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is about to be saved!
[2024-12-03 17:43:59,907] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-20/global_step20/mp_rank_00_model_states.pt
[2024-12-03 17:43:59,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-20/global_step20/mp_rank_00_model_states.pt...
[2024-12-03 17:44:10,744] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-20/global_step20/mp_rank_00_model_states.pt.
[2024-12-03 17:44:10,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:44:17,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:44:17,750] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:44:17,750] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:44:17
End of save checkpoint, device rank: 1, time: 2024-12-03 17:44:17
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:44:21
[2024-12-03 17:44:22,900] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
[2024-12-03 17:44:22,900] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.70 | bwd_microstep: 589.44 | bwd_inner_microstep: 570.31 | bwd_allreduce_microstep: 19.07 | step_microstep: 33.12
[2024-12-03 17:44:22,901] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.68 | bwd: 589.45 | bwd_inner: 570.31 | bwd_allreduce: 19.09 | step: 33.13
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:44:22
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:44:22Start of save checkpoint, device rank: 2, time: 2024-12-03 17:44:22

is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:44:33,519] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step21 is about to be saved!
[2024-12-03 17:44:33,529] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-21/global_step21/mp_rank_00_model_states.pt
[2024-12-03 17:44:33,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-21/global_step21/mp_rank_00_model_states.pt...
[2024-12-03 17:44:44,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-21/global_step21/mp_rank_00_model_states.pt.
[2024-12-03 17:44:44,105] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-21/global_step21/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:44:51,032] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-21/global_step21/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:44:51,035] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-21/global_step21/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:44:51,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:44:51
End of save checkpoint, device rank: 1, time: 2024-12-03 17:44:51
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:44:55
[2024-12-03 17:44:59,838] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 38.35 | optimizer_gradients: 32.32 | optimizer_step: 2331.14
[2024-12-03 17:44:59,839] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 202.32 | bwd_microstep: 583.95 | bwd_inner_microstep: 564.87 | bwd_allreduce_microstep: 19.03 | step_microstep: 3343.89
[2024-12-03 17:44:59,839] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 202.30 | bwd: 583.96 | bwd_inner: 564.87 | bwd_allreduce: 19.05 | step: 3343.89
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:44:59Start of save checkpoint, device rank: 2, time: 2024-12-03 17:44:59

Start of save checkpoint, device rank: 0, time: 2024-12-03 17:44:59
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:45:10,258] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22 is about to be saved!
[2024-12-03 17:45:13,820] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-22/global_step22/mp_rank_00_model_states.pt
[2024-12-03 17:45:13,820] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-22/global_step22/mp_rank_00_model_states.pt...
[2024-12-03 17:45:24,958] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-22/global_step22/mp_rank_00_model_states.pt.
[2024-12-03 17:45:24,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-22/global_step22/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:45:50,084] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-22/global_step22/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:45:50,089] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-22/global_step22/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:45:50,089] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 17:45:50End of save checkpoint, device rank: 2, time: 2024-12-03 17:45:50

End of save checkpoint, device rank: 0, time: 2024-12-03 17:45:57
[2024-12-03 17:45:59,186] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.76 | optimizer_gradients: 16.17 | optimizer_step: 66.23
[2024-12-03 17:45:59,187] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 212.77 | bwd_microstep: 584.93 | bwd_inner_microstep: 566.01 | bwd_allreduce_microstep: 18.86 | step_microstep: 170.85
[2024-12-03 17:45:59,187] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 212.75 | bwd: 584.94 | bwd_inner: 566.01 | bwd_allreduce: 18.88 | step: 170.86
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:45:59
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:45:59
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:45:59
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:46:10,304] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step23 is about to be saved!
[2024-12-03 17:46:13,577] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-23/global_step23/mp_rank_00_model_states.pt
[2024-12-03 17:46:13,578] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-23/global_step23/mp_rank_00_model_states.pt...
[2024-12-03 17:46:23,807] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-23/global_step23/mp_rank_00_model_states.pt.
[2024-12-03 17:46:23,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-23/global_step23/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:46:45,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-23/global_step23/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:46:45,996] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-23/global_step23/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:46:45,996] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:46:46
End of save checkpoint, device rank: 1, time: 2024-12-03 17:46:46
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:46:54
[2024-12-03 17:46:55,520] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.66 | optimizer_gradients: 16.21 | optimizer_step: 66.23
[2024-12-03 17:46:55,521] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.37 | bwd_microstep: 585.49 | bwd_inner_microstep: 566.51 | bwd_allreduce_microstep: 18.92 | step_microstep: 164.25
[2024-12-03 17:46:55,521] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.35 | bwd: 585.50 | bwd_inner: 566.51 | bwd_allreduce: 18.94 | step: 164.26
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:46:55Start of save checkpoint, device rank: 1, time: 2024-12-03 17:46:55

Start of save checkpoint, device rank: 0, time: 2024-12-03 17:46:55
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:47:06,741] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24 is about to be saved!
[2024-12-03 17:47:12,551] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-24/global_step24/mp_rank_00_model_states.pt
[2024-12-03 17:47:12,551] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-24/global_step24/mp_rank_00_model_states.pt...
[2024-12-03 17:47:23,139] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-24/global_step24/mp_rank_00_model_states.pt.
[2024-12-03 17:47:23,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:47:46,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:47:46,181] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:47:46,181] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:47:46
End of save checkpoint, device rank: 2, time: 2024-12-03 17:47:46
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:47:52
[2024-12-03 17:47:53,772] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.68 | optimizer_gradients: 16.24 | optimizer_step: 66.20
[2024-12-03 17:47:53,772] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.87 | bwd_microstep: 582.70 | bwd_inner_microstep: 563.67 | bwd_allreduce_microstep: 18.97 | step_microstep: 164.37
[2024-12-03 17:47:53,773] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.86 | bwd: 582.71 | bwd_inner: 563.67 | bwd_allreduce: 18.99 | step: 164.37
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:47:53
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:47:53
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:47:53
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:48:04,404] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step25 is about to be saved!
[2024-12-03 17:48:04,413] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-25/global_step25/mp_rank_00_model_states.pt
[2024-12-03 17:48:04,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-25/global_step25/mp_rank_00_model_states.pt...
[2024-12-03 17:48:14,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-25/global_step25/mp_rank_00_model_states.pt.
[2024-12-03 17:48:14,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-25/global_step25/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:48:37,794] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-25/global_step25/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:48:37,797] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-25/global_step25/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:48:37,797] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:48:37
End of save checkpoint, device rank: 2, time: 2024-12-03 17:48:37
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:48:43
[2024-12-03 17:48:44,525] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.66 | optimizer_gradients: 16.17 | optimizer_step: 66.20
[2024-12-03 17:48:44,526] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 205.98 | bwd_microstep: 579.72 | bwd_inner_microstep: 560.83 | bwd_allreduce_microstep: 18.84 | step_microstep: 164.78
[2024-12-03 17:48:44,526] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 205.96 | bwd: 579.73 | bwd_inner: 560.83 | bwd_allreduce: 18.85 | step: 164.80
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:48:44
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:48:44
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:48:44
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:48:55,298] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step26 is about to be saved!
[2024-12-03 17:48:55,310] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-26/global_step26/mp_rank_00_model_states.pt
[2024-12-03 17:48:55,310] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-26/global_step26/mp_rank_00_model_states.pt...
[2024-12-03 17:49:05,678] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-26/global_step26/mp_rank_00_model_states.pt.
[2024-12-03 17:49:05,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-26/global_step26/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:49:28,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-26/global_step26/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:49:28,781] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-26/global_step26/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:49:28,781] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:49:28
End of save checkpoint, device rank: 1, time: 2024-12-03 17:49:28
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:49:36
[2024-12-03 17:49:37,697] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.60 | optimizer_gradients: 16.20 | optimizer_step: 66.33
[2024-12-03 17:49:37,698] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.94 | bwd_microstep: 592.48 | bwd_inner_microstep: 573.54 | bwd_allreduce_microstep: 18.89 | step_microstep: 164.13
[2024-12-03 17:49:37,698] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.92 | bwd: 592.49 | bwd_inner: 573.54 | bwd_allreduce: 18.90 | step: 164.14
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:49:37
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:49:37Start of save checkpoint, device rank: 2, time: 2024-12-03 17:49:37

is_deepspeed_enabled
is_deepspeed_enabledis_deepspeed_enabled

isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:49:48,641] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27 is about to be saved!
[2024-12-03 17:49:48,650] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-27/global_step27/mp_rank_00_model_states.pt
[2024-12-03 17:49:48,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-27/global_step27/mp_rank_00_model_states.pt...
[2024-12-03 17:49:58,863] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-27/global_step27/mp_rank_00_model_states.pt.
[2024-12-03 17:49:58,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-27/global_step27/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:50:21,652] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-27/global_step27/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:50:21,655] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-27/global_step27/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:50:21,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:50:21
End of save checkpoint, device rank: 2, time: 2024-12-03 17:50:21
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:50:26
[2024-12-03 17:50:28,291] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.89 | optimizer_gradients: 16.18 | optimizer_step: 66.18
[2024-12-03 17:50:28,292] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 212.94 | bwd_microstep: 578.24 | bwd_inner_microstep: 560.11 | bwd_allreduce_microstep: 18.08 | step_microstep: 163.85
[2024-12-03 17:50:28,292] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 212.92 | bwd: 578.26 | bwd_inner: 560.11 | bwd_allreduce: 18.10 | step: 163.86
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:50:28
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:50:28
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:50:28
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:50:39,263] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step28 is about to be saved!
[2024-12-03 17:50:39,273] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-28/global_step28/mp_rank_00_model_states.pt
[2024-12-03 17:50:39,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-28/global_step28/mp_rank_00_model_states.pt...
[2024-12-03 17:50:49,890] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-28/global_step28/mp_rank_00_model_states.pt.
[2024-12-03 17:50:49,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-28/global_step28/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:51:13,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-28/global_step28/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:51:13,055] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-28/global_step28/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:51:13,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:51:13
End of save checkpoint, device rank: 2, time: 2024-12-03 17:51:13
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:51:20
[2024-12-03 17:51:22,064] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.62 | optimizer_gradients: 16.18 | optimizer_step: 66.22
[2024-12-03 17:51:22,064] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.79 | bwd_microstep: 595.65 | bwd_inner_microstep: 576.42 | bwd_allreduce_microstep: 19.17 | step_microstep: 163.76
[2024-12-03 17:51:22,065] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.77 | bwd: 595.66 | bwd_inner: 576.42 | bwd_allreduce: 19.19 | step: 163.76
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:51:22
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:51:22
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:51:22
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:51:32,739] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29 is about to be saved!
[2024-12-03 17:51:35,313] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-29/global_step29/mp_rank_00_model_states.pt
[2024-12-03 17:51:35,313] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-29/global_step29/mp_rank_00_model_states.pt...
[2024-12-03 17:51:45,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-29/global_step29/mp_rank_00_model_states.pt.
[2024-12-03 17:51:45,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-29/global_step29/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:52:08,632] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-29/global_step29/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:52:08,635] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-29/global_step29/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:52:08,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:52:08
End of save checkpoint, device rank: 1, time: 2024-12-03 17:52:08
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:52:16
[2024-12-03 17:52:17,550] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.52 | optimizer_gradients: 16.18 | optimizer_step: 66.21
[2024-12-03 17:52:17,551] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 210.31 | bwd_microstep: 595.61 | bwd_inner_microstep: 576.55 | bwd_allreduce_microstep: 19.00 | step_microstep: 164.17
[2024-12-03 17:52:17,551] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 210.28 | bwd: 595.62 | bwd_inner: 576.55 | bwd_allreduce: 19.02 | step: 164.18
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:52:17
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:52:17
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:52:17
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:52:34,213] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is about to be saved!
[2024-12-03 17:52:34,223] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-30/global_step30/mp_rank_00_model_states.pt
[2024-12-03 17:52:34,223] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-30/global_step30/mp_rank_00_model_states.pt...
[2024-12-03 17:52:46,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-30/global_step30/mp_rank_00_model_states.pt.
[2024-12-03 17:52:46,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:53:09,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:53:09,344] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:53:09,344] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:53:09
End of save checkpoint, device rank: 2, time: 2024-12-03 17:53:09
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:53:14
[2024-12-03 17:53:16,131] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.80 | optimizer_gradients: 16.18 | optimizer_step: 66.27
[2024-12-03 17:53:16,131] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.73 | bwd_microstep: 582.34 | bwd_inner_microstep: 563.37 | bwd_allreduce_microstep: 18.91 | step_microstep: 164.06
[2024-12-03 17:53:16,131] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.71 | bwd: 582.35 | bwd_inner: 563.37 | bwd_allreduce: 18.93 | step: 164.06
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:53:16Start of save checkpoint, device rank: 2, time: 2024-12-03 17:53:16

Start of save checkpoint, device rank: 0, time: 2024-12-03 17:53:16
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:53:27,016] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31 is about to be saved!
[2024-12-03 17:53:27,028] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-31/global_step31/mp_rank_00_model_states.pt
[2024-12-03 17:53:27,028] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-31/global_step31/mp_rank_00_model_states.pt...
[2024-12-03 17:53:37,360] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-31/global_step31/mp_rank_00_model_states.pt.
[2024-12-03 17:53:37,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-31/global_step31/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:54:00,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-31/global_step31/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:54:00,294] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-31/global_step31/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:54:00,294] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:54:00
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 17:54:00
End of save checkpoint, device rank: 0, time: 2024-12-03 17:54:07
[2024-12-03 17:54:09,006] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.73 | optimizer_gradients: 16.17 | optimizer_step: 66.27
[2024-12-03 17:54:09,006] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.03 | bwd_microstep: 591.79 | bwd_inner_microstep: 572.77 | bwd_allreduce_microstep: 18.97 | step_microstep: 163.92
[2024-12-03 17:54:09,007] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.00 | bwd: 591.81 | bwd_inner: 572.77 | bwd_allreduce: 18.99 | step: 163.93
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:54:09Start of save checkpoint, device rank: 1, time: 2024-12-03 17:54:09

Start of save checkpoint, device rank: 0, time: 2024-12-03 17:54:09
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:54:20,133] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step32 is about to be saved!
[2024-12-03 17:54:20,146] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-32/global_step32/mp_rank_00_model_states.pt
[2024-12-03 17:54:20,146] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-32/global_step32/mp_rank_00_model_states.pt...
[2024-12-03 17:54:30,697] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-32/global_step32/mp_rank_00_model_states.pt.
[2024-12-03 17:54:30,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-32/global_step32/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:54:53,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-32/global_step32/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:54:53,587] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-32/global_step32/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:54:53,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:54:53
End of save checkpoint, device rank: 2, time: 2024-12-03 17:54:53
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:55:00
[2024-12-03 17:55:02,269] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.78 | optimizer_gradients: 16.21 | optimizer_step: 66.20
[2024-12-03 17:55:02,270] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.02 | bwd_microstep: 599.51 | bwd_inner_microstep: 580.36 | bwd_allreduce_microstep: 19.10 | step_microstep: 164.16
[2024-12-03 17:55:02,270] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.00 | bwd: 599.52 | bwd_inner: 580.36 | bwd_allreduce: 19.12 | step: 164.17
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:55:02
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:55:02
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:55:02
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:55:12,879] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step33 is about to be saved!
[2024-12-03 17:55:12,901] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-33/global_step33/mp_rank_00_model_states.pt
[2024-12-03 17:55:12,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-33/global_step33/mp_rank_00_model_states.pt...
[2024-12-03 17:55:23,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-33/global_step33/mp_rank_00_model_states.pt.
[2024-12-03 17:55:23,031] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-33/global_step33/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:55:46,457] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-33/global_step33/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:55:46,460] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-33/global_step33/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:55:46,460] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:55:46
End of save checkpoint, device rank: 1, time: 2024-12-03 17:55:46
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:55:52
[2024-12-03 17:55:53,448] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.67 | optimizer_gradients: 16.19 | optimizer_step: 66.19
[2024-12-03 17:55:53,449] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.90 | bwd_microstep: 578.88 | bwd_inner_microstep: 559.86 | bwd_allreduce_microstep: 18.96 | step_microstep: 163.91
[2024-12-03 17:55:53,449] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.88 | bwd: 578.89 | bwd_inner: 559.86 | bwd_allreduce: 18.98 | step: 163.92
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:55:53
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:55:53
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:55:53
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:56:04,620] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step34 is about to be saved!
[2024-12-03 17:56:04,633] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-34/global_step34/mp_rank_00_model_states.pt
[2024-12-03 17:56:04,633] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-34/global_step34/mp_rank_00_model_states.pt...
[2024-12-03 17:56:15,101] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-34/global_step34/mp_rank_00_model_states.pt.
[2024-12-03 17:56:15,104] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-34/global_step34/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:56:37,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-34/global_step34/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:56:37,942] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-34/global_step34/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:56:37,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:56:38
End of save checkpoint, device rank: 1, time: 2024-12-03 17:56:38
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:56:43
[2024-12-03 17:56:44,512] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.89 | optimizer_gradients: 16.17 | optimizer_step: 66.17
[2024-12-03 17:56:44,513] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.69 | bwd_microstep: 580.67 | bwd_inner_microstep: 561.73 | bwd_allreduce_microstep: 18.88 | step_microstep: 164.20
[2024-12-03 17:56:44,513] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.67 | bwd: 580.68 | bwd_inner: 561.73 | bwd_allreduce: 18.90 | step: 164.21
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:56:44
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:56:44
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:56:44
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:56:55,425] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step35 is about to be saved!
[2024-12-03 17:56:55,440] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-35/global_step35/mp_rank_00_model_states.pt
[2024-12-03 17:56:55,441] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-35/global_step35/mp_rank_00_model_states.pt...
[2024-12-03 17:57:05,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-35/global_step35/mp_rank_00_model_states.pt.
[2024-12-03 17:57:05,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-35/global_step35/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:57:28,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-35/global_step35/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:57:28,322] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-35/global_step35/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:57:28,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 17:57:28
End of save checkpoint, device rank: 1, time: 2024-12-03 17:57:28
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:57:35
[2024-12-03 17:57:36,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.55 | optimizer_gradients: 16.19 | optimizer_step: 66.21
[2024-12-03 17:57:36,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.24 | bwd_microstep: 585.56 | bwd_inner_microstep: 566.61 | bwd_allreduce_microstep: 18.89 | step_microstep: 163.91
[2024-12-03 17:57:36,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.22 | bwd: 585.57 | bwd_inner: 566.61 | bwd_allreduce: 18.91 | step: 163.92
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:57:36
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:57:36
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:57:36
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:57:47,622] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step36 is about to be saved!
[2024-12-03 17:57:47,634] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-36/global_step36/mp_rank_00_model_states.pt
[2024-12-03 17:57:47,634] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-36/global_step36/mp_rank_00_model_states.pt...
[2024-12-03 17:57:58,214] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-36/global_step36/mp_rank_00_model_states.pt.
[2024-12-03 17:57:58,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-36/global_step36/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:58:21,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-36/global_step36/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:58:21,201] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-36/global_step36/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:58:21,201] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:58:21
End of save checkpoint, device rank: 2, time: 2024-12-03 17:58:21
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:58:29
[2024-12-03 17:58:30,336] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.62 | optimizer_gradients: 16.19 | optimizer_step: 66.19
[2024-12-03 17:58:30,337] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.02 | bwd_microstep: 591.56 | bwd_inner_microstep: 572.68 | bwd_allreduce_microstep: 18.83 | step_microstep: 163.99
[2024-12-03 17:58:30,337] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.00 | bwd: 591.57 | bwd_inner: 572.68 | bwd_allreduce: 18.84 | step: 163.99
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:58:30
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:58:30Start of save checkpoint, device rank: 0, time: 2024-12-03 17:58:30

is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:58:41,153] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step37 is about to be saved!
[2024-12-03 17:58:41,163] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-37/global_step37/mp_rank_00_model_states.pt
[2024-12-03 17:58:41,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-37/global_step37/mp_rank_00_model_states.pt...
[2024-12-03 17:58:51,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-37/global_step37/mp_rank_00_model_states.pt.
[2024-12-03 17:58:51,876] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-37/global_step37/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 17:59:14,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-37/global_step37/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 17:59:14,864] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-37/global_step37/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 17:59:14,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 17:59:14
End of save checkpoint, device rank: 2, time: 2024-12-03 17:59:14
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 17:59:21
[2024-12-03 17:59:22,941] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.69 | optimizer_gradients: 16.18 | optimizer_step: 66.16
[2024-12-03 17:59:22,942] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 211.85 | bwd_microstep: 579.84 | bwd_inner_microstep: 560.81 | bwd_allreduce_microstep: 18.97 | step_microstep: 163.97
[2024-12-03 17:59:22,942] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 211.83 | bwd: 579.85 | bwd_inner: 560.81 | bwd_allreduce: 18.98 | step: 163.98
Start of save checkpoint, device rank: 2, time: 2024-12-03 17:59:22
Start of save checkpoint, device rank: 1, time: 2024-12-03 17:59:22
Start of save checkpoint, device rank: 0, time: 2024-12-03 17:59:22
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 17:59:33,906] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step38 is about to be saved!
[2024-12-03 17:59:33,919] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-38/global_step38/mp_rank_00_model_states.pt
[2024-12-03 17:59:33,919] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-38/global_step38/mp_rank_00_model_states.pt...
[2024-12-03 17:59:47,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-38/global_step38/mp_rank_00_model_states.pt.
[2024-12-03 17:59:47,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-38/global_step38/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:00:10,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-38/global_step38/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:00:10,785] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-38/global_step38/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:00:10,785] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:00:10
End of save checkpoint, device rank: 2, time: 2024-12-03 18:00:10
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:00:16
[2024-12-03 18:00:18,088] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.62 | optimizer_gradients: 16.26 | optimizer_step: 66.17
[2024-12-03 18:00:18,088] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 203.88 | bwd_microstep: 578.74 | bwd_inner_microstep: 560.73 | bwd_allreduce_microstep: 17.96 | step_microstep: 164.56
[2024-12-03 18:00:18,089] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 203.86 | bwd: 578.75 | bwd_inner: 560.73 | bwd_allreduce: 17.98 | step: 164.56
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:00:18
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:00:18
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:00:18
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:00:29,073] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step39 is about to be saved!
[2024-12-03 18:00:29,093] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-39/global_step39/mp_rank_00_model_states.pt
[2024-12-03 18:00:29,093] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-39/global_step39/mp_rank_00_model_states.pt...
[2024-12-03 18:00:39,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-39/global_step39/mp_rank_00_model_states.pt.
[2024-12-03 18:00:39,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-39/global_step39/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:01:03,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-39/global_step39/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:01:03,127] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-39/global_step39/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:01:03,128] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:01:03
End of save checkpoint, device rank: 1, time: 2024-12-03 18:01:03
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:01:08
[2024-12-03 18:01:10,099] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.53 | optimizer_gradients: 16.25 | optimizer_step: 66.19
[2024-12-03 18:01:10,100] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 208.06 | bwd_microstep: 551.00 | bwd_inner_microstep: 532.89 | bwd_allreduce_microstep: 18.05 | step_microstep: 163.99
[2024-12-03 18:01:10,100] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 208.04 | bwd: 551.01 | bwd_inner: 532.89 | bwd_allreduce: 18.07 | step: 164.00
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:01:10
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:01:10
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:01:10
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:01:20,916] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step40 is about to be saved!
[2024-12-03 18:01:20,929] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-40/global_step40/mp_rank_00_model_states.pt
[2024-12-03 18:01:20,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-40/global_step40/mp_rank_00_model_states.pt...
[2024-12-03 18:01:31,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-40/global_step40/mp_rank_00_model_states.pt.
[2024-12-03 18:01:31,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-40/global_step40/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:01:55,033] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-40/global_step40/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:01:55,037] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-40/global_step40/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:01:55,037] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:01:55
End of save checkpoint, device rank: 1, time: 2024-12-03 18:01:55
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:02:02
[2024-12-03 18:02:03,984] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.62 | optimizer_gradients: 16.23 | optimizer_step: 66.22
[2024-12-03 18:02:03,985] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.43 | bwd_microstep: 577.01 | bwd_inner_microstep: 558.02 | bwd_allreduce_microstep: 18.93 | step_microstep: 163.90
[2024-12-03 18:02:03,985] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.41 | bwd: 577.03 | bwd_inner: 558.02 | bwd_allreduce: 18.95 | step: 163.90
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:02:03
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:02:03
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:02:03
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:02:14,875] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step41 is about to be saved!
[2024-12-03 18:02:14,892] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-41/global_step41/mp_rank_00_model_states.pt
[2024-12-03 18:02:14,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-41/global_step41/mp_rank_00_model_states.pt...
[2024-12-03 18:02:25,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-41/global_step41/mp_rank_00_model_states.pt.
[2024-12-03 18:02:25,048] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-41/global_step41/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:02:48,050] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-41/global_step41/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:02:48,053] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-41/global_step41/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:02:48,053] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:02:48
End of save checkpoint, device rank: 1, time: 2024-12-03 18:02:48
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:02:55
[2024-12-03 18:02:56,530] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.61 | optimizer_gradients: 16.24 | optimizer_step: 66.14
[2024-12-03 18:02:56,530] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 204.17 | bwd_microstep: 578.59 | bwd_inner_microstep: 560.58 | bwd_allreduce_microstep: 17.96 | step_microstep: 164.21
[2024-12-03 18:02:56,531] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 204.14 | bwd: 578.60 | bwd_inner: 560.58 | bwd_allreduce: 17.97 | step: 164.22
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:02:56
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:02:56
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:02:56
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:03:07,591] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step42 is about to be saved!
[2024-12-03 18:03:07,605] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-42/global_step42/mp_rank_00_model_states.pt
[2024-12-03 18:03:07,605] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-42/global_step42/mp_rank_00_model_states.pt...
[2024-12-03 18:03:18,047] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-42/global_step42/mp_rank_00_model_states.pt.
[2024-12-03 18:03:18,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-42/global_step42/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:03:41,406] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-42/global_step42/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:03:41,409] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-42/global_step42/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:03:41,409] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:03:41End of save checkpoint, device rank: 2, time: 2024-12-03 18:03:41

should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:03:48
[2024-12-03 18:03:49,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.76 | optimizer_gradients: 16.19 | optimizer_step: 66.18
[2024-12-03 18:03:49,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.96 | bwd_microstep: 586.59 | bwd_inner_microstep: 567.52 | bwd_allreduce_microstep: 19.02 | step_microstep: 164.01
[2024-12-03 18:03:49,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.94 | bwd: 586.60 | bwd_inner: 567.52 | bwd_allreduce: 19.03 | step: 164.02
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:03:49Start of save checkpoint, device rank: 2, time: 2024-12-03 18:03:49

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:03:49
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:04:00,993] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step43 is about to be saved!
[2024-12-03 18:04:01,009] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-43/global_step43/mp_rank_00_model_states.pt
[2024-12-03 18:04:01,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-43/global_step43/mp_rank_00_model_states.pt...
[2024-12-03 18:04:11,458] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-43/global_step43/mp_rank_00_model_states.pt.
[2024-12-03 18:04:11,460] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-43/global_step43/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:04:34,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-43/global_step43/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:04:34,298] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-43/global_step43/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:04:34,298] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:04:34
End of save checkpoint, device rank: 1, time: 2024-12-03 18:04:34
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:04:40
[2024-12-03 18:04:42,019] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.72 | optimizer_gradients: 16.18 | optimizer_step: 66.17
[2024-12-03 18:04:42,019] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.85 | bwd_microstep: 580.23 | bwd_inner_microstep: 561.25 | bwd_allreduce_microstep: 18.93 | step_microstep: 163.79
[2024-12-03 18:04:42,019] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.83 | bwd: 580.25 | bwd_inner: 561.25 | bwd_allreduce: 18.95 | step: 163.80
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:04:42
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:04:42
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:04:42
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:04:52,878] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step44 is about to be saved!
[2024-12-03 18:04:52,888] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-44/global_step44/mp_rank_00_model_states.pt
[2024-12-03 18:04:52,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-44/global_step44/mp_rank_00_model_states.pt...
[2024-12-03 18:05:03,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-44/global_step44/mp_rank_00_model_states.pt.
[2024-12-03 18:05:03,485] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-44/global_step44/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:05:26,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-44/global_step44/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:05:26,825] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-44/global_step44/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:05:26,825] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step44 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:05:26
End of save checkpoint, device rank: 1, time: 2024-12-03 18:05:26
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:05:32
[2024-12-03 18:05:33,332] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.54 | optimizer_gradients: 16.21 | optimizer_step: 66.24
[2024-12-03 18:05:33,333] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 210.80 | bwd_microstep: 578.96 | bwd_inner_microstep: 559.93 | bwd_allreduce_microstep: 18.97 | step_microstep: 163.91
[2024-12-03 18:05:33,333] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 210.78 | bwd: 578.98 | bwd_inner: 559.93 | bwd_allreduce: 18.99 | step: 163.92
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:05:33
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:05:33
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:05:33
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:05:44,034] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step45 is about to be saved!
[2024-12-03 18:05:44,044] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-45/global_step45/mp_rank_00_model_states.pt
[2024-12-03 18:05:44,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-45/global_step45/mp_rank_00_model_states.pt...
[2024-12-03 18:05:54,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-45/global_step45/mp_rank_00_model_states.pt.
[2024-12-03 18:05:54,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-45/global_step45/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:06:16,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-45/global_step45/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:06:16,786] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-45/global_step45/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:06:16,786] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:06:17
End of save checkpoint, device rank: 2, time: 2024-12-03 18:06:17
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:06:24
[2024-12-03 18:06:25,769] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.59 | optimizer_gradients: 16.18 | optimizer_step: 66.23
[2024-12-03 18:06:25,770] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.26 | bwd_microstep: 579.02 | bwd_inner_microstep: 559.96 | bwd_allreduce_microstep: 19.00 | step_microstep: 163.85
[2024-12-03 18:06:25,770] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.23 | bwd: 579.03 | bwd_inner: 559.96 | bwd_allreduce: 19.02 | step: 163.85
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:06:25
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:06:25
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:06:25
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:06:36,877] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step46 is about to be saved!
[2024-12-03 18:06:36,886] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-46/global_step46/mp_rank_00_model_states.pt
[2024-12-03 18:06:36,886] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-46/global_step46/mp_rank_00_model_states.pt...
[2024-12-03 18:06:47,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-46/global_step46/mp_rank_00_model_states.pt.
[2024-12-03 18:06:47,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-46/global_step46/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:07:09,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-46/global_step46/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:07:09,521] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-46/global_step46/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:07:09,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step46 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:07:10
End of save checkpoint, device rank: 2, time: 2024-12-03 18:07:10
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:07:17
[2024-12-03 18:07:18,637] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.75 | optimizer_gradients: 16.19 | optimizer_step: 66.20
[2024-12-03 18:07:18,638] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.19 | bwd_microstep: 578.58 | bwd_inner_microstep: 559.51 | bwd_allreduce_microstep: 19.02 | step_microstep: 163.91
[2024-12-03 18:07:18,638] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.17 | bwd: 578.60 | bwd_inner: 559.51 | bwd_allreduce: 19.04 | step: 163.91
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:07:18
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:07:18
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:07:18
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:07:29,256] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step47 is about to be saved!
[2024-12-03 18:07:29,271] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-47/global_step47/mp_rank_00_model_states.pt
[2024-12-03 18:07:29,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-47/global_step47/mp_rank_00_model_states.pt...
[2024-12-03 18:07:39,453] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-47/global_step47/mp_rank_00_model_states.pt.
[2024-12-03 18:07:39,455] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-47/global_step47/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:08:02,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-47/global_step47/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:08:02,543] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-47/global_step47/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:08:02,543] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step47 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:08:02
End of save checkpoint, device rank: 2, time: 2024-12-03 18:08:02
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:08:08
[2024-12-03 18:08:09,306] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.86 | optimizer_gradients: 16.17 | optimizer_step: 66.25
[2024-12-03 18:08:09,306] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.14 | bwd_microstep: 587.07 | bwd_inner_microstep: 568.07 | bwd_allreduce_microstep: 18.95 | step_microstep: 164.06
[2024-12-03 18:08:09,307] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.12 | bwd: 587.08 | bwd_inner: 568.07 | bwd_allreduce: 18.97 | step: 164.07
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:08:09Start of save checkpoint, device rank: 1, time: 2024-12-03 18:08:09

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:08:09
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:08:20,491] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step48 is about to be saved!
[2024-12-03 18:08:20,503] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-48/global_step48/mp_rank_00_model_states.pt
[2024-12-03 18:08:20,503] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-48/global_step48/mp_rank_00_model_states.pt...
[2024-12-03 18:08:30,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-48/global_step48/mp_rank_00_model_states.pt.
[2024-12-03 18:08:30,877] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-48/global_step48/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:08:53,680] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-48/global_step48/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:08:53,683] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-48/global_step48/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:08:53,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step48 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 18:08:54
End of save checkpoint, device rank: 1, time: 2024-12-03 18:08:54
End of save checkpoint, device rank: 0, time: 2024-12-03 18:09:00
[2024-12-03 18:09:02,023] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.56 | optimizer_gradients: 16.25 | optimizer_step: 96.17
[2024-12-03 18:09:02,024] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 209.76 | bwd_microstep: 577.18 | bwd_inner_microstep: 558.12 | bwd_allreduce_microstep: 19.00 | step_microstep: 355.32
[2024-12-03 18:09:02,024] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 209.74 | bwd: 577.19 | bwd_inner: 558.12 | bwd_allreduce: 19.02 | step: 355.33
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:09:02
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:09:02
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:09:02
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:09:12,639] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step49 is about to be saved!
[2024-12-03 18:09:12,683] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-49/global_step49/mp_rank_00_model_states.pt
[2024-12-03 18:09:12,683] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-49/global_step49/mp_rank_00_model_states.pt...
[2024-12-03 18:09:23,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-49/global_step49/mp_rank_00_model_states.pt.
[2024-12-03 18:09:23,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-49/global_step49/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:09:45,689] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-49/global_step49/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:09:45,692] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-49/global_step49/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:09:45,692] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step49 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:09:46
End of save checkpoint, device rank: 2, time: 2024-12-03 18:09:46
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:09:51
[2024-12-03 18:09:52,368] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.86 | optimizer_gradients: 16.17 | optimizer_step: 66.24
[2024-12-03 18:09:52,368] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.05 | bwd_microstep: 588.69 | bwd_inner_microstep: 570.70 | bwd_allreduce_microstep: 17.94 | step_microstep: 164.01
[2024-12-03 18:09:52,369] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.03 | bwd: 588.70 | bwd_inner: 570.70 | bwd_allreduce: 17.95 | step: 164.02
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:09:52
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:09:52
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:09:52
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:10:03,634] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is about to be saved!
[2024-12-03 18:10:03,646] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-50/global_step50/mp_rank_00_model_states.pt
[2024-12-03 18:10:03,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-50/global_step50/mp_rank_00_model_states.pt...
[2024-12-03 18:10:14,288] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-50/global_step50/mp_rank_00_model_states.pt.
[2024-12-03 18:10:14,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:10:36,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:10:36,446] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:10:36,446] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:10:37
End of save checkpoint, device rank: 1, time: 2024-12-03 18:10:37
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:10:44
[2024-12-03 18:10:45,739] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.77 | optimizer_gradients: 16.19 | optimizer_step: 66.23
[2024-12-03 18:10:45,739] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.63 | bwd_microstep: 578.90 | bwd_inner_microstep: 559.81 | bwd_allreduce_microstep: 19.03 | step_microstep: 164.20
[2024-12-03 18:10:45,740] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.61 | bwd: 578.91 | bwd_inner: 559.81 | bwd_allreduce: 19.05 | step: 164.21
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:10:45
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:10:45
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:10:45
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:10:56,495] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step51 is about to be saved!
[2024-12-03 18:10:56,507] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-51/global_step51/mp_rank_00_model_states.pt
[2024-12-03 18:10:56,507] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-51/global_step51/mp_rank_00_model_states.pt...
[2024-12-03 18:11:06,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-51/global_step51/mp_rank_00_model_states.pt.
[2024-12-03 18:11:06,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-51/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:11:29,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-51/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:11:29,961] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-51/global_step51/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:11:29,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:11:29
End of save checkpoint, device rank: 1, time: 2024-12-03 18:11:29
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:11:35
[2024-12-03 18:11:36,772] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.25 | optimizer_gradients: 16.23 | optimizer_step: 66.16
[2024-12-03 18:11:36,773] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.77 | bwd_microstep: 586.27 | bwd_inner_microstep: 568.31 | bwd_allreduce_microstep: 17.91 | step_microstep: 163.79
[2024-12-03 18:11:36,773] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.75 | bwd: 586.28 | bwd_inner: 568.31 | bwd_allreduce: 17.92 | step: 163.80
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:11:36
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:11:36
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:11:36
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:11:47,708] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step52 is about to be saved!
[2024-12-03 18:11:47,720] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-52/global_step52/mp_rank_00_model_states.pt
[2024-12-03 18:11:47,720] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-52/global_step52/mp_rank_00_model_states.pt...
[2024-12-03 18:11:58,096] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-52/global_step52/mp_rank_00_model_states.pt.
[2024-12-03 18:11:58,099] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-52/global_step52/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:12:20,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-52/global_step52/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:12:20,879] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-52/global_step52/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:12:20,879] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step52 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:12:20
End of save checkpoint, device rank: 1, time: 2024-12-03 18:12:20
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:12:26
[2024-12-03 18:12:27,887] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.69 | optimizer_gradients: 16.18 | optimizer_step: 66.22
[2024-12-03 18:12:27,887] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.11 | bwd_microstep: 584.21 | bwd_inner_microstep: 566.25 | bwd_allreduce_microstep: 17.90 | step_microstep: 164.03
[2024-12-03 18:12:27,887] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.09 | bwd: 584.22 | bwd_inner: 566.25 | bwd_allreduce: 17.92 | step: 164.04
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:12:27
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:12:27
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:12:27
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:12:38,771] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step53 is about to be saved!
[2024-12-03 18:12:38,785] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-53/global_step53/mp_rank_00_model_states.pt
[2024-12-03 18:12:38,785] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-53/global_step53/mp_rank_00_model_states.pt...
[2024-12-03 18:12:49,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-53/global_step53/mp_rank_00_model_states.pt.
[2024-12-03 18:12:49,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-53/global_step53/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:13:11,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-53/global_step53/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:13:11,833] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-53/global_step53/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:13:11,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step53 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:13:12
End of save checkpoint, device rank: 2, time: 2024-12-03 18:13:12
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:13:19
[2024-12-03 18:13:20,832] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.79 | optimizer_gradients: 16.18 | optimizer_step: 66.22
[2024-12-03 18:13:20,832] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.07 | bwd_microstep: 578.72 | bwd_inner_microstep: 559.71 | bwd_allreduce_microstep: 18.96 | step_microstep: 164.19
[2024-12-03 18:13:20,832] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.05 | bwd: 578.74 | bwd_inner: 559.71 | bwd_allreduce: 18.97 | step: 164.19
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:13:20
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:13:20
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:13:20
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:13:31,461] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step54 is about to be saved!
[2024-12-03 18:13:31,472] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-54/global_step54/mp_rank_00_model_states.pt
[2024-12-03 18:13:31,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-54/global_step54/mp_rank_00_model_states.pt...
[2024-12-03 18:13:42,128] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-54/global_step54/mp_rank_00_model_states.pt.
[2024-12-03 18:13:42,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-54/global_step54/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:14:05,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-54/global_step54/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:14:05,055] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-54/global_step54/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:14:05,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step54 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:14:05
End of save checkpoint, device rank: 2, time: 2024-12-03 18:14:05
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:14:10
[2024-12-03 18:14:11,818] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.78 | optimizer_gradients: 16.19 | optimizer_step: 66.25
[2024-12-03 18:14:11,819] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.89 | bwd_microstep: 578.48 | bwd_inner_microstep: 559.48 | bwd_allreduce_microstep: 18.94 | step_microstep: 164.15
[2024-12-03 18:14:11,819] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.87 | bwd: 578.49 | bwd_inner: 559.47 | bwd_allreduce: 18.96 | step: 164.16
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:14:11
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:14:11
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:14:11
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:14:23,106] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step55 is about to be saved!
[2024-12-03 18:14:23,118] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-55/global_step55/mp_rank_00_model_states.pt
[2024-12-03 18:14:23,118] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-55/global_step55/mp_rank_00_model_states.pt...
[2024-12-03 18:14:33,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-55/global_step55/mp_rank_00_model_states.pt.
[2024-12-03 18:14:33,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-55/global_step55/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:14:56,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-55/global_step55/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:14:56,917] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-55/global_step55/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:14:56,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step55 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:14:56
End of save checkpoint, device rank: 2, time: 2024-12-03 18:14:56
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:15:03
[2024-12-03 18:15:04,864] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.69 | optimizer_gradients: 16.19 | optimizer_step: 66.20
[2024-12-03 18:15:04,864] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.56 | bwd_microstep: 586.45 | bwd_inner_microstep: 567.28 | bwd_allreduce_microstep: 19.12 | step_microstep: 163.75
[2024-12-03 18:15:04,864] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.54 | bwd: 586.47 | bwd_inner: 567.28 | bwd_allreduce: 19.13 | step: 163.75
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:15:04
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:15:04
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:15:04
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:15:15,650] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step56 is about to be saved!
[2024-12-03 18:15:15,662] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-56/global_step56/mp_rank_00_model_states.pt
[2024-12-03 18:15:15,662] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-56/global_step56/mp_rank_00_model_states.pt...
[2024-12-03 18:15:26,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-56/global_step56/mp_rank_00_model_states.pt.
[2024-12-03 18:15:26,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-56/global_step56/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:15:49,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-56/global_step56/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:15:49,478] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-56/global_step56/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:15:49,478] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step56 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:15:49
End of save checkpoint, device rank: 2, time: 2024-12-03 18:15:49
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:15:55
[2024-12-03 18:15:56,541] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.57 | optimizer_gradients: 16.17 | optimizer_step: 66.20
[2024-12-03 18:15:56,542] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.15 | bwd_microstep: 597.54 | bwd_inner_microstep: 578.53 | bwd_allreduce_microstep: 18.96 | step_microstep: 163.82
[2024-12-03 18:15:56,542] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.13 | bwd: 597.56 | bwd_inner: 578.53 | bwd_allreduce: 18.98 | step: 163.82
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:15:56
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:15:56
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:15:56
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:16:07,279] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step57 is about to be saved!
[2024-12-03 18:16:07,289] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-57/global_step57/mp_rank_00_model_states.pt
[2024-12-03 18:16:07,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-57/global_step57/mp_rank_00_model_states.pt...
[2024-12-03 18:16:17,802] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-57/global_step57/mp_rank_00_model_states.pt.
[2024-12-03 18:16:17,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-57/global_step57/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:16:40,801] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-57/global_step57/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:16:40,804] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-57/global_step57/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:16:40,804] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step57 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:16:40
End of save checkpoint, device rank: 1, time: 2024-12-03 18:16:40
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:16:46
[2024-12-03 18:16:48,241] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.52 | optimizer_gradients: 16.16 | optimizer_step: 66.23
[2024-12-03 18:16:48,241] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.29 | bwd_microstep: 591.88 | bwd_inner_microstep: 572.89 | bwd_allreduce_microstep: 18.94 | step_microstep: 163.76
[2024-12-03 18:16:48,242] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.27 | bwd: 591.90 | bwd_inner: 572.89 | bwd_allreduce: 18.96 | step: 163.77
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:16:48
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:16:48
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:16:48
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:16:59,205] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step58 is about to be saved!
[2024-12-03 18:16:59,218] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-58/global_step58/mp_rank_00_model_states.pt
[2024-12-03 18:16:59,218] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-58/global_step58/mp_rank_00_model_states.pt...
[2024-12-03 18:17:09,698] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-58/global_step58/mp_rank_00_model_states.pt.
[2024-12-03 18:17:09,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-58/global_step58/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:17:32,653] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-58/global_step58/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:17:32,656] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-58/global_step58/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:17:32,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step58 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:17:32
End of save checkpoint, device rank: 2, time: 2024-12-03 18:17:32
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:17:39
[2024-12-03 18:17:41,509] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.80 | optimizer_gradients: 16.17 | optimizer_step: 66.29
[2024-12-03 18:17:41,510] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.55 | bwd_microstep: 592.71 | bwd_inner_microstep: 573.74 | bwd_allreduce_microstep: 18.92 | step_microstep: 164.22
[2024-12-03 18:17:41,510] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.53 | bwd: 592.72 | bwd_inner: 573.74 | bwd_allreduce: 18.94 | step: 164.23
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:17:41
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:17:41
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:17:41
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:17:52,572] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step59 is about to be saved!
[2024-12-03 18:17:52,592] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-59/global_step59/mp_rank_00_model_states.pt
[2024-12-03 18:17:52,592] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-59/global_step59/mp_rank_00_model_states.pt...
[2024-12-03 18:18:02,979] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-59/global_step59/mp_rank_00_model_states.pt.
[2024-12-03 18:18:02,981] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-59/global_step59/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:18:25,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-59/global_step59/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:18:25,921] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-59/global_step59/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:18:25,921] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step59 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:18:25
End of save checkpoint, device rank: 2, time: 2024-12-03 18:18:25
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:18:33
[2024-12-03 18:18:34,839] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.63 | optimizer_gradients: 16.20 | optimizer_step: 66.29
[2024-12-03 18:18:34,840] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.51 | bwd_microstep: 587.41 | bwd_inner_microstep: 568.36 | bwd_allreduce_microstep: 19.00 | step_microstep: 163.80
[2024-12-03 18:18:34,840] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.49 | bwd: 587.43 | bwd_inner: 568.36 | bwd_allreduce: 19.02 | step: 163.81
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:18:34
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:18:34
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:18:34
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:18:45,922] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is about to be saved!
[2024-12-03 18:18:45,934] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-60/global_step60/mp_rank_00_model_states.pt
[2024-12-03 18:18:45,934] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-60/global_step60/mp_rank_00_model_states.pt...
[2024-12-03 18:18:56,396] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-60/global_step60/mp_rank_00_model_states.pt.
[2024-12-03 18:18:56,399] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:19:18,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:19:18,901] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:19:18,901] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:19:19
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 18:19:19
End of save checkpoint, device rank: 0, time: 2024-12-03 18:19:26
[2024-12-03 18:19:27,793] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.95 | optimizer_gradients: 16.18 | optimizer_step: 66.26
[2024-12-03 18:19:27,793] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 204.22 | bwd_microstep: 577.79 | bwd_inner_microstep: 558.77 | bwd_allreduce_microstep: 18.97 | step_microstep: 164.25
[2024-12-03 18:19:27,794] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 204.20 | bwd: 577.81 | bwd_inner: 558.77 | bwd_allreduce: 18.99 | step: 164.26
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:19:27
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:19:27
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:19:27
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:19:38,600] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step61 is about to be saved!
[2024-12-03 18:19:38,614] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-61/global_step61/mp_rank_00_model_states.pt
[2024-12-03 18:19:38,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-61/global_step61/mp_rank_00_model_states.pt...
[2024-12-03 18:19:49,007] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-61/global_step61/mp_rank_00_model_states.pt.
[2024-12-03 18:19:49,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-61/global_step61/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:20:12,209] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-61/global_step61/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:20:12,213] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-61/global_step61/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:20:12,213] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step61 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:20:12
End of save checkpoint, device rank: 1, time: 2024-12-03 18:20:12
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:20:17
[2024-12-03 18:20:18,847] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 31.10 | optimizer_gradients: 16.20 | optimizer_step: 66.25
[2024-12-03 18:20:18,847] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 205.86 | bwd_microstep: 578.10 | bwd_inner_microstep: 559.08 | bwd_allreduce_microstep: 18.96 | step_microstep: 164.39
[2024-12-03 18:20:18,847] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 205.85 | bwd: 578.11 | bwd_inner: 559.08 | bwd_allreduce: 18.98 | step: 164.40
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:20:18
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:20:18
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:20:18
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:20:29,982] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step62 is about to be saved!
[2024-12-03 18:20:29,994] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-62/global_step62/mp_rank_00_model_states.pt
[2024-12-03 18:20:29,994] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-62/global_step62/mp_rank_00_model_states.pt...
[2024-12-03 18:20:40,414] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-62/global_step62/mp_rank_00_model_states.pt.
[2024-12-03 18:20:40,417] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-62/global_step62/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:21:03,779] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-62/global_step62/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:21:03,783] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-62/global_step62/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:21:03,783] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step62 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:21:03
End of save checkpoint, device rank: 2, time: 2024-12-03 18:21:03
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:21:11
[2024-12-03 18:21:12,248] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.91 | optimizer_gradients: 16.17 | optimizer_step: 66.18
[2024-12-03 18:21:12,249] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.67 | bwd_microstep: 576.79 | bwd_inner_microstep: 557.85 | bwd_allreduce_microstep: 18.89 | step_microstep: 164.53
[2024-12-03 18:21:12,249] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.65 | bwd: 576.80 | bwd_inner: 557.85 | bwd_allreduce: 18.91 | step: 164.53
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:21:12
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:21:12
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:21:12
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:21:23,295] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step63 is about to be saved!
[2024-12-03 18:21:23,306] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-63/global_step63/mp_rank_00_model_states.pt
[2024-12-03 18:21:23,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-63/global_step63/mp_rank_00_model_states.pt...
[2024-12-03 18:21:33,852] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-63/global_step63/mp_rank_00_model_states.pt.
[2024-12-03 18:21:33,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-63/global_step63/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:21:57,039] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-63/global_step63/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:21:57,042] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-63/global_step63/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:21:57,042] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step63 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:21:57
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 18:21:57
End of save checkpoint, device rank: 0, time: 2024-12-03 18:22:04
[2024-12-03 18:22:05,819] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.70 | optimizer_gradients: 16.18 | optimizer_step: 66.20
[2024-12-03 18:22:05,820] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 216.85 | bwd_microstep: 577.20 | bwd_inner_microstep: 558.19 | bwd_allreduce_microstep: 18.95 | step_microstep: 163.91
[2024-12-03 18:22:05,820] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 216.83 | bwd: 577.21 | bwd_inner: 558.19 | bwd_allreduce: 18.97 | step: 163.92
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:22:05
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:22:05
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:22:05
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:22:16,706] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step64 is about to be saved!
[2024-12-03 18:22:16,718] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-64/global_step64/mp_rank_00_model_states.pt
[2024-12-03 18:22:16,718] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-64/global_step64/mp_rank_00_model_states.pt...
[2024-12-03 18:22:27,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-64/global_step64/mp_rank_00_model_states.pt.
[2024-12-03 18:22:27,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-64/global_step64/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:22:50,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-64/global_step64/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:22:50,741] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-64/global_step64/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:22:50,742] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step64 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:22:50
End of save checkpoint, device rank: 2, time: 2024-12-03 18:22:50
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:22:56
[2024-12-03 18:22:57,389] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.56 | optimizer_gradients: 16.18 | optimizer_step: 66.27
[2024-12-03 18:22:57,390] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.54 | bwd_microstep: 580.80 | bwd_inner_microstep: 562.72 | bwd_allreduce_microstep: 18.03 | step_microstep: 163.99
[2024-12-03 18:22:57,390] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.52 | bwd: 580.82 | bwd_inner: 562.72 | bwd_allreduce: 18.04 | step: 164.00
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:22:57
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:22:57
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:22:57
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:23:08,213] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step65 is about to be saved!
[2024-12-03 18:23:08,225] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-65/global_step65/mp_rank_00_model_states.pt
[2024-12-03 18:23:08,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-65/global_step65/mp_rank_00_model_states.pt...
[2024-12-03 18:23:18,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-65/global_step65/mp_rank_00_model_states.pt.
[2024-12-03 18:23:18,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-65/global_step65/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:23:41,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-65/global_step65/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:23:41,413] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-65/global_step65/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:23:41,414] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step65 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:23:41
End of save checkpoint, device rank: 1, time: 2024-12-03 18:23:41
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:23:46
[2024-12-03 18:23:47,904] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.79 | optimizer_gradients: 16.17 | optimizer_step: 66.15
[2024-12-03 18:23:47,904] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.63 | bwd_microstep: 585.30 | bwd_inner_microstep: 566.35 | bwd_allreduce_microstep: 18.89 | step_microstep: 163.89
[2024-12-03 18:23:47,904] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.61 | bwd: 585.31 | bwd_inner: 566.35 | bwd_allreduce: 18.91 | step: 163.89
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:23:47
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:23:47
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:23:47
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:23:58,707] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step66 is about to be saved!
[2024-12-03 18:23:58,716] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-66/global_step66/mp_rank_00_model_states.pt
[2024-12-03 18:23:58,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-66/global_step66/mp_rank_00_model_states.pt...
[2024-12-03 18:24:09,127] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-66/global_step66/mp_rank_00_model_states.pt.
[2024-12-03 18:24:09,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-66/global_step66/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:24:32,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-66/global_step66/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:24:32,028] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-66/global_step66/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:24:32,028] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step66 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:24:32
End of save checkpoint, device rank: 1, time: 2024-12-03 18:24:32
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:24:37
[2024-12-03 18:24:38,945] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.78 | optimizer_gradients: 16.17 | optimizer_step: 66.27
[2024-12-03 18:24:38,946] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.51 | bwd_microstep: 577.75 | bwd_inner_microstep: 558.68 | bwd_allreduce_microstep: 19.02 | step_microstep: 163.96
[2024-12-03 18:24:38,946] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.49 | bwd: 577.76 | bwd_inner: 558.68 | bwd_allreduce: 19.04 | step: 163.96
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:24:38
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:24:38
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:24:38
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:24:50,197] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step67 is about to be saved!
[2024-12-03 18:24:50,212] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-67/global_step67/mp_rank_00_model_states.pt
[2024-12-03 18:24:50,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-67/global_step67/mp_rank_00_model_states.pt...
[2024-12-03 18:25:00,586] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-67/global_step67/mp_rank_00_model_states.pt.
[2024-12-03 18:25:00,588] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-67/global_step67/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:25:23,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-67/global_step67/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:25:23,420] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-67/global_step67/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:25:23,420] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step67 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:25:23
End of save checkpoint, device rank: 2, time: 2024-12-03 18:25:23should save

End of save checkpoint, device rank: 0, time: 2024-12-03 18:25:30
[2024-12-03 18:25:31,719] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.76 | optimizer_gradients: 16.17 | optimizer_step: 66.24
[2024-12-03 18:25:31,719] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 214.90 | bwd_microstep: 576.39 | bwd_inner_microstep: 558.56 | bwd_allreduce_microstep: 17.78 | step_microstep: 164.06
[2024-12-03 18:25:31,720] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 214.89 | bwd: 576.40 | bwd_inner: 558.56 | bwd_allreduce: 17.79 | step: 164.06
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:25:31
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:25:31
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:25:31
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:25:42,837] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step68 is about to be saved!
[2024-12-03 18:25:42,846] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-68/global_step68/mp_rank_00_model_states.pt
[2024-12-03 18:25:42,847] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-68/global_step68/mp_rank_00_model_states.pt...
[2024-12-03 18:25:53,207] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-68/global_step68/mp_rank_00_model_states.pt.
[2024-12-03 18:25:53,209] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-68/global_step68/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:26:16,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-68/global_step68/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:26:16,083] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-68/global_step68/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:26:16,083] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step68 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:26:16
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 18:26:16
End of save checkpoint, device rank: 0, time: 2024-12-03 18:26:21
[2024-12-03 18:26:22,457] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.59 | optimizer_gradients: 16.20 | optimizer_step: 66.19
[2024-12-03 18:26:22,458] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.62 | bwd_microstep: 585.74 | bwd_inner_microstep: 566.70 | bwd_allreduce_microstep: 18.98 | step_microstep: 163.95
[2024-12-03 18:26:22,458] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.60 | bwd: 585.75 | bwd_inner: 566.70 | bwd_allreduce: 19.00 | step: 163.95
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:26:22
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:26:22
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:26:22
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:26:32,923] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step69 is about to be saved!
[2024-12-03 18:26:32,941] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-69/global_step69/mp_rank_00_model_states.pt
[2024-12-03 18:26:32,941] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-69/global_step69/mp_rank_00_model_states.pt...
[2024-12-03 18:26:43,423] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-69/global_step69/mp_rank_00_model_states.pt.
[2024-12-03 18:26:43,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-69/global_step69/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:27:05,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-69/global_step69/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:27:05,976] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-69/global_step69/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:27:05,976] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step69 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:27:06
End of save checkpoint, device rank: 1, time: 2024-12-03 18:27:06
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:27:13
[2024-12-03 18:27:14,590] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.55 | optimizer_gradients: 16.23 | optimizer_step: 66.20
[2024-12-03 18:27:14,591] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.96 | bwd_microstep: 585.73 | bwd_inner_microstep: 566.69 | bwd_allreduce_microstep: 18.98 | step_microstep: 164.00
[2024-12-03 18:27:14,591] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.93 | bwd: 585.74 | bwd_inner: 566.70 | bwd_allreduce: 19.00 | step: 164.01
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:27:14
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:27:14
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:27:14
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:27:25,694] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step70 is about to be saved!
[2024-12-03 18:27:25,706] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-70/global_step70/mp_rank_00_model_states.pt
[2024-12-03 18:27:25,707] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-70/global_step70/mp_rank_00_model_states.pt...
[2024-12-03 18:27:36,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-70/global_step70/mp_rank_00_model_states.pt.
[2024-12-03 18:27:36,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-70/global_step70/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:27:59,614] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-70/global_step70/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:27:59,618] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-70/global_step70/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:27:59,618] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step70 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:27:59
End of save checkpoint, device rank: 1, time: 2024-12-03 18:27:59
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:28:05
[2024-12-03 18:28:06,846] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 31.01 | optimizer_gradients: 16.18 | optimizer_step: 66.26
[2024-12-03 18:28:06,846] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.39 | bwd_microstep: 576.83 | bwd_inner_microstep: 557.75 | bwd_allreduce_microstep: 19.03 | step_microstep: 164.15
[2024-12-03 18:28:06,847] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.37 | bwd: 576.84 | bwd_inner: 557.75 | bwd_allreduce: 19.04 | step: 164.15
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:28:06Start of save checkpoint, device rank: 2, time: 2024-12-03 18:28:06

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:28:06
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:28:17,562] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step71 is about to be saved!
[2024-12-03 18:28:17,574] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-71/global_step71/mp_rank_00_model_states.pt
[2024-12-03 18:28:17,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-71/global_step71/mp_rank_00_model_states.pt...
[2024-12-03 18:28:28,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-71/global_step71/mp_rank_00_model_states.pt.
[2024-12-03 18:28:28,480] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-71/global_step71/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:28:51,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-71/global_step71/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:28:51,395] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-71/global_step71/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:28:51,395] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step71 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:28:51
End of save checkpoint, device rank: 2, time: 2024-12-03 18:28:51
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:28:56
[2024-12-03 18:28:58,035] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.80 | optimizer_gradients: 16.17 | optimizer_step: 66.20
[2024-12-03 18:28:58,036] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 202.57 | bwd_microstep: 583.35 | bwd_inner_microstep: 564.44 | bwd_allreduce_microstep: 18.86 | step_microstep: 163.92
[2024-12-03 18:28:58,036] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 202.56 | bwd: 583.36 | bwd_inner: 564.44 | bwd_allreduce: 18.87 | step: 163.93
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:28:58
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:28:58
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:28:58
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:29:08,880] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step72 is about to be saved!
[2024-12-03 18:29:08,893] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-72/global_step72/mp_rank_00_model_states.pt
[2024-12-03 18:29:08,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-72/global_step72/mp_rank_00_model_states.pt...
[2024-12-03 18:29:19,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-72/global_step72/mp_rank_00_model_states.pt.
[2024-12-03 18:29:19,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-72/global_step72/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:29:42,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-72/global_step72/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:29:42,215] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-72/global_step72/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:29:42,215] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step72 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:29:42
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 18:29:42
End of save checkpoint, device rank: 0, time: 2024-12-03 18:29:49
[2024-12-03 18:29:50,327] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.98 | optimizer_gradients: 16.17 | optimizer_step: 66.16
[2024-12-03 18:29:50,328] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.97 | bwd_microstep: 591.16 | bwd_inner_microstep: 572.12 | bwd_allreduce_microstep: 18.99 | step_microstep: 164.11
[2024-12-03 18:29:50,328] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.95 | bwd: 591.17 | bwd_inner: 572.12 | bwd_allreduce: 19.00 | step: 164.12
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:29:50
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:29:50
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:29:50
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:30:01,096] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step73 is about to be saved!
[2024-12-03 18:30:01,106] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-73/global_step73/mp_rank_00_model_states.pt
[2024-12-03 18:30:01,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-73/global_step73/mp_rank_00_model_states.pt...
[2024-12-03 18:30:11,625] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-73/global_step73/mp_rank_00_model_states.pt.
[2024-12-03 18:30:11,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-73/global_step73/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:30:34,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-73/global_step73/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:30:34,441] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-73/global_step73/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:30:34,441] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step73 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:30:34
End of save checkpoint, device rank: 1, time: 2024-12-03 18:30:34
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:30:41
[2024-12-03 18:30:42,985] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 31.06 | optimizer_gradients: 16.18 | optimizer_step: 66.13
[2024-12-03 18:30:42,985] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 201.34 | bwd_microstep: 581.07 | bwd_inner_microstep: 562.09 | bwd_allreduce_microstep: 18.93 | step_microstep: 164.14
[2024-12-03 18:30:42,985] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 201.32 | bwd: 581.09 | bwd_inner: 562.09 | bwd_allreduce: 18.95 | step: 164.15
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:30:42
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:30:42
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:30:42
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:30:53,594] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step74 is about to be saved!
[2024-12-03 18:30:53,611] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-74/global_step74/mp_rank_00_model_states.pt
[2024-12-03 18:30:53,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-74/global_step74/mp_rank_00_model_states.pt...
[2024-12-03 18:31:04,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-74/global_step74/mp_rank_00_model_states.pt.
[2024-12-03 18:31:04,014] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-74/global_step74/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:31:27,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-74/global_step74/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:31:27,187] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-74/global_step74/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:31:27,187] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step74 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:31:27
End of save checkpoint, device rank: 1, time: 2024-12-03 18:31:27
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:31:34
[2024-12-03 18:31:35,516] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 31.09 | optimizer_gradients: 16.18 | optimizer_step: 66.21
[2024-12-03 18:31:35,516] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.63 | bwd_microstep: 596.53 | bwd_inner_microstep: 577.56 | bwd_allreduce_microstep: 18.92 | step_microstep: 164.24
[2024-12-03 18:31:35,517] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.61 | bwd: 596.54 | bwd_inner: 577.56 | bwd_allreduce: 18.94 | step: 164.25
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:31:35
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:31:35
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:31:35
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:31:46,238] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step75 is about to be saved!
[2024-12-03 18:31:46,250] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-75/global_step75/mp_rank_00_model_states.pt
[2024-12-03 18:31:46,250] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-75/global_step75/mp_rank_00_model_states.pt...
[2024-12-03 18:31:56,863] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-75/global_step75/mp_rank_00_model_states.pt.
[2024-12-03 18:31:56,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-75/global_step75/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:32:20,055] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-75/global_step75/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:32:20,058] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-75/global_step75/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:32:20,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step75 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:32:20
End of save checkpoint, device rank: 1, time: 2024-12-03 18:32:20
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:32:25
[2024-12-03 18:32:26,291] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.97 | optimizer_gradients: 16.18 | optimizer_step: 66.17
[2024-12-03 18:32:26,291] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 216.99 | bwd_microstep: 576.36 | bwd_inner_microstep: 557.29 | bwd_allreduce_microstep: 19.02 | step_microstep: 164.14
[2024-12-03 18:32:26,291] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 216.97 | bwd: 576.37 | bwd_inner: 557.29 | bwd_allreduce: 19.04 | step: 164.14
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:32:26
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:32:26
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:32:26
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:32:36,980] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step76 is about to be saved!
[2024-12-03 18:32:36,995] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-76/global_step76/mp_rank_00_model_states.pt
[2024-12-03 18:32:36,995] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-76/global_step76/mp_rank_00_model_states.pt...
[2024-12-03 18:32:47,721] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-76/global_step76/mp_rank_00_model_states.pt.
[2024-12-03 18:32:47,724] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-76/global_step76/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:33:10,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-76/global_step76/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:33:10,559] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-76/global_step76/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:33:10,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step76 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:33:10
End of save checkpoint, device rank: 2, time: 2024-12-03 18:33:10
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:33:17
[2024-12-03 18:33:18,244] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.75 | optimizer_gradients: 16.17 | optimizer_step: 66.11
[2024-12-03 18:33:18,245] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.54 | bwd_microstep: 586.19 | bwd_inner_microstep: 567.24 | bwd_allreduce_microstep: 18.90 | step_microstep: 163.85
[2024-12-03 18:33:18,245] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.52 | bwd: 586.20 | bwd_inner: 567.24 | bwd_allreduce: 18.91 | step: 163.86
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:33:18
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:33:18
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:33:18
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:33:29,009] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step77 is about to be saved!
[2024-12-03 18:33:29,018] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-77/global_step77/mp_rank_00_model_states.pt
[2024-12-03 18:33:29,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-77/global_step77/mp_rank_00_model_states.pt...
[2024-12-03 18:33:39,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-77/global_step77/mp_rank_00_model_states.pt.
[2024-12-03 18:33:39,354] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-77/global_step77/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:34:02,301] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-77/global_step77/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:34:02,305] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-77/global_step77/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:34:02,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step77 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:34:02
End of save checkpoint, device rank: 2, time: 2024-12-03 18:34:02
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:34:09
[2024-12-03 18:34:10,456] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.58 | optimizer_gradients: 16.17 | optimizer_step: 66.21
[2024-12-03 18:34:10,457] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 203.75 | bwd_microstep: 577.13 | bwd_inner_microstep: 559.09 | bwd_allreduce_microstep: 17.99 | step_microstep: 163.64
[2024-12-03 18:34:10,457] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 203.73 | bwd: 577.15 | bwd_inner: 559.09 | bwd_allreduce: 18.00 | step: 163.64
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:34:10
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:34:10
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:34:10
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:34:21,277] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step78 is about to be saved!
[2024-12-03 18:34:21,304] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-78/global_step78/mp_rank_00_model_states.pt
[2024-12-03 18:34:21,304] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-78/global_step78/mp_rank_00_model_states.pt...
[2024-12-03 18:34:31,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-78/global_step78/mp_rank_00_model_states.pt.
[2024-12-03 18:34:32,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-78/global_step78/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:34:55,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-78/global_step78/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:34:55,154] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-78/global_step78/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:34:55,154] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step78 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 18:34:55
End of save checkpoint, device rank: 2, time: 2024-12-03 18:34:55
End of save checkpoint, device rank: 0, time: 2024-12-03 18:35:01
[2024-12-03 18:35:03,904] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.70 | optimizer_gradients: 16.23 | optimizer_step: 311.99
[2024-12-03 18:35:03,905] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 213.08 | bwd_microstep: 577.27 | bwd_inner_microstep: 558.34 | bwd_allreduce_microstep: 18.88 | step_microstep: 773.45
[2024-12-03 18:35:03,905] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 213.06 | bwd: 577.28 | bwd_inner: 558.34 | bwd_allreduce: 18.90 | step: 773.46
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:35:03
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:35:03
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:35:03
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:35:15,162] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step79 is about to be saved!
[2024-12-03 18:35:15,271] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-79/global_step79/mp_rank_00_model_states.pt
[2024-12-03 18:35:15,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-79/global_step79/mp_rank_00_model_states.pt...
[2024-12-03 18:35:25,646] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-79/global_step79/mp_rank_00_model_states.pt.
[2024-12-03 18:35:25,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-79/global_step79/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:35:48,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-79/global_step79/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:35:48,410] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-79/global_step79/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:35:48,410] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step79 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:35:48
End of save checkpoint, device rank: 1, time: 2024-12-03 18:35:48
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:35:54
[2024-12-03 18:35:55,860] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.55 | optimizer_gradients: 16.19 | optimizer_step: 66.22
[2024-12-03 18:35:55,861] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.20 | bwd_microstep: 588.24 | bwd_inner_microstep: 569.09 | bwd_allreduce_microstep: 19.10 | step_microstep: 163.85
[2024-12-03 18:35:55,861] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.18 | bwd: 588.25 | bwd_inner: 569.09 | bwd_allreduce: 19.12 | step: 163.85
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:35:55
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:35:55
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:35:55
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:36:06,487] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step80 is about to be saved!
[2024-12-03 18:36:06,497] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-80/global_step80/mp_rank_00_model_states.pt
[2024-12-03 18:36:06,497] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-80/global_step80/mp_rank_00_model_states.pt...
[2024-12-03 18:36:17,018] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-80/global_step80/mp_rank_00_model_states.pt.
[2024-12-03 18:36:17,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-80/global_step80/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:36:39,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-80/global_step80/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:36:39,762] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-80/global_step80/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:36:39,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step80 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:36:39
End of save checkpoint, device rank: 1, time: 2024-12-03 18:36:39
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:36:45
[2024-12-03 18:36:46,423] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.56 | optimizer_gradients: 16.18 | optimizer_step: 66.21
[2024-12-03 18:36:46,424] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.10 | bwd_microstep: 578.25 | bwd_inner_microstep: 559.16 | bwd_allreduce_microstep: 19.04 | step_microstep: 164.32
[2024-12-03 18:36:46,424] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.07 | bwd: 578.27 | bwd_inner: 559.16 | bwd_allreduce: 19.05 | step: 164.33
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:36:46
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:36:46
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:36:46
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:36:57,296] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step81 is about to be saved!
[2024-12-03 18:36:57,310] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-81/global_step81/mp_rank_00_model_states.pt
[2024-12-03 18:36:57,310] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-81/global_step81/mp_rank_00_model_states.pt...
[2024-12-03 18:37:07,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-81/global_step81/mp_rank_00_model_states.pt.
[2024-12-03 18:37:07,776] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-81/global_step81/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:37:30,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-81/global_step81/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:37:30,697] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-81/global_step81/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:37:30,698] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step81 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:37:30
End of save checkpoint, device rank: 1, time: 2024-12-03 18:37:30
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:37:36
[2024-12-03 18:37:37,592] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.91 | optimizer_gradients: 16.18 | optimizer_step: 66.13
[2024-12-03 18:37:37,593] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 202.58 | bwd_microstep: 584.07 | bwd_inner_microstep: 564.90 | bwd_allreduce_microstep: 19.12 | step_microstep: 163.93
[2024-12-03 18:37:37,593] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 202.56 | bwd: 584.08 | bwd_inner: 564.90 | bwd_allreduce: 19.14 | step: 163.94
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:37:37
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:37:37
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:37:37
is_deepspeed_enabled
is_deepspeed_enabledis_deepspeed_enabled

isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:37:48,575] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step82 is about to be saved!
[2024-12-03 18:37:48,585] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-82/global_step82/mp_rank_00_model_states.pt
[2024-12-03 18:37:48,585] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-82/global_step82/mp_rank_00_model_states.pt...
[2024-12-03 18:37:59,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-82/global_step82/mp_rank_00_model_states.pt.
[2024-12-03 18:37:59,184] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-82/global_step82/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:38:21,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-82/global_step82/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:38:21,479] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-82/global_step82/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:38:21,479] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step82 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:38:22
End of save checkpoint, device rank: 2, time: 2024-12-03 18:38:22
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:38:29
[2024-12-03 18:38:30,693] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.74 | optimizer_gradients: 16.17 | optimizer_step: 66.13
[2024-12-03 18:38:30,693] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 209.17 | bwd_microstep: 590.84 | bwd_inner_microstep: 571.86 | bwd_allreduce_microstep: 18.93 | step_microstep: 163.80
[2024-12-03 18:38:30,693] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 209.15 | bwd: 590.85 | bwd_inner: 571.86 | bwd_allreduce: 18.95 | step: 163.81
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:38:30Start of save checkpoint, device rank: 2, time: 2024-12-03 18:38:30

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:38:30
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:38:41,685] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step83 is about to be saved!
[2024-12-03 18:38:41,694] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-83/global_step83/mp_rank_00_model_states.pt
[2024-12-03 18:38:41,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-83/global_step83/mp_rank_00_model_states.pt...
[2024-12-03 18:38:51,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-83/global_step83/mp_rank_00_model_states.pt.
[2024-12-03 18:38:51,915] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-83/global_step83/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:39:14,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-83/global_step83/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:39:14,864] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-83/global_step83/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:39:14,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step83 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 18:39:14
End of save checkpoint, device rank: 2, time: 2024-12-03 18:39:14
End of save checkpoint, device rank: 0, time: 2024-12-03 18:39:20
[2024-12-03 18:39:21,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.91 | optimizer_gradients: 16.17 | optimizer_step: 66.13
[2024-12-03 18:39:21,692] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.51 | bwd_microstep: 576.45 | bwd_inner_microstep: 557.48 | bwd_allreduce_microstep: 18.92 | step_microstep: 164.00
[2024-12-03 18:39:21,692] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.49 | bwd: 576.46 | bwd_inner: 557.48 | bwd_allreduce: 18.93 | step: 164.01
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:39:21
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:39:21
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:39:21
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:39:32,603] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step84 is about to be saved!
[2024-12-03 18:39:32,615] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-84/global_step84/mp_rank_00_model_states.pt
[2024-12-03 18:39:32,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-84/global_step84/mp_rank_00_model_states.pt...
[2024-12-03 18:39:43,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-84/global_step84/mp_rank_00_model_states.pt.
[2024-12-03 18:39:43,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-84/global_step84/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:40:05,715] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-84/global_step84/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:40:05,719] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-84/global_step84/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:40:05,719] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step84 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:40:05
End of save checkpoint, device rank: 1, time: 2024-12-03 18:40:05
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:40:12
[2024-12-03 18:40:14,151] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.70 | optimizer_gradients: 16.17 | optimizer_step: 66.11
[2024-12-03 18:40:14,151] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.45 | bwd_microstep: 579.09 | bwd_inner_microstep: 560.06 | bwd_allreduce_microstep: 18.98 | step_microstep: 163.92
[2024-12-03 18:40:14,152] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.43 | bwd: 579.11 | bwd_inner: 560.06 | bwd_allreduce: 19.00 | step: 163.93
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:40:14Start of save checkpoint, device rank: 2, time: 2024-12-03 18:40:14

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:40:14
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:40:25,076] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step85 is about to be saved!
[2024-12-03 18:40:25,089] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-85/global_step85/mp_rank_00_model_states.pt
[2024-12-03 18:40:25,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-85/global_step85/mp_rank_00_model_states.pt...
[2024-12-03 18:40:35,330] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-85/global_step85/mp_rank_00_model_states.pt.
[2024-12-03 18:40:35,332] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-85/global_step85/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:40:57,978] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-85/global_step85/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:40:57,982] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-85/global_step85/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:40:57,982] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step85 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:40:58
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 18:40:58
End of save checkpoint, device rank: 0, time: 2024-12-03 18:41:05
[2024-12-03 18:41:06,618] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 41.55 | optimizer_gradients: 16.17 | optimizer_step: 66.17
[2024-12-03 18:41:06,619] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 215.77 | bwd_microstep: 576.91 | bwd_inner_microstep: 557.83 | bwd_allreduce_microstep: 19.03 | step_microstep: 174.69
[2024-12-03 18:41:06,619] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 215.75 | bwd: 576.92 | bwd_inner: 557.83 | bwd_allreduce: 19.05 | step: 174.69
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:41:06
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:41:06
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:41:06
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:41:17,412] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step86 is about to be saved!
[2024-12-03 18:41:17,422] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-86/global_step86/mp_rank_00_model_states.pt
[2024-12-03 18:41:17,422] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-86/global_step86/mp_rank_00_model_states.pt...
[2024-12-03 18:41:27,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-86/global_step86/mp_rank_00_model_states.pt.
[2024-12-03 18:41:27,760] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-86/global_step86/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:41:50,683] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-86/global_step86/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:41:50,686] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-86/global_step86/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:41:50,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step86 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:41:50
End of save checkpoint, device rank: 2, time: 2024-12-03 18:41:50
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:41:56
[2024-12-03 18:41:57,476] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.53 | optimizer_gradients: 16.17 | optimizer_step: 66.20
[2024-12-03 18:41:57,477] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.48 | bwd_microstep: 582.59 | bwd_inner_microstep: 563.59 | bwd_allreduce_microstep: 18.95 | step_microstep: 163.60
[2024-12-03 18:41:57,477] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.46 | bwd: 582.60 | bwd_inner: 563.59 | bwd_allreduce: 18.96 | step: 163.61
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:41:57Start of save checkpoint, device rank: 1, time: 2024-12-03 18:41:57

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:41:57
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:42:08,701] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step87 is about to be saved!
[2024-12-03 18:42:08,723] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-87/global_step87/mp_rank_00_model_states.pt
[2024-12-03 18:42:08,723] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-87/global_step87/mp_rank_00_model_states.pt...
[2024-12-03 18:42:19,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-87/global_step87/mp_rank_00_model_states.pt.
[2024-12-03 18:42:19,378] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-87/global_step87/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:42:42,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-87/global_step87/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:42:42,345] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-87/global_step87/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:42:42,345] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step87 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:42:42
End of save checkpoint, device rank: 2, time: 2024-12-03 18:42:42
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:42:47
[2024-12-03 18:42:49,130] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.65 | optimizer_gradients: 16.18 | optimizer_step: 66.18
[2024-12-03 18:42:49,130] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.83 | bwd_microstep: 576.24 | bwd_inner_microstep: 557.34 | bwd_allreduce_microstep: 18.84 | step_microstep: 163.94
[2024-12-03 18:42:49,131] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.81 | bwd: 576.26 | bwd_inner: 557.34 | bwd_allreduce: 18.86 | step: 163.95
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:42:49
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:42:49
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:42:49
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:42:59,846] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step88 is about to be saved!
[2024-12-03 18:42:59,856] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-88/global_step88/mp_rank_00_model_states.pt
[2024-12-03 18:42:59,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-88/global_step88/mp_rank_00_model_states.pt...
[2024-12-03 18:43:10,301] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-88/global_step88/mp_rank_00_model_states.pt.
[2024-12-03 18:43:10,303] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-88/global_step88/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:43:33,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-88/global_step88/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:43:33,522] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-88/global_step88/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:43:33,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step88 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:43:33
End of save checkpoint, device rank: 1, time: 2024-12-03 18:43:33
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:43:41
[2024-12-03 18:43:42,426] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.72 | optimizer_gradients: 16.22 | optimizer_step: 66.16
[2024-12-03 18:43:42,427] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.34 | bwd_microstep: 576.48 | bwd_inner_microstep: 557.45 | bwd_allreduce_microstep: 18.98 | step_microstep: 164.01
[2024-12-03 18:43:42,427] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.32 | bwd: 576.49 | bwd_inner: 557.45 | bwd_allreduce: 19.00 | step: 164.02
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:43:42
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:43:42
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:43:42
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:43:53,280] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step89 is about to be saved!
[2024-12-03 18:43:53,290] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-89/global_step89/mp_rank_00_model_states.pt
[2024-12-03 18:43:53,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-89/global_step89/mp_rank_00_model_states.pt...
[2024-12-03 18:44:03,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-89/global_step89/mp_rank_00_model_states.pt.
[2024-12-03 18:44:03,390] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-89/global_step89/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:44:26,051] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-89/global_step89/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:44:26,054] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-89/global_step89/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:44:26,054] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step89 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:44:26
should saveEnd of save checkpoint, device rank: 2, time: 2024-12-03 18:44:26

End of save checkpoint, device rank: 0, time: 2024-12-03 18:44:33
[2024-12-03 18:44:34,476] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.72 | optimizer_gradients: 16.16 | optimizer_step: 66.14
[2024-12-03 18:44:34,477] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.77 | bwd_microstep: 578.38 | bwd_inner_microstep: 559.35 | bwd_allreduce_microstep: 18.97 | step_microstep: 163.95
[2024-12-03 18:44:34,477] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.75 | bwd: 578.39 | bwd_inner: 559.35 | bwd_allreduce: 18.99 | step: 163.95
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:44:34
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:44:34
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:44:34
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:44:45,477] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step90 is about to be saved!
[2024-12-03 18:44:45,487] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-90/global_step90/mp_rank_00_model_states.pt
[2024-12-03 18:44:45,487] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-90/global_step90/mp_rank_00_model_states.pt...
[2024-12-03 18:44:55,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-90/global_step90/mp_rank_00_model_states.pt.
[2024-12-03 18:44:55,832] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-90/global_step90/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:45:18,736] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-90/global_step90/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:45:18,739] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-90/global_step90/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:45:18,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step90 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:45:18
End of save checkpoint, device rank: 1, time: 2024-12-03 18:45:18
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:45:25
[2024-12-03 18:45:26,800] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.79 | optimizer_gradients: 16.19 | optimizer_step: 66.21
[2024-12-03 18:45:26,800] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 216.38 | bwd_microstep: 576.94 | bwd_inner_microstep: 557.95 | bwd_allreduce_microstep: 18.93 | step_microstep: 164.22
[2024-12-03 18:45:26,800] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 216.36 | bwd: 576.95 | bwd_inner: 557.95 | bwd_allreduce: 18.95 | step: 164.22
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:45:26Start of save checkpoint, device rank: 1, time: 2024-12-03 18:45:26

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:45:26
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:45:37,628] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step91 is about to be saved!
[2024-12-03 18:45:37,637] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-91/global_step91/mp_rank_00_model_states.pt
[2024-12-03 18:45:37,637] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-91/global_step91/mp_rank_00_model_states.pt...
[2024-12-03 18:45:47,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-91/global_step91/mp_rank_00_model_states.pt.
[2024-12-03 18:45:47,977] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-91/global_step91/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:46:11,238] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-91/global_step91/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:46:11,241] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-91/global_step91/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:46:11,242] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step91 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:46:11
End of save checkpoint, device rank: 2, time: 2024-12-03 18:46:11
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:46:18
[2024-12-03 18:46:19,959] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.65 | optimizer_gradients: 16.18 | optimizer_step: 66.16
[2024-12-03 18:46:19,959] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 202.51 | bwd_microstep: 576.69 | bwd_inner_microstep: 557.72 | bwd_allreduce_microstep: 18.92 | step_microstep: 163.92
[2024-12-03 18:46:19,960] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 202.49 | bwd: 576.70 | bwd_inner: 557.72 | bwd_allreduce: 18.94 | step: 163.92
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:46:19
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:46:19
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:46:19
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:46:30,872] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step92 is about to be saved!
[2024-12-03 18:46:30,886] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-92/global_step92/mp_rank_00_model_states.pt
[2024-12-03 18:46:30,886] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-92/global_step92/mp_rank_00_model_states.pt...
[2024-12-03 18:46:41,041] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-92/global_step92/mp_rank_00_model_states.pt.
[2024-12-03 18:46:41,043] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-92/global_step92/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:47:03,921] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-92/global_step92/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:47:03,925] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-92/global_step92/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:47:03,925] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step92 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:47:03
End of save checkpoint, device rank: 2, time: 2024-12-03 18:47:03
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:47:09
[2024-12-03 18:47:10,844] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.80 | optimizer_gradients: 16.21 | optimizer_step: 66.26
[2024-12-03 18:47:10,845] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 203.64 | bwd_microstep: 582.54 | bwd_inner_microstep: 563.29 | bwd_allreduce_microstep: 19.19 | step_microstep: 164.20
[2024-12-03 18:47:10,845] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 203.62 | bwd: 582.55 | bwd_inner: 563.29 | bwd_allreduce: 19.21 | step: 164.21
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:47:10
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:47:10
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:47:10
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:47:21,797] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step93 is about to be saved!
[2024-12-03 18:47:21,806] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-93/global_step93/mp_rank_00_model_states.pt
[2024-12-03 18:47:21,806] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-93/global_step93/mp_rank_00_model_states.pt...
[2024-12-03 18:47:32,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-93/global_step93/mp_rank_00_model_states.pt.
[2024-12-03 18:47:32,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-93/global_step93/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:47:55,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-93/global_step93/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:47:55,418] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-93/global_step93/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:47:55,418] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step93 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:47:55
End of save checkpoint, device rank: 1, time: 2024-12-03 18:47:55
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:48:01
[2024-12-03 18:48:03,068] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.77 | optimizer_gradients: 16.18 | optimizer_step: 66.16
[2024-12-03 18:48:03,068] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.49 | bwd_microstep: 576.99 | bwd_inner_microstep: 557.95 | bwd_allreduce_microstep: 18.98 | step_microstep: 163.82
[2024-12-03 18:48:03,069] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.47 | bwd: 577.00 | bwd_inner: 557.95 | bwd_allreduce: 19.00 | step: 163.82
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:48:03
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:48:03
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:48:03
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:48:14,635] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step94 is about to be saved!
[2024-12-03 18:48:14,649] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-94/global_step94/mp_rank_00_model_states.pt
[2024-12-03 18:48:14,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-94/global_step94/mp_rank_00_model_states.pt...
[2024-12-03 18:48:25,635] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-94/global_step94/mp_rank_00_model_states.pt.
[2024-12-03 18:48:25,637] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-94/global_step94/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:48:48,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-94/global_step94/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:48:48,302] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-94/global_step94/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:48:48,302] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step94 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:48:48
End of save checkpoint, device rank: 1, time: 2024-12-03 18:48:48
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:48:53
[2024-12-03 18:48:55,154] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.77 | optimizer_gradients: 16.18 | optimizer_step: 66.20
[2024-12-03 18:48:55,155] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 208.48 | bwd_microstep: 575.84 | bwd_inner_microstep: 556.84 | bwd_allreduce_microstep: 18.95 | step_microstep: 163.83
[2024-12-03 18:48:55,155] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 208.46 | bwd: 575.86 | bwd_inner: 556.84 | bwd_allreduce: 18.96 | step: 163.83
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:48:55
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:48:55
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:48:55
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:49:06,159] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step95 is about to be saved!
[2024-12-03 18:49:06,175] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-95/global_step95/mp_rank_00_model_states.pt
[2024-12-03 18:49:06,175] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-95/global_step95/mp_rank_00_model_states.pt...
[2024-12-03 18:49:16,435] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-95/global_step95/mp_rank_00_model_states.pt.
[2024-12-03 18:49:16,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-95/global_step95/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:49:38,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-95/global_step95/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:49:38,684] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-95/global_step95/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:49:38,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step95 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:49:39
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 18:49:39
End of save checkpoint, device rank: 0, time: 2024-12-03 18:49:44
[2024-12-03 18:49:46,208] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.51 | optimizer_gradients: 16.17 | optimizer_step: 66.13
[2024-12-03 18:49:46,208] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 208.95 | bwd_microstep: 576.74 | bwd_inner_microstep: 557.74 | bwd_allreduce_microstep: 18.95 | step_microstep: 163.68
[2024-12-03 18:49:46,208] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 208.93 | bwd: 576.75 | bwd_inner: 557.74 | bwd_allreduce: 18.97 | step: 163.68
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:49:46Start of save checkpoint, device rank: 1, time: 2024-12-03 18:49:46

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:49:46
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:49:57,056] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step96 is about to be saved!
[2024-12-03 18:49:57,065] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-96/global_step96/mp_rank_00_model_states.pt
[2024-12-03 18:49:57,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-96/global_step96/mp_rank_00_model_states.pt...
[2024-12-03 18:50:07,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-96/global_step96/mp_rank_00_model_states.pt.
[2024-12-03 18:50:07,447] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-96/global_step96/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:50:29,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-96/global_step96/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:50:29,870] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-96/global_step96/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:50:29,870] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step96 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:50:30
End of save checkpoint, device rank: 1, time: 2024-12-03 18:50:30
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:50:37
[2024-12-03 18:50:38,693] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.77 | optimizer_gradients: 16.17 | optimizer_step: 66.22
[2024-12-03 18:50:38,693] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.05 | bwd_microstep: 575.99 | bwd_inner_microstep: 558.10 | bwd_allreduce_microstep: 17.84 | step_microstep: 163.98
[2024-12-03 18:50:38,693] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.03 | bwd: 576.01 | bwd_inner: 558.10 | bwd_allreduce: 17.86 | step: 163.99
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:50:38
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:50:38
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:50:38
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:50:49,548] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step97 is about to be saved!
[2024-12-03 18:50:49,565] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-97/global_step97/mp_rank_00_model_states.pt
[2024-12-03 18:50:49,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-97/global_step97/mp_rank_00_model_states.pt...
[2024-12-03 18:50:59,817] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-97/global_step97/mp_rank_00_model_states.pt.
[2024-12-03 18:50:59,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-97/global_step97/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:51:22,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-97/global_step97/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:51:22,590] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-97/global_step97/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:51:22,590] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step97 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:51:22
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 18:51:22
End of save checkpoint, device rank: 0, time: 2024-12-03 18:51:29
[2024-12-03 18:51:31,093] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.79 | optimizer_gradients: 16.18 | optimizer_step: 66.26
[2024-12-03 18:51:31,094] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.68 | bwd_microstep: 576.79 | bwd_inner_microstep: 558.49 | bwd_allreduce_microstep: 18.25 | step_microstep: 163.91
[2024-12-03 18:51:31,094] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.66 | bwd: 576.80 | bwd_inner: 558.49 | bwd_allreduce: 18.27 | step: 163.92
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:51:31
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:51:31
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:51:31
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:51:42,009] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step98 is about to be saved!
[2024-12-03 18:51:42,022] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-98/global_step98/mp_rank_00_model_states.pt
[2024-12-03 18:51:42,022] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-98/global_step98/mp_rank_00_model_states.pt...
[2024-12-03 18:51:52,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-98/global_step98/mp_rank_00_model_states.pt.
[2024-12-03 18:51:52,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-98/global_step98/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:52:15,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-98/global_step98/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:52:15,373] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-98/global_step98/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:52:15,373] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step98 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 18:52:15End of save checkpoint, device rank: 2, time: 2024-12-03 18:52:15

End of save checkpoint, device rank: 0, time: 2024-12-03 18:52:20
[2024-12-03 18:52:21,690] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.56 | optimizer_gradients: 16.18 | optimizer_step: 66.21
[2024-12-03 18:52:21,690] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 203.55 | bwd_microstep: 577.10 | bwd_inner_microstep: 558.03 | bwd_allreduce_microstep: 19.01 | step_microstep: 205.34
[2024-12-03 18:52:21,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 203.53 | bwd: 577.11 | bwd_inner: 558.03 | bwd_allreduce: 19.03 | step: 205.35
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:52:21
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:52:21
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:52:21
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:52:33,136] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step99 is about to be saved!
[2024-12-03 18:52:33,150] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-99/global_step99/mp_rank_00_model_states.pt
[2024-12-03 18:52:33,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-99/global_step99/mp_rank_00_model_states.pt...
[2024-12-03 18:52:43,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-99/global_step99/mp_rank_00_model_states.pt.
[2024-12-03 18:52:43,878] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-99/global_step99/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:53:06,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-99/global_step99/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:53:06,841] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-99/global_step99/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:53:06,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step99 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:53:06
End of save checkpoint, device rank: 1, time: 2024-12-03 18:53:06
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:53:14
[2024-12-03 18:53:15,491] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.53 | optimizer_gradients: 16.18 | optimizer_step: 66.32
[2024-12-03 18:53:15,491] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.77 | bwd_microstep: 579.88 | bwd_inner_microstep: 560.78 | bwd_allreduce_microstep: 19.05 | step_microstep: 163.88
[2024-12-03 18:53:15,492] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.75 | bwd: 579.89 | bwd_inner: 560.78 | bwd_allreduce: 19.06 | step: 163.89
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:53:15
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:53:15
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:53:15
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:53:26,330] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[2024-12-03 18:53:26,340] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-100/global_step100/mp_rank_00_model_states.pt
[2024-12-03 18:53:26,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-100/global_step100/mp_rank_00_model_states.pt...
[2024-12-03 18:53:36,723] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-100/global_step100/mp_rank_00_model_states.pt.
[2024-12-03 18:53:36,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:53:59,500] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:53:59,503] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:53:59,503] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:53:59
should saveEnd of save checkpoint, device rank: 1, time: 2024-12-03 18:53:59

End of save checkpoint, device rank: 0, time: 2024-12-03 18:54:07
[2024-12-03 18:54:08,573] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.65 | optimizer_gradients: 16.17 | optimizer_step: 66.17
[2024-12-03 18:54:08,574] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 212.77 | bwd_microstep: 576.59 | bwd_inner_microstep: 557.58 | bwd_allreduce_microstep: 18.96 | step_microstep: 163.87
[2024-12-03 18:54:08,574] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 212.75 | bwd: 576.60 | bwd_inner: 557.58 | bwd_allreduce: 18.97 | step: 163.87
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:54:08
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:54:08
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:54:08
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:54:19,566] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step101 is about to be saved!
[2024-12-03 18:54:19,575] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-101/global_step101/mp_rank_00_model_states.pt
[2024-12-03 18:54:19,575] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-101/global_step101/mp_rank_00_model_states.pt...
[2024-12-03 18:54:30,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-101/global_step101/mp_rank_00_model_states.pt.
[2024-12-03 18:54:30,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-101/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:54:53,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-101/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:54:53,079] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-101/global_step101/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:54:53,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step101 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:54:53
End of save checkpoint, device rank: 2, time: 2024-12-03 18:54:53
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:54:58
[2024-12-03 18:55:00,188] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 46.11 | optimizer_gradients: 16.17 | optimizer_step: 66.22
[2024-12-03 18:55:00,189] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.67 | bwd_microstep: 581.43 | bwd_inner_microstep: 562.44 | bwd_allreduce_microstep: 18.94 | step_microstep: 179.81
[2024-12-03 18:55:00,189] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.65 | bwd: 581.44 | bwd_inner: 562.44 | bwd_allreduce: 18.95 | step: 179.81
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:55:00
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:55:00
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:55:00
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:55:11,659] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step102 is about to be saved!
[2024-12-03 18:55:11,668] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-102/global_step102/mp_rank_00_model_states.pt
[2024-12-03 18:55:11,668] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-102/global_step102/mp_rank_00_model_states.pt...
[2024-12-03 18:55:22,044] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-102/global_step102/mp_rank_00_model_states.pt.
[2024-12-03 18:55:22,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-102/global_step102/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:55:45,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-102/global_step102/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:55:45,016] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-102/global_step102/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:55:45,016] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step102 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:55:45
End of save checkpoint, device rank: 2, time: 2024-12-03 18:55:45
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:55:51
[2024-12-03 18:55:53,139] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.54 | optimizer_gradients: 16.18 | optimizer_step: 66.24
[2024-12-03 18:55:53,140] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.31 | bwd_microstep: 577.98 | bwd_inner_microstep: 559.04 | bwd_allreduce_microstep: 18.89 | step_microstep: 163.65
[2024-12-03 18:55:53,140] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.29 | bwd: 577.99 | bwd_inner: 559.04 | bwd_allreduce: 18.91 | step: 163.66
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:55:53
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:55:53
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:55:53
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:56:04,265] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step103 is about to be saved!
[2024-12-03 18:56:04,273] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-103/global_step103/mp_rank_00_model_states.pt
[2024-12-03 18:56:04,274] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-103/global_step103/mp_rank_00_model_states.pt...
[2024-12-03 18:56:14,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-103/global_step103/mp_rank_00_model_states.pt.
[2024-12-03 18:56:14,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-103/global_step103/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:56:37,093] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-103/global_step103/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:56:37,096] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-103/global_step103/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:56:37,097] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step103 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:56:37
End of save checkpoint, device rank: 1, time: 2024-12-03 18:56:37
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:56:42
[2024-12-03 18:56:43,723] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.96 | optimizer_gradients: 16.17 | optimizer_step: 66.23
[2024-12-03 18:56:43,724] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.57 | bwd_microstep: 580.52 | bwd_inner_microstep: 561.50 | bwd_allreduce_microstep: 18.97 | step_microstep: 164.19
[2024-12-03 18:56:43,724] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.54 | bwd: 580.54 | bwd_inner: 561.50 | bwd_allreduce: 18.98 | step: 164.20
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:56:43Start of save checkpoint, device rank: 1, time: 2024-12-03 18:56:43

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:56:43
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:56:54,451] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step104 is about to be saved!
[2024-12-03 18:56:54,461] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-104/global_step104/mp_rank_00_model_states.pt
[2024-12-03 18:56:54,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-104/global_step104/mp_rank_00_model_states.pt...
[2024-12-03 18:57:04,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-104/global_step104/mp_rank_00_model_states.pt.
[2024-12-03 18:57:04,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-104/global_step104/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:57:27,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-104/global_step104/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:57:27,911] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-104/global_step104/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:57:27,912] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step104 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 18:57:27
End of save checkpoint, device rank: 1, time: 2024-12-03 18:57:27
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:57:35
[2024-12-03 18:57:36,446] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.41 | optimizer_gradients: 16.17 | optimizer_step: 66.26
[2024-12-03 18:57:36,447] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.74 | bwd_microstep: 586.14 | bwd_inner_microstep: 567.16 | bwd_allreduce_microstep: 18.92 | step_microstep: 163.70
[2024-12-03 18:57:36,447] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.72 | bwd: 586.15 | bwd_inner: 567.16 | bwd_allreduce: 18.94 | step: 163.71
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:57:36
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:57:36
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:57:36
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:57:47,322] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step105 is about to be saved!
[2024-12-03 18:57:47,331] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-105/global_step105/mp_rank_00_model_states.pt
[2024-12-03 18:57:47,331] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-105/global_step105/mp_rank_00_model_states.pt...
[2024-12-03 18:57:57,788] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-105/global_step105/mp_rank_00_model_states.pt.
[2024-12-03 18:57:57,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-105/global_step105/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:58:20,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-105/global_step105/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:58:20,575] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-105/global_step105/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:58:20,575] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step105 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 18:58:20
End of save checkpoint, device rank: 2, time: 2024-12-03 18:58:20
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 18:58:25
[2024-12-03 18:58:27,066] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.90 | optimizer_gradients: 16.17 | optimizer_step: 66.17
[2024-12-03 18:58:27,067] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 256.32 | bwd_microstep: 581.07 | bwd_inner_microstep: 563.22 | bwd_allreduce_microstep: 17.79 | step_microstep: 164.58
[2024-12-03 18:58:27,067] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 256.30 | bwd: 581.08 | bwd_inner: 563.22 | bwd_allreduce: 17.81 | step: 164.59
Start of save checkpoint, device rank: 2, time: 2024-12-03 18:58:27
Start of save checkpoint, device rank: 0, time: 2024-12-03 18:58:27
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:58:27
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:58:37,987] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step106 is about to be saved!
[2024-12-03 18:58:38,002] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-106/global_step106/mp_rank_00_model_states.pt
[2024-12-03 18:58:38,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-106/global_step106/mp_rank_00_model_states.pt...
[2024-12-03 18:58:48,552] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-106/global_step106/mp_rank_00_model_states.pt.
[2024-12-03 18:58:48,554] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-106/global_step106/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 18:59:11,399] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-106/global_step106/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 18:59:11,402] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-106/global_step106/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 18:59:11,402] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step106 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 18:59:11
End of save checkpoint, device rank: 1, time: 2024-12-03 18:59:11
End of save checkpoint, device rank: 0, time: 2024-12-03 18:59:17
[2024-12-03 18:59:18,188] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.73 | optimizer_gradients: 16.17 | optimizer_step: 66.18
[2024-12-03 18:59:18,189] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.05 | bwd_microstep: 582.06 | bwd_inner_microstep: 563.11 | bwd_allreduce_microstep: 18.91 | step_microstep: 163.82
[2024-12-03 18:59:18,189] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.03 | bwd: 582.07 | bwd_inner: 563.10 | bwd_allreduce: 18.92 | step: 163.82
Start of save checkpoint, device rank: 1, time: 2024-12-03 18:59:18Start of save checkpoint, device rank: 2, time: 2024-12-03 18:59:18

Start of save checkpoint, device rank: 0, time: 2024-12-03 18:59:18
is_deepspeed_enabledis_deepspeed_enabled
is_deepspeed_enabled

isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 18:59:29,197] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step107 is about to be saved!
[2024-12-03 18:59:29,212] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-107/global_step107/mp_rank_00_model_states.pt
[2024-12-03 18:59:29,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-107/global_step107/mp_rank_00_model_states.pt...
[2024-12-03 18:59:39,442] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-107/global_step107/mp_rank_00_model_states.pt.
[2024-12-03 18:59:39,444] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-107/global_step107/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:00:02,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-107/global_step107/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:00:02,325] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-107/global_step107/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:00:02,325] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step107 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:00:02
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 19:00:02
End of save checkpoint, device rank: 0, time: 2024-12-03 19:00:09
[2024-12-03 19:00:10,909] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.80 | optimizer_gradients: 16.18 | optimizer_step: 66.23
[2024-12-03 19:00:10,910] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.33 | bwd_microstep: 575.91 | bwd_inner_microstep: 556.94 | bwd_allreduce_microstep: 18.92 | step_microstep: 164.05
[2024-12-03 19:00:10,910] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.31 | bwd: 575.92 | bwd_inner: 556.94 | bwd_allreduce: 18.93 | step: 164.05
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:00:10
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:00:10
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:00:10
is_deepspeed_enabled
is_deepspeed_enabledis_deepspeed_enabled

isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:00:22,205] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step108 is about to be saved!
[2024-12-03 19:00:22,231] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-108/global_step108/mp_rank_00_model_states.pt
[2024-12-03 19:00:22,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-108/global_step108/mp_rank_00_model_states.pt...
[2024-12-03 19:00:33,036] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-108/global_step108/mp_rank_00_model_states.pt.
[2024-12-03 19:00:33,038] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-108/global_step108/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:00:56,449] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-108/global_step108/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:00:56,452] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-108/global_step108/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:00:56,452] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step108 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:00:56
End of save checkpoint, device rank: 2, time: 2024-12-03 19:00:56
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:01:03
[2024-12-03 19:01:05,123] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.57 | optimizer_gradients: 16.18 | optimizer_step: 66.25
[2024-12-03 19:01:05,124] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.43 | bwd_microstep: 588.09 | bwd_inner_microstep: 569.13 | bwd_allreduce_microstep: 18.91 | step_microstep: 163.67
[2024-12-03 19:01:05,124] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.41 | bwd: 588.10 | bwd_inner: 569.13 | bwd_allreduce: 18.93 | step: 163.68
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:01:05
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:01:05
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:01:05
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:01:15,978] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step109 is about to be saved!
[2024-12-03 19:01:15,993] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-109/global_step109/mp_rank_00_model_states.pt
[2024-12-03 19:01:15,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-109/global_step109/mp_rank_00_model_states.pt...
[2024-12-03 19:01:26,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-109/global_step109/mp_rank_00_model_states.pt.
[2024-12-03 19:01:26,757] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-109/global_step109/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:01:49,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-109/global_step109/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:01:49,184] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-109/global_step109/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:01:49,184] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step109 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:01:49
End of save checkpoint, device rank: 2, time: 2024-12-03 19:01:49
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:01:56
[2024-12-03 19:01:57,612] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 37.28 | optimizer_gradients: 16.18 | optimizer_step: 66.17
[2024-12-03 19:01:57,612] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.55 | bwd_microstep: 578.31 | bwd_inner_microstep: 559.41 | bwd_allreduce_microstep: 18.85 | step_microstep: 170.50
[2024-12-03 19:01:57,613] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.53 | bwd: 578.32 | bwd_inner: 559.40 | bwd_allreduce: 18.87 | step: 170.50
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:01:57
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:01:57
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:01:57
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:02:08,643] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step110 is about to be saved!
[2024-12-03 19:02:08,653] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-110/global_step110/mp_rank_00_model_states.pt
[2024-12-03 19:02:08,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-110/global_step110/mp_rank_00_model_states.pt...
[2024-12-03 19:02:18,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-110/global_step110/mp_rank_00_model_states.pt.
[2024-12-03 19:02:18,996] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-110/global_step110/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:02:41,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-110/global_step110/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:02:41,518] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-110/global_step110/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:02:41,518] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step110 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:02:42
End of save checkpoint, device rank: 2, time: 2024-12-03 19:02:42
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:02:47
[2024-12-03 19:02:48,635] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.63 | optimizer_gradients: 16.17 | optimizer_step: 66.19
[2024-12-03 19:02:48,636] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.59 | bwd_microstep: 586.09 | bwd_inner_microstep: 567.12 | bwd_allreduce_microstep: 18.92 | step_microstep: 163.70
[2024-12-03 19:02:48,636] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.57 | bwd: 586.10 | bwd_inner: 567.12 | bwd_allreduce: 18.94 | step: 163.70
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:02:48
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:02:48
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:02:48
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:03:01,365] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step111 is about to be saved!
[2024-12-03 19:03:01,378] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-111/global_step111/mp_rank_00_model_states.pt
[2024-12-03 19:03:01,378] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-111/global_step111/mp_rank_00_model_states.pt...
[2024-12-03 19:03:11,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-111/global_step111/mp_rank_00_model_states.pt.
[2024-12-03 19:03:11,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-111/global_step111/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:03:34,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-111/global_step111/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:03:34,522] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-111/global_step111/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:03:34,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step111 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:03:34
End of save checkpoint, device rank: 1, time: 2024-12-03 19:03:34
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:03:40
[2024-12-03 19:03:41,230] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.72 | optimizer_gradients: 16.17 | optimizer_step: 66.23
[2024-12-03 19:03:41,231] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 216.97 | bwd_microstep: 577.89 | bwd_inner_microstep: 558.74 | bwd_allreduce_microstep: 19.10 | step_microstep: 163.82
[2024-12-03 19:03:41,231] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 216.95 | bwd: 577.90 | bwd_inner: 558.74 | bwd_allreduce: 19.12 | step: 163.82
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:03:41
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:03:41
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:03:41
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:03:52,206] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step112 is about to be saved!
[2024-12-03 19:03:52,216] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-112/global_step112/mp_rank_00_model_states.pt
[2024-12-03 19:03:52,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-112/global_step112/mp_rank_00_model_states.pt...
[2024-12-03 19:04:02,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-112/global_step112/mp_rank_00_model_states.pt.
[2024-12-03 19:04:02,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-112/global_step112/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:04:25,137] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-112/global_step112/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:04:25,140] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-112/global_step112/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:04:25,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step112 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:04:25
End of save checkpoint, device rank: 1, time: 2024-12-03 19:04:25
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:04:31
[2024-12-03 19:04:32,296] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.80 | optimizer_gradients: 16.18 | optimizer_step: 66.18
[2024-12-03 19:04:32,296] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 204.60 | bwd_microstep: 578.09 | bwd_inner_microstep: 559.04 | bwd_allreduce_microstep: 19.00 | step_microstep: 163.86
[2024-12-03 19:04:32,296] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 204.58 | bwd: 578.10 | bwd_inner: 559.04 | bwd_allreduce: 19.02 | step: 163.87
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:04:32
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:04:32
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:04:32
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:04:43,166] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step113 is about to be saved!
[2024-12-03 19:04:43,180] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-113/global_step113/mp_rank_00_model_states.pt
[2024-12-03 19:04:43,180] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-113/global_step113/mp_rank_00_model_states.pt...
[2024-12-03 19:04:53,489] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-113/global_step113/mp_rank_00_model_states.pt.
[2024-12-03 19:04:53,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-113/global_step113/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:05:16,292] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-113/global_step113/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:05:16,295] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-113/global_step113/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:05:16,295] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step113 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:05:16
End of save checkpoint, device rank: 2, time: 2024-12-03 19:05:16
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:05:24
[2024-12-03 19:05:25,275] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.59 | optimizer_gradients: 16.17 | optimizer_step: 66.15
[2024-12-03 19:05:25,276] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.68 | bwd_microstep: 584.70 | bwd_inner_microstep: 565.67 | bwd_allreduce_microstep: 18.98 | step_microstep: 163.68
[2024-12-03 19:05:25,276] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.66 | bwd: 584.71 | bwd_inner: 565.67 | bwd_allreduce: 18.99 | step: 163.68
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:05:25
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:05:25
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:05:25
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:05:36,427] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step114 is about to be saved!
[2024-12-03 19:05:36,439] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-114/global_step114/mp_rank_00_model_states.pt
[2024-12-03 19:05:36,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-114/global_step114/mp_rank_00_model_states.pt...
[2024-12-03 19:05:46,649] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-114/global_step114/mp_rank_00_model_states.pt.
[2024-12-03 19:05:46,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-114/global_step114/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:06:09,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-114/global_step114/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:06:09,564] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-114/global_step114/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:06:09,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step114 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:06:09
End of save checkpoint, device rank: 2, time: 2024-12-03 19:06:09
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:06:15
[2024-12-03 19:06:16,606] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.77 | optimizer_gradients: 16.17 | optimizer_step: 66.25
[2024-12-03 19:06:16,607] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.82 | bwd_microstep: 579.10 | bwd_inner_microstep: 560.10 | bwd_allreduce_microstep: 18.95 | step_microstep: 163.89
[2024-12-03 19:06:16,607] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.80 | bwd: 579.11 | bwd_inner: 560.10 | bwd_allreduce: 18.96 | step: 163.89
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:06:16
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:06:16
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:06:16
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:06:27,773] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step115 is about to be saved!
[2024-12-03 19:06:27,782] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-115/global_step115/mp_rank_00_model_states.pt
[2024-12-03 19:06:27,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-115/global_step115/mp_rank_00_model_states.pt...
[2024-12-03 19:06:38,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-115/global_step115/mp_rank_00_model_states.pt.
[2024-12-03 19:06:38,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-115/global_step115/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:07:01,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-115/global_step115/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:07:01,191] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-115/global_step115/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:07:01,191] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step115 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:07:01
End of save checkpoint, device rank: 1, time: 2024-12-03 19:07:01
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:07:08
[2024-12-03 19:07:09,420] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 31.03 | optimizer_gradients: 16.19 | optimizer_step: 66.14
[2024-12-03 19:07:09,420] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.45 | bwd_microstep: 553.13 | bwd_inner_microstep: 534.10 | bwd_allreduce_microstep: 18.97 | step_microstep: 164.20
[2024-12-03 19:07:09,420] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.43 | bwd: 553.14 | bwd_inner: 534.10 | bwd_allreduce: 18.99 | step: 164.21
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:07:09
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:07:09
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:07:09
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:07:20,052] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step116 is about to be saved!
[2024-12-03 19:07:20,061] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-116/global_step116/mp_rank_00_model_states.pt
[2024-12-03 19:07:20,061] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-116/global_step116/mp_rank_00_model_states.pt...
[2024-12-03 19:07:31,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-116/global_step116/mp_rank_00_model_states.pt.
[2024-12-03 19:07:31,032] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-116/global_step116/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:07:54,278] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-116/global_step116/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:07:54,281] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-116/global_step116/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:07:54,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step116 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:07:54End of save checkpoint, device rank: 2, time: 2024-12-03 19:07:54

should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:08:00
[2024-12-03 19:08:01,970] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.59 | optimizer_gradients: 16.17 | optimizer_step: 66.28
[2024-12-03 19:08:01,970] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.01 | bwd_microstep: 583.82 | bwd_inner_microstep: 564.90 | bwd_allreduce_microstep: 18.87 | step_microstep: 163.86
[2024-12-03 19:08:01,970] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 216.99 | bwd: 583.83 | bwd_inner: 564.90 | bwd_allreduce: 18.88 | step: 163.87
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:08:01
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:08:01
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:08:01
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:08:12,676] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step117 is about to be saved!
[2024-12-03 19:08:12,684] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-117/global_step117/mp_rank_00_model_states.pt
[2024-12-03 19:08:12,684] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-117/global_step117/mp_rank_00_model_states.pt...
[2024-12-03 19:08:23,271] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-117/global_step117/mp_rank_00_model_states.pt.
[2024-12-03 19:08:23,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-117/global_step117/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:08:46,176] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-117/global_step117/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:08:46,179] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-117/global_step117/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:08:46,179] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step117 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:08:46
End of save checkpoint, device rank: 2, time: 2024-12-03 19:08:46
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:08:51
[2024-12-03 19:08:52,421] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.63 | optimizer_gradients: 16.17 | optimizer_step: 66.19
[2024-12-03 19:08:52,421] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.66 | bwd_microstep: 578.04 | bwd_inner_microstep: 559.08 | bwd_allreduce_microstep: 18.91 | step_microstep: 163.84
[2024-12-03 19:08:52,421] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.64 | bwd: 578.06 | bwd_inner: 559.08 | bwd_allreduce: 18.92 | step: 163.85
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:08:52
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:08:52
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:08:52
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:09:03,751] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step118 is about to be saved!
[2024-12-03 19:09:03,761] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-118/global_step118/mp_rank_00_model_states.pt
[2024-12-03 19:09:03,761] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-118/global_step118/mp_rank_00_model_states.pt...
[2024-12-03 19:09:14,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-118/global_step118/mp_rank_00_model_states.pt.
[2024-12-03 19:09:14,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-118/global_step118/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:09:37,294] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-118/global_step118/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:09:37,297] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-118/global_step118/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:09:37,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step118 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:09:37
End of save checkpoint, device rank: 2, time: 2024-12-03 19:09:37
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:09:42
[2024-12-03 19:09:43,751] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 36.71 | optimizer_gradients: 16.21 | optimizer_step: 66.19
[2024-12-03 19:09:43,751] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.83 | bwd_microstep: 576.52 | bwd_inner_microstep: 557.42 | bwd_allreduce_microstep: 19.04 | step_microstep: 169.81
[2024-12-03 19:09:43,752] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.81 | bwd: 576.53 | bwd_inner: 557.42 | bwd_allreduce: 19.06 | step: 169.82
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:09:43
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:09:43
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:09:43
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:09:54,393] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step119 is about to be saved!
[2024-12-03 19:09:54,402] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-119/global_step119/mp_rank_00_model_states.pt
[2024-12-03 19:09:54,402] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-119/global_step119/mp_rank_00_model_states.pt...
[2024-12-03 19:10:04,576] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-119/global_step119/mp_rank_00_model_states.pt.
[2024-12-03 19:10:04,578] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-119/global_step119/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:10:27,841] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-119/global_step119/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:10:27,844] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-119/global_step119/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:10:27,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step119 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:10:27
End of save checkpoint, device rank: 2, time: 2024-12-03 19:10:27
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:10:33
[2024-12-03 19:10:34,381] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.74 | optimizer_gradients: 16.17 | optimizer_step: 66.16
[2024-12-03 19:10:34,381] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.33 | bwd_microstep: 582.72 | bwd_inner_microstep: 563.58 | bwd_allreduce_microstep: 19.09 | step_microstep: 163.74
[2024-12-03 19:10:34,382] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.31 | bwd: 582.73 | bwd_inner: 563.58 | bwd_allreduce: 19.10 | step: 163.75
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:10:34
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:10:34
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:10:34
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:10:45,500] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step120 is about to be saved!
[2024-12-03 19:10:45,509] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-120/global_step120/mp_rank_00_model_states.pt
[2024-12-03 19:10:45,509] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-120/global_step120/mp_rank_00_model_states.pt...
[2024-12-03 19:10:55,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-120/global_step120/mp_rank_00_model_states.pt.
[2024-12-03 19:10:55,832] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-120/global_step120/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:11:18,618] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-120/global_step120/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:11:18,622] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-120/global_step120/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:11:18,623] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step120 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:11:18
End of save checkpoint, device rank: 1, time: 2024-12-03 19:11:18
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:11:24
[2024-12-03 19:11:25,481] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.72 | optimizer_gradients: 16.21 | optimizer_step: 66.16
[2024-12-03 19:11:25,482] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 209.18 | bwd_microstep: 578.60 | bwd_inner_microstep: 559.58 | bwd_allreduce_microstep: 18.96 | step_microstep: 163.76
[2024-12-03 19:11:25,482] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 209.16 | bwd: 578.61 | bwd_inner: 559.58 | bwd_allreduce: 18.98 | step: 163.77
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:11:25
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:11:25
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:11:25
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:11:37,109] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step121 is about to be saved!
[2024-12-03 19:11:37,118] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-121/global_step121/mp_rank_00_model_states.pt
[2024-12-03 19:11:37,118] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-121/global_step121/mp_rank_00_model_states.pt...
[2024-12-03 19:11:47,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-121/global_step121/mp_rank_00_model_states.pt.
[2024-12-03 19:11:47,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-121/global_step121/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:12:10,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-121/global_step121/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:12:10,067] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-121/global_step121/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:12:10,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step121 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:12:10
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 19:12:10
End of save checkpoint, device rank: 0, time: 2024-12-03 19:12:16
[2024-12-03 19:12:17,314] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.92 | optimizer_gradients: 16.22 | optimizer_step: 66.17
[2024-12-03 19:12:17,315] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 215.05 | bwd_microstep: 578.19 | bwd_inner_microstep: 558.87 | bwd_allreduce_microstep: 19.25 | step_microstep: 164.16
[2024-12-03 19:12:17,315] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 215.03 | bwd: 578.20 | bwd_inner: 558.87 | bwd_allreduce: 19.28 | step: 164.17
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:12:17
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:12:17
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:12:17
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:12:27,953] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step122 is about to be saved!
[2024-12-03 19:12:27,962] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-122/global_step122/mp_rank_00_model_states.pt
[2024-12-03 19:12:27,962] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-122/global_step122/mp_rank_00_model_states.pt...
[2024-12-03 19:12:38,406] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-122/global_step122/mp_rank_00_model_states.pt.
[2024-12-03 19:12:38,407] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-122/global_step122/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:13:01,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-122/global_step122/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:13:01,583] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-122/global_step122/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:13:01,583] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step122 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:13:01
End of save checkpoint, device rank: 1, time: 2024-12-03 19:13:01
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:13:07
[2024-12-03 19:13:08,643] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.54 | optimizer_gradients: 16.22 | optimizer_step: 66.25
[2024-12-03 19:13:08,643] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 203.24 | bwd_microstep: 577.27 | bwd_inner_microstep: 558.10 | bwd_allreduce_microstep: 19.11 | step_microstep: 163.73
[2024-12-03 19:13:08,643] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 203.22 | bwd: 577.28 | bwd_inner: 558.10 | bwd_allreduce: 19.13 | step: 163.74
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:13:08
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:13:08
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:13:08
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:13:19,411] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step123 is about to be saved!
[2024-12-03 19:13:19,420] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-123/global_step123/mp_rank_00_model_states.pt
[2024-12-03 19:13:19,420] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-123/global_step123/mp_rank_00_model_states.pt...
[2024-12-03 19:13:29,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-123/global_step123/mp_rank_00_model_states.pt.
[2024-12-03 19:13:29,656] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-123/global_step123/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:13:52,284] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-123/global_step123/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:13:52,287] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-123/global_step123/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:13:52,287] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step123 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:13:52
End of save checkpoint, device rank: 2, time: 2024-12-03 19:13:52
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:13:59
[2024-12-03 19:14:00,555] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.69 | optimizer_gradients: 16.18 | optimizer_step: 66.33
[2024-12-03 19:14:00,555] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.39 | bwd_microstep: 590.06 | bwd_inner_microstep: 571.05 | bwd_allreduce_microstep: 18.96 | step_microstep: 164.35
[2024-12-03 19:14:00,556] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.30 | bwd: 590.07 | bwd_inner: 571.05 | bwd_allreduce: 18.98 | step: 164.35
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:14:00
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:14:00
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:14:00
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:14:11,370] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step124 is about to be saved!
[2024-12-03 19:14:11,379] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-124/global_step124/mp_rank_00_model_states.pt
[2024-12-03 19:14:11,379] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-124/global_step124/mp_rank_00_model_states.pt...
[2024-12-03 19:14:22,176] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-124/global_step124/mp_rank_00_model_states.pt.
[2024-12-03 19:14:22,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-124/global_step124/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:14:44,952] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-124/global_step124/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:14:44,955] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-124/global_step124/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:14:44,955] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step124 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:14:44
End of save checkpoint, device rank: 2, time: 2024-12-03 19:14:44
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:14:50
[2024-12-03 19:14:51,403] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.91 | optimizer_gradients: 16.18 | optimizer_step: 66.10
[2024-12-03 19:14:51,404] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 201.95 | bwd_microstep: 583.77 | bwd_inner_microstep: 564.68 | bwd_allreduce_microstep: 19.04 | step_microstep: 163.85
[2024-12-03 19:14:51,404] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 201.93 | bwd: 583.78 | bwd_inner: 564.68 | bwd_allreduce: 19.05 | step: 163.85
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:14:51
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:14:51Start of save checkpoint, device rank: 1, time: 2024-12-03 19:14:51

is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:15:02,415] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step125 is about to be saved!
[2024-12-03 19:15:02,425] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-125/global_step125/mp_rank_00_model_states.pt
[2024-12-03 19:15:02,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-125/global_step125/mp_rank_00_model_states.pt...
[2024-12-03 19:15:12,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-125/global_step125/mp_rank_00_model_states.pt.
[2024-12-03 19:15:12,509] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-125/global_step125/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:15:34,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-125/global_step125/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:15:34,896] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-125/global_step125/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:15:34,896] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step125 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:15:35
End of save checkpoint, device rank: 2, time: 2024-12-03 19:15:35
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:15:41
[2024-12-03 19:15:42,968] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 55.35 | optimizer_gradients: 16.17 | optimizer_step: 66.19
[2024-12-03 19:15:42,968] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.32 | bwd_microstep: 579.38 | bwd_inner_microstep: 560.46 | bwd_allreduce_microstep: 18.87 | step_microstep: 188.46
[2024-12-03 19:15:42,968] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.30 | bwd: 579.40 | bwd_inner: 560.46 | bwd_allreduce: 18.89 | step: 188.47
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:15:42
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:15:42
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:15:42
is_deepspeed_enabled
is_deepspeed_enabledis_deepspeed_enabled

isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:15:53,712] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step126 is about to be saved!
[2024-12-03 19:15:53,722] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-126/global_step126/mp_rank_00_model_states.pt
[2024-12-03 19:15:53,722] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-126/global_step126/mp_rank_00_model_states.pt...
[2024-12-03 19:16:03,961] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-126/global_step126/mp_rank_00_model_states.pt.
[2024-12-03 19:16:03,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-126/global_step126/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:16:26,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-126/global_step126/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:16:26,757] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-126/global_step126/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:16:26,757] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step126 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:16:26
End of save checkpoint, device rank: 2, time: 2024-12-03 19:16:26
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:16:32
[2024-12-03 19:16:33,912] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.61 | optimizer_gradients: 16.20 | optimizer_step: 66.14
[2024-12-03 19:16:33,913] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 215.66 | bwd_microstep: 577.74 | bwd_inner_microstep: 558.67 | bwd_allreduce_microstep: 19.01 | step_microstep: 163.77
[2024-12-03 19:16:33,913] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 215.64 | bwd: 577.76 | bwd_inner: 558.67 | bwd_allreduce: 19.03 | step: 163.77
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:16:33Start of save checkpoint, device rank: 1, time: 2024-12-03 19:16:33

Start of save checkpoint, device rank: 0, time: 2024-12-03 19:16:33
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:16:44,331] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step127 is about to be saved!
[2024-12-03 19:16:44,343] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-127/global_step127/mp_rank_00_model_states.pt
[2024-12-03 19:16:44,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-127/global_step127/mp_rank_00_model_states.pt...
[2024-12-03 19:16:54,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-127/global_step127/mp_rank_00_model_states.pt.
[2024-12-03 19:16:54,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-127/global_step127/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:17:17,606] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-127/global_step127/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:17:17,609] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-127/global_step127/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:17:17,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step127 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:17:17
End of save checkpoint, device rank: 2, time: 2024-12-03 19:17:17
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:17:24
[2024-12-03 19:17:25,992] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.54 | optimizer_gradients: 16.17 | optimizer_step: 66.33
[2024-12-03 19:17:25,993] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 204.28 | bwd_microstep: 576.37 | bwd_inner_microstep: 557.38 | bwd_allreduce_microstep: 18.94 | step_microstep: 163.76
[2024-12-03 19:17:25,993] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 204.26 | bwd: 576.38 | bwd_inner: 557.38 | bwd_allreduce: 18.95 | step: 163.76
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:17:25Start of save checkpoint, device rank: 1, time: 2024-12-03 19:17:25

Start of save checkpoint, device rank: 0, time: 2024-12-03 19:17:25
is_deepspeed_enabledis_deepspeed_enabled
is_deepspeed_enabled

isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:17:37,043] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step128 is about to be saved!
[2024-12-03 19:17:37,052] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-128/global_step128/mp_rank_00_model_states.pt
[2024-12-03 19:17:37,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-128/global_step128/mp_rank_00_model_states.pt...
[2024-12-03 19:17:47,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-128/global_step128/mp_rank_00_model_states.pt.
[2024-12-03 19:17:47,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-128/global_step128/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:18:10,354] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-128/global_step128/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:18:10,359] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-128/global_step128/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:18:10,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step128 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 19:18:10End of save checkpoint, device rank: 2, time: 2024-12-03 19:18:10

End of save checkpoint, device rank: 0, time: 2024-12-03 19:18:15
[2024-12-03 19:18:16,984] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 31.21 | optimizer_gradients: 16.20 | optimizer_step: 67.18
[2024-12-03 19:18:16,984] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.79 | bwd_microstep: 582.84 | bwd_inner_microstep: 563.81 | bwd_allreduce_microstep: 18.97 | step_microstep: 208.52
[2024-12-03 19:18:16,984] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.61 | bwd: 582.85 | bwd_inner: 563.81 | bwd_allreduce: 18.99 | step: 208.53
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:18:16
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:18:16
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:18:16
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:18:28,071] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step129 is about to be saved!
[2024-12-03 19:18:28,090] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-129/global_step129/mp_rank_00_model_states.pt
[2024-12-03 19:18:28,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-129/global_step129/mp_rank_00_model_states.pt...
[2024-12-03 19:18:38,387] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-129/global_step129/mp_rank_00_model_states.pt.
[2024-12-03 19:18:38,389] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-129/global_step129/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:19:01,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-129/global_step129/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:19:01,226] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-129/global_step129/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:19:01,226] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step129 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:19:01
End of save checkpoint, device rank: 2, time: 2024-12-03 19:19:01
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:19:06
[2024-12-03 19:19:07,693] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.78 | optimizer_gradients: 16.17 | optimizer_step: 66.21
[2024-12-03 19:19:07,694] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 209.74 | bwd_microstep: 577.57 | bwd_inner_microstep: 558.55 | bwd_allreduce_microstep: 18.96 | step_microstep: 164.17
[2024-12-03 19:19:07,694] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 209.72 | bwd: 577.58 | bwd_inner: 558.55 | bwd_allreduce: 18.98 | step: 164.17
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:19:07
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:19:07
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:19:07
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:19:18,699] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step130 is about to be saved!
[2024-12-03 19:19:18,708] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-130/global_step130/mp_rank_00_model_states.pt
[2024-12-03 19:19:18,708] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-130/global_step130/mp_rank_00_model_states.pt...
[2024-12-03 19:19:28,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-130/global_step130/mp_rank_00_model_states.pt.
[2024-12-03 19:19:28,912] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-130/global_step130/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:19:51,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-130/global_step130/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:19:51,785] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-130/global_step130/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:19:51,785] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step130 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:19:52
End of save checkpoint, device rank: 2, time: 2024-12-03 19:19:52
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:19:59
[2024-12-03 19:20:01,189] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.71 | optimizer_gradients: 16.17 | optimizer_step: 66.20
[2024-12-03 19:20:01,190] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 266.43 | bwd_microstep: 551.07 | bwd_inner_microstep: 532.11 | bwd_allreduce_microstep: 18.91 | step_microstep: 163.98
[2024-12-03 19:20:01,190] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 266.41 | bwd: 551.09 | bwd_inner: 532.11 | bwd_allreduce: 18.93 | step: 163.99
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:20:01
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:20:01
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:20:01
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:20:12,013] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step131 is about to be saved!
[2024-12-03 19:20:12,022] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-131/global_step131/mp_rank_00_model_states.pt
[2024-12-03 19:20:12,022] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-131/global_step131/mp_rank_00_model_states.pt...
[2024-12-03 19:20:22,292] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-131/global_step131/mp_rank_00_model_states.pt.
[2024-12-03 19:20:22,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-131/global_step131/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:20:44,915] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-131/global_step131/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:20:44,918] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-131/global_step131/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:20:44,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step131 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:20:44
End of save checkpoint, device rank: 1, time: 2024-12-03 19:20:44
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:20:52
[2024-12-03 19:20:53,236] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.61 | optimizer_gradients: 16.17 | optimizer_step: 66.20
[2024-12-03 19:20:53,237] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 213.75 | bwd_microstep: 576.82 | bwd_inner_microstep: 557.74 | bwd_allreduce_microstep: 19.03 | step_microstep: 163.68
[2024-12-03 19:20:53,237] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 213.73 | bwd: 576.83 | bwd_inner: 557.74 | bwd_allreduce: 19.05 | step: 163.69
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:20:53
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:20:53
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:20:53
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:21:03,835] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step132 is about to be saved!
[2024-12-03 19:21:03,847] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-132/global_step132/mp_rank_00_model_states.pt
[2024-12-03 19:21:03,847] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-132/global_step132/mp_rank_00_model_states.pt...
[2024-12-03 19:21:14,418] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-132/global_step132/mp_rank_00_model_states.pt.
[2024-12-03 19:21:14,420] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-132/global_step132/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:21:36,944] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-132/global_step132/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:21:36,947] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-132/global_step132/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:21:36,948] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step132 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:21:37
End of save checkpoint, device rank: 1, time: 2024-12-03 19:21:37
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:21:43
[2024-12-03 19:21:44,172] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.57 | optimizer_gradients: 16.18 | optimizer_step: 66.18
[2024-12-03 19:21:44,173] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 200.36 | bwd_microstep: 580.00 | bwd_inner_microstep: 561.03 | bwd_allreduce_microstep: 18.92 | step_microstep: 163.64
[2024-12-03 19:21:44,173] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 200.34 | bwd: 580.01 | bwd_inner: 561.03 | bwd_allreduce: 18.93 | step: 163.64
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:21:44Start of save checkpoint, device rank: 1, time: 2024-12-03 19:21:44

Start of save checkpoint, device rank: 0, time: 2024-12-03 19:21:44
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:21:55,329] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step133 is about to be saved!
[2024-12-03 19:21:55,341] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-133/global_step133/mp_rank_00_model_states.pt
[2024-12-03 19:21:55,341] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-133/global_step133/mp_rank_00_model_states.pt...
[2024-12-03 19:22:05,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-133/global_step133/mp_rank_00_model_states.pt.
[2024-12-03 19:22:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-133/global_step133/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:22:28,549] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-133/global_step133/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:22:28,552] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-133/global_step133/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:22:28,552] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step133 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:22:28
End of save checkpoint, device rank: 2, time: 2024-12-03 19:22:28
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:22:34
[2024-12-03 19:22:35,752] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.96 | optimizer_gradients: 16.22 | optimizer_step: 66.18
[2024-12-03 19:22:35,753] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.41 | bwd_microstep: 587.50 | bwd_inner_microstep: 568.44 | bwd_allreduce_microstep: 19.01 | step_microstep: 164.52
[2024-12-03 19:22:35,753] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.39 | bwd: 587.51 | bwd_inner: 568.44 | bwd_allreduce: 19.03 | step: 164.52
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:22:35
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:22:35
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:22:35
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:22:46,745] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step134 is about to be saved!
[2024-12-03 19:22:46,755] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-134/global_step134/mp_rank_00_model_states.pt
[2024-12-03 19:22:46,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-134/global_step134/mp_rank_00_model_states.pt...
[2024-12-03 19:22:57,247] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-134/global_step134/mp_rank_00_model_states.pt.
[2024-12-03 19:22:57,249] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-134/global_step134/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:23:20,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-134/global_step134/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:23:20,170] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-134/global_step134/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:23:20,170] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step134 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:23:20
End of save checkpoint, device rank: 2, time: 2024-12-03 19:23:20
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:23:25
[2024-12-03 19:23:27,200] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 31.03 | optimizer_gradients: 16.18 | optimizer_step: 66.10
[2024-12-03 19:23:27,200] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 218.09 | bwd_microstep: 582.08 | bwd_inner_microstep: 563.00 | bwd_allreduce_microstep: 19.03 | step_microstep: 164.17
[2024-12-03 19:23:27,201] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 218.07 | bwd: 582.09 | bwd_inner: 563.00 | bwd_allreduce: 19.05 | step: 164.17
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:23:27
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:23:27Start of save checkpoint, device rank: 0, time: 2024-12-03 19:23:27

is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:23:38,294] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step135 is about to be saved!
[2024-12-03 19:23:38,303] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-135/global_step135/mp_rank_00_model_states.pt
[2024-12-03 19:23:38,303] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-135/global_step135/mp_rank_00_model_states.pt...
[2024-12-03 19:23:48,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-135/global_step135/mp_rank_00_model_states.pt.
[2024-12-03 19:23:48,709] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-135/global_step135/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:24:11,644] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-135/global_step135/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:24:11,647] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-135/global_step135/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:24:11,647] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step135 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:24:11
End of save checkpoint, device rank: 2, time: 2024-12-03 19:24:11
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:24:18
[2024-12-03 19:24:19,919] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.70 | optimizer_gradients: 16.17 | optimizer_step: 66.17
[2024-12-03 19:24:19,920] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.01 | bwd_microstep: 585.14 | bwd_inner_microstep: 566.15 | bwd_allreduce_microstep: 18.94 | step_microstep: 163.68
[2024-12-03 19:24:19,920] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 216.99 | bwd: 585.15 | bwd_inner: 566.15 | bwd_allreduce: 18.96 | step: 163.68
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:24:19
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:24:19
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:24:19
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:24:30,778] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step136 is about to be saved!
[2024-12-03 19:24:30,788] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-136/global_step136/mp_rank_00_model_states.pt
[2024-12-03 19:24:30,788] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-136/global_step136/mp_rank_00_model_states.pt...
[2024-12-03 19:24:41,108] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-136/global_step136/mp_rank_00_model_states.pt.
[2024-12-03 19:24:41,110] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-136/global_step136/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:25:03,986] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-136/global_step136/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:25:03,989] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-136/global_step136/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:25:03,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step136 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:25:03
End of save checkpoint, device rank: 1, time: 2024-12-03 19:25:03
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:25:09
[2024-12-03 19:25:10,526] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.76 | optimizer_gradients: 16.16 | optimizer_step: 66.16
[2024-12-03 19:25:10,527] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.96 | bwd_microstep: 596.55 | bwd_inner_microstep: 577.61 | bwd_allreduce_microstep: 18.89 | step_microstep: 163.98
[2024-12-03 19:25:10,527] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.94 | bwd: 596.56 | bwd_inner: 577.61 | bwd_allreduce: 18.91 | step: 163.98
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:25:10
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:25:10
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:25:10
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:25:21,495] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step137 is about to be saved!
[2024-12-03 19:25:21,512] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-137/global_step137/mp_rank_00_model_states.pt
[2024-12-03 19:25:21,512] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-137/global_step137/mp_rank_00_model_states.pt...
[2024-12-03 19:25:32,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-137/global_step137/mp_rank_00_model_states.pt.
[2024-12-03 19:25:32,117] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-137/global_step137/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:25:54,975] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-137/global_step137/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:25:54,978] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-137/global_step137/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:25:54,978] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step137 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:25:55
End of save checkpoint, device rank: 1, time: 2024-12-03 19:25:55
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:26:00
[2024-12-03 19:26:01,496] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.81 | optimizer_gradients: 16.17 | optimizer_step: 66.19
[2024-12-03 19:26:01,497] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.05 | bwd_microstep: 577.54 | bwd_inner_microstep: 559.49 | bwd_allreduce_microstep: 18.00 | step_microstep: 163.88
[2024-12-03 19:26:01,497] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.04 | bwd: 577.55 | bwd_inner: 559.49 | bwd_allreduce: 18.01 | step: 163.89
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:26:01
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:26:01
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:26:01
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:26:12,254] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step138 is about to be saved!
[2024-12-03 19:26:12,267] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-138/global_step138/mp_rank_00_model_states.pt
[2024-12-03 19:26:12,267] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-138/global_step138/mp_rank_00_model_states.pt...
[2024-12-03 19:26:22,501] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-138/global_step138/mp_rank_00_model_states.pt.
[2024-12-03 19:26:22,503] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-138/global_step138/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:26:45,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-138/global_step138/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:26:45,446] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-138/global_step138/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:26:45,447] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step138 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:26:45
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 19:26:45
End of save checkpoint, device rank: 0, time: 2024-12-03 19:26:53
[2024-12-03 19:26:54,212] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.87 | optimizer_gradients: 16.18 | optimizer_step: 66.20
[2024-12-03 19:26:54,212] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 201.63 | bwd_microstep: 577.65 | bwd_inner_microstep: 558.58 | bwd_allreduce_microstep: 19.02 | step_microstep: 163.86
[2024-12-03 19:26:54,213] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 201.61 | bwd: 577.66 | bwd_inner: 558.58 | bwd_allreduce: 19.04 | step: 163.87
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:26:54
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:26:54
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:26:54
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:27:05,062] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step139 is about to be saved!
[2024-12-03 19:27:05,076] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-139/global_step139/mp_rank_00_model_states.pt
[2024-12-03 19:27:05,077] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-139/global_step139/mp_rank_00_model_states.pt...
[2024-12-03 19:27:15,488] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-139/global_step139/mp_rank_00_model_states.pt.
[2024-12-03 19:27:15,490] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-139/global_step139/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:27:38,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-139/global_step139/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:27:38,734] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-139/global_step139/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:27:38,734] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step139 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:27:38
End of save checkpoint, device rank: 1, time: 2024-12-03 19:27:38
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:27:46
[2024-12-03 19:27:47,378] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.50 | optimizer_gradients: 16.17 | optimizer_step: 66.28
[2024-12-03 19:27:47,378] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.32 | bwd_microstep: 592.69 | bwd_inner_microstep: 573.72 | bwd_allreduce_microstep: 18.91 | step_microstep: 163.76
[2024-12-03 19:27:47,379] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.30 | bwd: 592.70 | bwd_inner: 573.72 | bwd_allreduce: 18.93 | step: 163.77
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:27:47
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:27:47
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:27:47
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:27:58,134] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step140 is about to be saved!
[2024-12-03 19:27:58,154] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-140/global_step140/mp_rank_00_model_states.pt
[2024-12-03 19:27:58,155] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-140/global_step140/mp_rank_00_model_states.pt...
[2024-12-03 19:28:08,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-140/global_step140/mp_rank_00_model_states.pt.
[2024-12-03 19:28:08,542] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-140/global_step140/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:28:31,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-140/global_step140/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:28:31,338] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-140/global_step140/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:28:31,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step140 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:28:31
End of save checkpoint, device rank: 1, time: 2024-12-03 19:28:31
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:28:38
[2024-12-03 19:28:39,548] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.90 | optimizer_gradients: 16.17 | optimizer_step: 66.17
[2024-12-03 19:28:39,549] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.49 | bwd_microstep: 577.53 | bwd_inner_microstep: 558.50 | bwd_allreduce_microstep: 18.97 | step_microstep: 164.01
[2024-12-03 19:28:39,549] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.47 | bwd: 577.54 | bwd_inner: 558.50 | bwd_allreduce: 18.99 | step: 164.02
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:28:39
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:28:39
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:28:39
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:28:50,315] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step141 is about to be saved!
[2024-12-03 19:28:50,324] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-141/global_step141/mp_rank_00_model_states.pt
[2024-12-03 19:28:50,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-141/global_step141/mp_rank_00_model_states.pt...
[2024-12-03 19:29:00,553] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-141/global_step141/mp_rank_00_model_states.pt.
[2024-12-03 19:29:00,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-141/global_step141/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:29:23,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-141/global_step141/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:29:23,447] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-141/global_step141/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:29:23,448] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step141 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:29:23
End of save checkpoint, device rank: 1, time: 2024-12-03 19:29:23
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:29:28
[2024-12-03 19:29:30,107] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 36.97 | optimizer_gradients: 16.19 | optimizer_step: 66.21
[2024-12-03 19:29:30,107] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.69 | bwd_microstep: 584.18 | bwd_inner_microstep: 565.02 | bwd_allreduce_microstep: 19.11 | step_microstep: 170.17
[2024-12-03 19:29:30,107] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.67 | bwd: 584.19 | bwd_inner: 565.02 | bwd_allreduce: 19.12 | step: 170.17
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:29:30Start of save checkpoint, device rank: 2, time: 2024-12-03 19:29:30

Start of save checkpoint, device rank: 0, time: 2024-12-03 19:29:30
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:29:41,086] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step142 is about to be saved!
[2024-12-03 19:29:41,097] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-142/global_step142/mp_rank_00_model_states.pt
[2024-12-03 19:29:41,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-142/global_step142/mp_rank_00_model_states.pt...
[2024-12-03 19:29:51,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-142/global_step142/mp_rank_00_model_states.pt.
[2024-12-03 19:29:51,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-142/global_step142/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:30:14,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-142/global_step142/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:30:14,901] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-142/global_step142/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:30:14,901] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step142 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:30:14
End of save checkpoint, device rank: 2, time: 2024-12-03 19:30:14
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:30:22
[2024-12-03 19:30:23,570] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.65 | optimizer_gradients: 16.17 | optimizer_step: 66.16
[2024-12-03 19:30:23,570] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 206.46 | bwd_microstep: 563.43 | bwd_inner_microstep: 544.38 | bwd_allreduce_microstep: 18.99 | step_microstep: 163.63
[2024-12-03 19:30:23,571] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 206.44 | bwd: 563.44 | bwd_inner: 544.38 | bwd_allreduce: 19.01 | step: 163.64
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:30:23
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:30:23
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:30:23
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:30:34,385] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step143 is about to be saved!
[2024-12-03 19:30:34,398] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-143/global_step143/mp_rank_00_model_states.pt
[2024-12-03 19:30:34,398] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-143/global_step143/mp_rank_00_model_states.pt...
[2024-12-03 19:30:44,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-143/global_step143/mp_rank_00_model_states.pt.
[2024-12-03 19:30:44,943] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-143/global_step143/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:31:07,980] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-143/global_step143/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:31:07,984] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-143/global_step143/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:31:07,984] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step143 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:31:07
End of save checkpoint, device rank: 2, time: 2024-12-03 19:31:07
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:31:13
[2024-12-03 19:31:14,603] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.40 | optimizer_gradients: 16.17 | optimizer_step: 66.27
[2024-12-03 19:31:14,604] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 201.94 | bwd_microstep: 583.58 | bwd_inner_microstep: 564.63 | bwd_allreduce_microstep: 18.89 | step_microstep: 163.45
[2024-12-03 19:31:14,604] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 201.92 | bwd: 583.59 | bwd_inner: 564.63 | bwd_allreduce: 18.91 | step: 163.46
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:31:14
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:31:14
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:31:14
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:31:25,680] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step144 is about to be saved!
[2024-12-03 19:31:25,695] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-144/global_step144/mp_rank_00_model_states.pt
[2024-12-03 19:31:25,696] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-144/global_step144/mp_rank_00_model_states.pt...
[2024-12-03 19:31:36,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-144/global_step144/mp_rank_00_model_states.pt.
[2024-12-03 19:31:36,363] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-144/global_step144/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:31:59,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-144/global_step144/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:31:59,567] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-144/global_step144/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:31:59,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step144 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:31:59
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 19:31:59
End of save checkpoint, device rank: 0, time: 2024-12-03 19:32:07
[2024-12-03 19:32:08,204] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.60 | optimizer_gradients: 16.17 | optimizer_step: 66.20
[2024-12-03 19:32:08,204] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.51 | bwd_microstep: 578.64 | bwd_inner_microstep: 560.78 | bwd_allreduce_microstep: 17.80 | step_microstep: 163.71
[2024-12-03 19:32:08,204] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.49 | bwd: 578.65 | bwd_inner: 560.78 | bwd_allreduce: 17.82 | step: 163.71
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:32:08
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:32:08
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:32:08
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:32:19,188] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step145 is about to be saved!
[2024-12-03 19:32:19,199] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-145/global_step145/mp_rank_00_model_states.pt
[2024-12-03 19:32:19,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-145/global_step145/mp_rank_00_model_states.pt...
[2024-12-03 19:32:29,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-145/global_step145/mp_rank_00_model_states.pt.
[2024-12-03 19:32:29,784] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-145/global_step145/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:32:52,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-145/global_step145/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:32:52,622] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-145/global_step145/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:32:52,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step145 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:32:52
End of save checkpoint, device rank: 1, time: 2024-12-03 19:32:52
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:32:59
[2024-12-03 19:33:00,449] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.92 | optimizer_gradients: 16.17 | optimizer_step: 66.24
[2024-12-03 19:33:00,450] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.31 | bwd_microstep: 581.46 | bwd_inner_microstep: 562.50 | bwd_allreduce_microstep: 18.90 | step_microstep: 164.02
[2024-12-03 19:33:00,450] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.29 | bwd: 581.47 | bwd_inner: 562.50 | bwd_allreduce: 18.92 | step: 164.03
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:33:00
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:33:00
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:33:00
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:33:11,218] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step146 is about to be saved!
[2024-12-03 19:33:11,230] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-146/global_step146/mp_rank_00_model_states.pt
[2024-12-03 19:33:11,230] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-146/global_step146/mp_rank_00_model_states.pt...
[2024-12-03 19:33:21,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-146/global_step146/mp_rank_00_model_states.pt.
[2024-12-03 19:33:21,541] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-146/global_step146/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:33:44,434] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-146/global_step146/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:33:44,438] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-146/global_step146/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:33:44,438] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step146 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:33:44
End of save checkpoint, device rank: 2, time: 2024-12-03 19:33:44
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:33:51
[2024-12-03 19:33:52,442] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.93 | optimizer_gradients: 16.18 | optimizer_step: 66.19
[2024-12-03 19:33:52,442] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 213.13 | bwd_microstep: 577.76 | bwd_inner_microstep: 558.64 | bwd_allreduce_microstep: 19.06 | step_microstep: 163.98
[2024-12-03 19:33:52,442] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 213.11 | bwd: 577.77 | bwd_inner: 558.64 | bwd_allreduce: 19.08 | step: 163.98
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:33:52
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:33:52
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:33:52
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:34:03,153] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step147 is about to be saved!
[2024-12-03 19:34:03,165] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-147/global_step147/mp_rank_00_model_states.pt
[2024-12-03 19:34:03,165] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-147/global_step147/mp_rank_00_model_states.pt...
[2024-12-03 19:34:13,611] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-147/global_step147/mp_rank_00_model_states.pt.
[2024-12-03 19:34:13,613] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-147/global_step147/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:34:36,239] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-147/global_step147/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:34:36,243] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-147/global_step147/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:34:36,243] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step147 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:34:36
End of save checkpoint, device rank: 2, time: 2024-12-03 19:34:36
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:34:42
[2024-12-03 19:34:43,983] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.48 | optimizer_gradients: 16.24 | optimizer_step: 66.21
[2024-12-03 19:34:43,984] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.57 | bwd_microstep: 583.66 | bwd_inner_microstep: 564.63 | bwd_allreduce_microstep: 18.98 | step_microstep: 163.92
[2024-12-03 19:34:43,984] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.55 | bwd: 583.67 | bwd_inner: 564.63 | bwd_allreduce: 18.99 | step: 163.92
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:34:43
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:34:43
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:34:43
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:34:55,226] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step148 is about to be saved!
[2024-12-03 19:34:55,241] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-148/global_step148/mp_rank_00_model_states.pt
[2024-12-03 19:34:55,241] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-148/global_step148/mp_rank_00_model_states.pt...
[2024-12-03 19:35:05,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-148/global_step148/mp_rank_00_model_states.pt.
[2024-12-03 19:35:05,784] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-148/global_step148/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:35:28,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-148/global_step148/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:35:28,065] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-148/global_step148/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:35:28,065] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step148 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:35:28
End of save checkpoint, device rank: 2, time: 2024-12-03 19:35:28
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:35:33
[2024-12-03 19:35:34,913] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.80 | optimizer_gradients: 16.19 | optimizer_step: 66.12
[2024-12-03 19:35:34,914] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 214.47 | bwd_microstep: 577.79 | bwd_inner_microstep: 558.64 | bwd_allreduce_microstep: 19.09 | step_microstep: 163.77
[2024-12-03 19:35:34,914] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 214.45 | bwd: 577.81 | bwd_inner: 558.64 | bwd_allreduce: 19.11 | step: 163.77
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:35:34
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:35:34
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:35:34
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:35:45,779] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step149 is about to be saved!
[2024-12-03 19:35:45,791] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-149/global_step149/mp_rank_00_model_states.pt
[2024-12-03 19:35:45,791] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-149/global_step149/mp_rank_00_model_states.pt...
[2024-12-03 19:35:56,019] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-149/global_step149/mp_rank_00_model_states.pt.
[2024-12-03 19:35:56,021] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-149/global_step149/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:36:18,986] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-149/global_step149/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:36:18,989] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-149/global_step149/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:36:18,990] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step149 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:36:18
End of save checkpoint, device rank: 2, time: 2024-12-03 19:36:18
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:36:26
[2024-12-03 19:36:27,783] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.76 | optimizer_gradients: 16.23 | optimizer_step: 66.24
[2024-12-03 19:36:27,784] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 215.72 | bwd_microstep: 577.11 | bwd_inner_microstep: 558.13 | bwd_allreduce_microstep: 18.93 | step_microstep: 164.02
[2024-12-03 19:36:27,784] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 215.70 | bwd: 577.12 | bwd_inner: 558.13 | bwd_allreduce: 18.94 | step: 164.03
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:36:27
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:36:27
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:36:27
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:36:38,793] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step150 is about to be saved!
[2024-12-03 19:36:38,802] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-150/global_step150/mp_rank_00_model_states.pt
[2024-12-03 19:36:38,802] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-150/global_step150/mp_rank_00_model_states.pt...
[2024-12-03 19:36:49,176] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-150/global_step150/mp_rank_00_model_states.pt.
[2024-12-03 19:36:49,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:37:12,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:37:12,192] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:37:12,192] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step150 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:37:12
End of save checkpoint, device rank: 2, time: 2024-12-03 19:37:12
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:37:17
[2024-12-03 19:37:18,533] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.77 | optimizer_gradients: 16.18 | optimizer_step: 66.17
[2024-12-03 19:37:18,533] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 202.85 | bwd_microstep: 583.16 | bwd_inner_microstep: 564.19 | bwd_allreduce_microstep: 18.92 | step_microstep: 163.79
[2024-12-03 19:37:18,534] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 202.83 | bwd: 583.18 | bwd_inner: 564.19 | bwd_allreduce: 18.94 | step: 163.79
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:37:18
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:37:18
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:37:18
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:37:29,473] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step151 is about to be saved!
[2024-12-03 19:37:29,483] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-151/global_step151/mp_rank_00_model_states.pt
[2024-12-03 19:37:29,483] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-151/global_step151/mp_rank_00_model_states.pt...
[2024-12-03 19:37:39,987] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-151/global_step151/mp_rank_00_model_states.pt.
[2024-12-03 19:37:39,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-151/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:38:02,770] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-151/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:38:02,773] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-151/global_step151/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:38:02,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step151 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:38:02
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 19:38:02
End of save checkpoint, device rank: 0, time: 2024-12-03 19:38:10
[2024-12-03 19:38:11,705] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.72 | optimizer_gradients: 16.20 | optimizer_step: 66.17
[2024-12-03 19:38:11,706] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.54 | bwd_microstep: 578.24 | bwd_inner_microstep: 560.19 | bwd_allreduce_microstep: 17.99 | step_microstep: 163.81
[2024-12-03 19:38:11,706] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.52 | bwd: 578.25 | bwd_inner: 560.19 | bwd_allreduce: 18.01 | step: 163.82
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:38:11
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:38:11
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:38:11
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:38:22,558] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step152 is about to be saved!
[2024-12-03 19:38:22,573] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-152/global_step152/mp_rank_00_model_states.pt
[2024-12-03 19:38:22,573] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-152/global_step152/mp_rank_00_model_states.pt...
[2024-12-03 19:38:32,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-152/global_step152/mp_rank_00_model_states.pt.
[2024-12-03 19:38:32,875] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-152/global_step152/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:38:55,329] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-152/global_step152/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:38:55,332] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-152/global_step152/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:38:55,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step152 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:38:56
End of save checkpoint, device rank: 1, time: 2024-12-03 19:38:56
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:39:03
[2024-12-03 19:39:04,430] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.95 | optimizer_gradients: 16.18 | optimizer_step: 66.14
[2024-12-03 19:39:04,431] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 212.58 | bwd_microstep: 576.27 | bwd_inner_microstep: 557.27 | bwd_allreduce_microstep: 18.95 | step_microstep: 164.22
[2024-12-03 19:39:04,431] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 212.56 | bwd: 576.28 | bwd_inner: 557.27 | bwd_allreduce: 18.96 | step: 164.23
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:39:04
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:39:04
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:39:04
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:39:15,409] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step153 is about to be saved!
[2024-12-03 19:39:15,418] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-153/global_step153/mp_rank_00_model_states.pt
[2024-12-03 19:39:15,418] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-153/global_step153/mp_rank_00_model_states.pt...
[2024-12-03 19:39:27,867] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-153/global_step153/mp_rank_00_model_states.pt.
[2024-12-03 19:39:27,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-153/global_step153/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:39:51,203] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-153/global_step153/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:39:51,206] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-153/global_step153/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:39:51,206] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step153 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:39:51
End of save checkpoint, device rank: 1, time: 2024-12-03 19:39:51
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:39:56
[2024-12-03 19:39:58,100] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.67 | optimizer_gradients: 16.17 | optimizer_step: 66.15
[2024-12-03 19:39:58,101] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 214.40 | bwd_microstep: 576.27 | bwd_inner_microstep: 557.33 | bwd_allreduce_microstep: 18.89 | step_microstep: 163.76
[2024-12-03 19:39:58,101] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 214.38 | bwd: 576.29 | bwd_inner: 557.33 | bwd_allreduce: 18.91 | step: 163.76
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:39:58
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:39:58
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:39:58
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:40:09,146] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step154 is about to be saved!
[2024-12-03 19:40:09,159] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-154/global_step154/mp_rank_00_model_states.pt
[2024-12-03 19:40:09,159] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-154/global_step154/mp_rank_00_model_states.pt...
[2024-12-03 19:40:19,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-154/global_step154/mp_rank_00_model_states.pt.
[2024-12-03 19:40:19,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-154/global_step154/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:40:42,648] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-154/global_step154/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:40:42,664] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-154/global_step154/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:40:42,664] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step154 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 19:40:42
End of save checkpoint, device rank: 2, time: 2024-12-03 19:40:42
End of save checkpoint, device rank: 0, time: 2024-12-03 19:40:47
[2024-12-03 19:40:48,956] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.78 | optimizer_gradients: 16.18 | optimizer_step: 66.13
[2024-12-03 19:40:48,956] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 216.30 | bwd_microstep: 577.51 | bwd_inner_microstep: 558.54 | bwd_allreduce_microstep: 18.92 | step_microstep: 164.19
[2024-12-03 19:40:48,957] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 216.28 | bwd: 577.52 | bwd_inner: 558.54 | bwd_allreduce: 18.93 | step: 164.20
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:40:48
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:40:48
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:40:48
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:40:59,883] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step155 is about to be saved!
[2024-12-03 19:40:59,893] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-155/global_step155/mp_rank_00_model_states.pt
[2024-12-03 19:40:59,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-155/global_step155/mp_rank_00_model_states.pt...
[2024-12-03 19:41:10,624] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-155/global_step155/mp_rank_00_model_states.pt.
[2024-12-03 19:41:10,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-155/global_step155/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:41:34,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-155/global_step155/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:41:34,231] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-155/global_step155/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:41:34,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step155 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:41:34
End of save checkpoint, device rank: 2, time: 2024-12-03 19:41:34
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:41:39
[2024-12-03 19:41:40,477] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 37.89 | optimizer_gradients: 16.17 | optimizer_step: 66.23
[2024-12-03 19:41:40,477] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 207.26 | bwd_microstep: 577.05 | bwd_inner_microstep: 558.12 | bwd_allreduce_microstep: 18.88 | step_microstep: 170.98
[2024-12-03 19:41:40,477] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 207.24 | bwd: 577.06 | bwd_inner: 558.12 | bwd_allreduce: 18.90 | step: 170.99
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:41:40
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:41:40
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:41:40
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:41:51,519] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step156 is about to be saved!
[2024-12-03 19:41:51,530] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-156/global_step156/mp_rank_00_model_states.pt
[2024-12-03 19:41:51,530] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-156/global_step156/mp_rank_00_model_states.pt...
[2024-12-03 19:42:01,780] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-156/global_step156/mp_rank_00_model_states.pt.
[2024-12-03 19:42:01,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-156/global_step156/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:42:24,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-156/global_step156/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:42:24,587] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-156/global_step156/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:42:24,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step156 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 19:42:24
End of save checkpoint, device rank: 1, time: 2024-12-03 19:42:24
End of save checkpoint, device rank: 0, time: 2024-12-03 19:42:31
[2024-12-03 19:42:32,334] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.67 | optimizer_gradients: 16.17 | optimizer_step: 66.28
[2024-12-03 19:42:32,335] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 208.03 | bwd_microstep: 577.18 | bwd_inner_microstep: 558.14 | bwd_allreduce_microstep: 19.00 | step_microstep: 163.95
[2024-12-03 19:42:32,335] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 208.01 | bwd: 577.20 | bwd_inner: 558.14 | bwd_allreduce: 19.01 | step: 163.96
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:42:32
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:42:32
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:42:32
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:42:43,424] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step157 is about to be saved!
[2024-12-03 19:42:43,436] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-157/global_step157/mp_rank_00_model_states.pt
[2024-12-03 19:42:43,436] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-157/global_step157/mp_rank_00_model_states.pt...
[2024-12-03 19:42:53,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-157/global_step157/mp_rank_00_model_states.pt.
[2024-12-03 19:42:53,715] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-157/global_step157/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:43:16,471] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-157/global_step157/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:43:16,474] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-157/global_step157/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:43:16,474] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step157 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:43:16
End of save checkpoint, device rank: 1, time: 2024-12-03 19:43:16
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:43:22
[2024-12-03 19:43:23,760] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.65 | optimizer_gradients: 16.18 | optimizer_step: 66.19
[2024-12-03 19:43:23,760] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 216.68 | bwd_microstep: 576.17 | bwd_inner_microstep: 557.17 | bwd_allreduce_microstep: 18.94 | step_microstep: 163.70
[2024-12-03 19:43:23,760] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 216.67 | bwd: 576.18 | bwd_inner: 557.17 | bwd_allreduce: 18.96 | step: 163.70
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:43:23
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:43:23
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:43:23
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:43:34,591] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step158 is about to be saved!
[2024-12-03 19:43:34,606] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-158/global_step158/mp_rank_00_model_states.pt
[2024-12-03 19:43:34,606] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-158/global_step158/mp_rank_00_model_states.pt...
[2024-12-03 19:43:45,186] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-158/global_step158/mp_rank_00_model_states.pt.
[2024-12-03 19:43:45,188] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-158/global_step158/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:44:08,108] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-158/global_step158/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:44:08,111] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-158/global_step158/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:44:08,111] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step158 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:44:08
End of save checkpoint, device rank: 2, time: 2024-12-03 19:44:08
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:44:15
[2024-12-03 19:44:16,216] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.91 | optimizer_gradients: 16.18 | optimizer_step: 66.18
[2024-12-03 19:44:16,217] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.33 | bwd_microstep: 578.34 | bwd_inner_microstep: 559.34 | bwd_allreduce_microstep: 18.94 | step_microstep: 163.94
[2024-12-03 19:44:16,217] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.31 | bwd: 578.35 | bwd_inner: 559.34 | bwd_allreduce: 18.96 | step: 163.95
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:44:16
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:44:16
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:44:16
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:44:27,241] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step159 is about to be saved!
[2024-12-03 19:44:27,255] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-159/global_step159/mp_rank_00_model_states.pt
[2024-12-03 19:44:27,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-159/global_step159/mp_rank_00_model_states.pt...
[2024-12-03 19:44:37,858] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-159/global_step159/mp_rank_00_model_states.pt.
[2024-12-03 19:44:37,859] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-159/global_step159/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:45:00,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-159/global_step159/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:45:00,149] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-159/global_step159/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:45:00,150] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step159 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:45:00
End of save checkpoint, device rank: 1, time: 2024-12-03 19:45:00
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:45:08
[2024-12-03 19:45:09,762] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.62 | optimizer_gradients: 16.17 | optimizer_step: 66.13
[2024-12-03 19:45:09,762] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 210.46 | bwd_microstep: 575.67 | bwd_inner_microstep: 556.78 | bwd_allreduce_microstep: 18.84 | step_microstep: 163.68
[2024-12-03 19:45:09,762] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 210.44 | bwd: 575.68 | bwd_inner: 556.78 | bwd_allreduce: 18.85 | step: 163.69
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:45:09Start of save checkpoint, device rank: 0, time: 2024-12-03 19:45:09

Start of save checkpoint, device rank: 2, time: 2024-12-03 19:45:09
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:45:20,554] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step160 is about to be saved!
[2024-12-03 19:45:20,564] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-160/global_step160/mp_rank_00_model_states.pt
[2024-12-03 19:45:20,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-160/global_step160/mp_rank_00_model_states.pt...
[2024-12-03 19:45:30,921] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-160/global_step160/mp_rank_00_model_states.pt.
[2024-12-03 19:45:30,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-160/global_step160/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:45:53,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-160/global_step160/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:45:53,577] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-160/global_step160/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:45:53,577] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step160 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:45:54
End of save checkpoint, device rank: 2, time: 2024-12-03 19:45:54should save

End of save checkpoint, device rank: 0, time: 2024-12-03 19:46:01
[2024-12-03 19:46:03,157] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.71 | optimizer_gradients: 16.17 | optimizer_step: 66.17
[2024-12-03 19:46:03,158] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.67 | bwd_microstep: 582.42 | bwd_inner_microstep: 563.50 | bwd_allreduce_microstep: 18.86 | step_microstep: 163.73
[2024-12-03 19:46:03,158] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.65 | bwd: 582.43 | bwd_inner: 563.50 | bwd_allreduce: 18.88 | step: 163.74
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:46:03Start of save checkpoint, device rank: 1, time: 2024-12-03 19:46:03

Start of save checkpoint, device rank: 0, time: 2024-12-03 19:46:03
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:46:13,884] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step161 is about to be saved!
[2024-12-03 19:46:13,896] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-161/global_step161/mp_rank_00_model_states.pt
[2024-12-03 19:46:13,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-161/global_step161/mp_rank_00_model_states.pt...
[2024-12-03 19:46:24,643] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-161/global_step161/mp_rank_00_model_states.pt.
[2024-12-03 19:46:24,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-161/global_step161/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:46:47,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-161/global_step161/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:46:47,540] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-161/global_step161/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:46:47,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step161 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:46:47
End of save checkpoint, device rank: 1, time: 2024-12-03 19:46:47
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:46:52
[2024-12-03 19:46:54,117] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.61 | optimizer_gradients: 16.17 | optimizer_step: 66.22
[2024-12-03 19:46:54,117] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.26 | bwd_microstep: 576.02 | bwd_inner_microstep: 556.97 | bwd_allreduce_microstep: 19.00 | step_microstep: 163.64
[2024-12-03 19:46:54,117] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.24 | bwd: 576.03 | bwd_inner: 556.97 | bwd_allreduce: 19.01 | step: 163.65
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:46:54
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:46:54
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:46:54
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:47:04,911] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step162 is about to be saved!
[2024-12-03 19:47:04,923] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-162/global_step162/mp_rank_00_model_states.pt
[2024-12-03 19:47:04,923] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-162/global_step162/mp_rank_00_model_states.pt...
[2024-12-03 19:47:15,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-162/global_step162/mp_rank_00_model_states.pt.
[2024-12-03 19:47:15,776] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-162/global_step162/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:47:38,795] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-162/global_step162/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:47:38,799] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-162/global_step162/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:47:38,799] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step162 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:47:38
End of save checkpoint, device rank: 2, time: 2024-12-03 19:47:38
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:47:44
[2024-12-03 19:47:45,325] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.77 | optimizer_gradients: 16.18 | optimizer_step: 66.28
[2024-12-03 19:47:45,326] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.67 | bwd_microstep: 577.43 | bwd_inner_microstep: 559.43 | bwd_allreduce_microstep: 17.95 | step_microstep: 163.87
[2024-12-03 19:47:45,326] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.65 | bwd: 577.44 | bwd_inner: 559.42 | bwd_allreduce: 17.97 | step: 163.87
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:47:45
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:47:45
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:47:45
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:47:56,526] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step163 is about to be saved!
[2024-12-03 19:47:56,536] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-163/global_step163/mp_rank_00_model_states.pt
[2024-12-03 19:47:56,536] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-163/global_step163/mp_rank_00_model_states.pt...
[2024-12-03 19:48:07,268] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-163/global_step163/mp_rank_00_model_states.pt.
[2024-12-03 19:48:07,270] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-163/global_step163/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:48:30,944] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-163/global_step163/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:48:30,947] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-163/global_step163/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:48:30,947] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step163 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:48:30
End of save checkpoint, device rank: 2, time: 2024-12-03 19:48:30
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:48:36
[2024-12-03 19:48:37,488] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 37.45 | optimizer_gradients: 16.18 | optimizer_step: 66.25
[2024-12-03 19:48:37,488] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 203.28 | bwd_microstep: 577.68 | bwd_inner_microstep: 558.65 | bwd_allreduce_microstep: 18.97 | step_microstep: 170.51
[2024-12-03 19:48:37,489] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 203.27 | bwd: 577.69 | bwd_inner: 558.65 | bwd_allreduce: 18.99 | step: 170.51
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:48:37
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:48:37
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:48:37
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:48:48,308] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step164 is about to be saved!
[2024-12-03 19:48:48,321] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-164/global_step164/mp_rank_00_model_states.pt
[2024-12-03 19:48:48,321] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-164/global_step164/mp_rank_00_model_states.pt...
[2024-12-03 19:48:58,577] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-164/global_step164/mp_rank_00_model_states.pt.
[2024-12-03 19:48:58,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-164/global_step164/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:49:21,177] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-164/global_step164/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:49:21,180] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-164/global_step164/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:49:21,180] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step164 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:49:21
End of save checkpoint, device rank: 2, time: 2024-12-03 19:49:21
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:49:27
[2024-12-03 19:49:28,943] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.64 | optimizer_gradients: 16.18 | optimizer_step: 66.23
[2024-12-03 19:49:28,943] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 204.52 | bwd_microstep: 575.71 | bwd_inner_microstep: 556.81 | bwd_allreduce_microstep: 18.85 | step_microstep: 163.76
[2024-12-03 19:49:28,943] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 204.50 | bwd: 575.72 | bwd_inner: 556.81 | bwd_allreduce: 18.87 | step: 163.76
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:49:28Start of save checkpoint, device rank: 2, time: 2024-12-03 19:49:28

Start of save checkpoint, device rank: 0, time: 2024-12-03 19:49:28
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:49:40,153] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step165 is about to be saved!
[2024-12-03 19:49:40,167] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-165/global_step165/mp_rank_00_model_states.pt
[2024-12-03 19:49:40,167] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-165/global_step165/mp_rank_00_model_states.pt...
[2024-12-03 19:49:50,609] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-165/global_step165/mp_rank_00_model_states.pt.
[2024-12-03 19:49:50,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-165/global_step165/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:50:13,490] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-165/global_step165/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:50:13,493] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-165/global_step165/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:50:13,493] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step165 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:50:13End of save checkpoint, device rank: 2, time: 2024-12-03 19:50:13

should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:50:20
[2024-12-03 19:50:21,818] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.63 | optimizer_gradients: 16.19 | optimizer_step: 66.18
[2024-12-03 19:50:21,818] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 215.49 | bwd_microstep: 577.46 | bwd_inner_microstep: 558.31 | bwd_allreduce_microstep: 19.09 | step_microstep: 163.63
[2024-12-03 19:50:21,818] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 215.47 | bwd: 577.47 | bwd_inner: 558.31 | bwd_allreduce: 19.11 | step: 163.63
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:50:21
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:50:21
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:50:21
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:50:32,451] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step166 is about to be saved!
[2024-12-03 19:50:32,460] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-166/global_step166/mp_rank_00_model_states.pt
[2024-12-03 19:50:32,460] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-166/global_step166/mp_rank_00_model_states.pt...
[2024-12-03 19:50:42,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-166/global_step166/mp_rank_00_model_states.pt.
[2024-12-03 19:50:42,785] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-166/global_step166/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:51:05,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-166/global_step166/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:51:05,856] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-166/global_step166/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:51:05,856] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step166 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:51:05
End of save checkpoint, device rank: 1, time: 2024-12-03 19:51:05
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:51:11
[2024-12-03 19:51:12,206] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.62 | optimizer_gradients: 16.22 | optimizer_step: 66.31
[2024-12-03 19:51:12,208] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.30 | bwd_microstep: 578.53 | bwd_inner_microstep: 559.48 | bwd_allreduce_microstep: 19.00 | step_microstep: 165.46
[2024-12-03 19:51:12,209] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.28 | bwd: 578.54 | bwd_inner: 559.48 | bwd_allreduce: 19.02 | step: 165.47
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:51:12
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:51:12
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:51:12
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:51:23,486] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step167 is about to be saved!
[2024-12-03 19:51:23,500] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-167/global_step167/mp_rank_00_model_states.pt
[2024-12-03 19:51:23,500] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-167/global_step167/mp_rank_00_model_states.pt...
[2024-12-03 19:51:33,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-167/global_step167/mp_rank_00_model_states.pt.
[2024-12-03 19:51:33,916] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-167/global_step167/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:51:56,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-167/global_step167/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:51:56,540] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-167/global_step167/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:51:56,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step167 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:51:56
End of save checkpoint, device rank: 1, time: 2024-12-03 19:51:56
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:52:03
[2024-12-03 19:52:05,033] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.96 | optimizer_gradients: 16.20 | optimizer_step: 66.19
[2024-12-03 19:52:05,034] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 213.86 | bwd_microstep: 596.04 | bwd_inner_microstep: 576.82 | bwd_allreduce_microstep: 19.15 | step_microstep: 164.14
[2024-12-03 19:52:05,034] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 213.85 | bwd: 596.05 | bwd_inner: 576.82 | bwd_allreduce: 19.18 | step: 164.15
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:52:05
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:52:05
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:52:05
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:52:15,769] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step168 is about to be saved!
[2024-12-03 19:52:15,784] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-168/global_step168/mp_rank_00_model_states.pt
[2024-12-03 19:52:15,784] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-168/global_step168/mp_rank_00_model_states.pt...
[2024-12-03 19:52:26,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-168/global_step168/mp_rank_00_model_states.pt.
[2024-12-03 19:52:26,089] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-168/global_step168/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:52:48,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-168/global_step168/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:52:48,642] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-168/global_step168/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:52:48,642] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step168 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:52:49
End of save checkpoint, device rank: 1, time: 2024-12-03 19:52:49
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:52:54
[2024-12-03 19:52:55,646] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.56 | optimizer_gradients: 16.19 | optimizer_step: 66.22
[2024-12-03 19:52:55,647] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 201.89 | bwd_microstep: 576.71 | bwd_inner_microstep: 557.70 | bwd_allreduce_microstep: 18.96 | step_microstep: 163.81
[2024-12-03 19:52:55,647] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 201.87 | bwd: 576.73 | bwd_inner: 557.70 | bwd_allreduce: 18.98 | step: 163.81
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:52:55
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:52:55
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:52:55
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:53:06,308] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step169 is about to be saved!
[2024-12-03 19:53:06,320] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-169/global_step169/mp_rank_00_model_states.pt
[2024-12-03 19:53:06,320] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-169/global_step169/mp_rank_00_model_states.pt...
[2024-12-03 19:53:16,818] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-169/global_step169/mp_rank_00_model_states.pt.
[2024-12-03 19:53:16,820] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-169/global_step169/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:53:39,956] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-169/global_step169/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:53:39,959] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-169/global_step169/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:53:39,959] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step169 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:53:39
End of save checkpoint, device rank: 1, time: 2024-12-03 19:53:39
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:53:46
[2024-12-03 19:53:47,711] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.57 | optimizer_gradients: 16.18 | optimizer_step: 66.16
[2024-12-03 19:53:47,712] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 204.41 | bwd_microstep: 575.89 | bwd_inner_microstep: 556.95 | bwd_allreduce_microstep: 18.89 | step_microstep: 163.62
[2024-12-03 19:53:47,712] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 204.39 | bwd: 575.91 | bwd_inner: 556.95 | bwd_allreduce: 18.91 | step: 163.63
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:53:47Start of save checkpoint, device rank: 0, time: 2024-12-03 19:53:47

Start of save checkpoint, device rank: 1, time: 2024-12-03 19:53:47
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:53:58,435] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step170 is about to be saved!
[2024-12-03 19:53:58,449] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-170/global_step170/mp_rank_00_model_states.pt
[2024-12-03 19:53:58,450] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-170/global_step170/mp_rank_00_model_states.pt...
[2024-12-03 19:54:09,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-170/global_step170/mp_rank_00_model_states.pt.
[2024-12-03 19:54:09,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-170/global_step170/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:54:31,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-170/global_step170/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:54:31,789] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-170/global_step170/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:54:31,790] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step170 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:54:32
End of save checkpoint, device rank: 1, time: 2024-12-03 19:54:32
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:54:37
[2024-12-03 19:54:38,322] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.76 | optimizer_gradients: 16.17 | optimizer_step: 66.19
[2024-12-03 19:54:38,322] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 213.53 | bwd_microstep: 577.32 | bwd_inner_microstep: 558.28 | bwd_allreduce_microstep: 18.98 | step_microstep: 163.83
[2024-12-03 19:54:38,322] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 213.51 | bwd: 577.33 | bwd_inner: 558.28 | bwd_allreduce: 19.00 | step: 163.83
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:54:38
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:54:38
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:54:38
is_deepspeed_enabled
is_deepspeed_enabled
is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:54:49,261] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step171 is about to be saved!
[2024-12-03 19:54:49,274] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-171/global_step171/mp_rank_00_model_states.pt
[2024-12-03 19:54:49,274] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-171/global_step171/mp_rank_00_model_states.pt...
[2024-12-03 19:54:59,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-171/global_step171/mp_rank_00_model_states.pt.
[2024-12-03 19:54:59,523] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-171/global_step171/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:55:22,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-171/global_step171/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:55:22,423] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-171/global_step171/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:55:22,423] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step171 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:55:22
End of save checkpoint, device rank: 1, time: 2024-12-03 19:55:22
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:55:27
[2024-12-03 19:55:28,593] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 37.39 | optimizer_gradients: 16.17 | optimizer_step: 66.19
[2024-12-03 19:55:28,593] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 213.49 | bwd_microstep: 577.68 | bwd_inner_microstep: 558.67 | bwd_allreduce_microstep: 18.95 | step_microstep: 170.53
[2024-12-03 19:55:28,593] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 213.48 | bwd: 577.69 | bwd_inner: 558.67 | bwd_allreduce: 18.97 | step: 170.54
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:55:28
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:55:28
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:55:28
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:55:39,707] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step172 is about to be saved!
[2024-12-03 19:55:39,721] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-172/global_step172/mp_rank_00_model_states.pt
[2024-12-03 19:55:39,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-172/global_step172/mp_rank_00_model_states.pt...
[2024-12-03 19:55:50,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-172/global_step172/mp_rank_00_model_states.pt.
[2024-12-03 19:55:50,016] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-172/global_step172/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:56:12,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-172/global_step172/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:56:12,993] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-172/global_step172/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:56:12,993] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step172 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:56:12
End of save checkpoint, device rank: 1, time: 2024-12-03 19:56:12
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:56:19
[2024-12-03 19:56:20,720] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.78 | optimizer_gradients: 16.17 | optimizer_step: 66.19
[2024-12-03 19:56:20,721] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 202.94 | bwd_microstep: 577.79 | bwd_inner_microstep: 558.86 | bwd_allreduce_microstep: 18.87 | step_microstep: 163.96
[2024-12-03 19:56:20,721] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 202.92 | bwd: 577.80 | bwd_inner: 558.86 | bwd_allreduce: 18.89 | step: 163.96
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:56:20
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:56:20
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:56:20
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:56:31,656] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step173 is about to be saved!
[2024-12-03 19:56:31,667] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-173/global_step173/mp_rank_00_model_states.pt
[2024-12-03 19:56:31,667] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-173/global_step173/mp_rank_00_model_states.pt...
[2024-12-03 19:56:41,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-173/global_step173/mp_rank_00_model_states.pt.
[2024-12-03 19:56:41,877] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-173/global_step173/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:57:05,159] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-173/global_step173/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:57:05,166] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-173/global_step173/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:57:05,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step173 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 19:57:05
End of save checkpoint, device rank: 1, time: 2024-12-03 19:57:05
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:57:10
[2024-12-03 19:57:11,761] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.61 | optimizer_gradients: 16.17 | optimizer_step: 66.18
[2024-12-03 19:57:11,762] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.42 | bwd_microstep: 576.45 | bwd_inner_microstep: 557.56 | bwd_allreduce_microstep: 18.84 | step_microstep: 163.77
[2024-12-03 19:57:11,762] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.40 | bwd: 576.46 | bwd_inner: 557.56 | bwd_allreduce: 18.85 | step: 163.77
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:57:11
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:57:11
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:57:11
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:57:23,001] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step174 is about to be saved!
[2024-12-03 19:57:23,017] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-174/global_step174/mp_rank_00_model_states.pt
[2024-12-03 19:57:23,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-174/global_step174/mp_rank_00_model_states.pt...
[2024-12-03 19:57:33,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-174/global_step174/mp_rank_00_model_states.pt.
[2024-12-03 19:57:33,363] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-174/global_step174/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:57:56,285] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-174/global_step174/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:57:56,288] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-174/global_step174/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:57:56,288] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step174 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:57:56
End of save checkpoint, device rank: 2, time: 2024-12-03 19:57:56
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:58:01
[2024-12-03 19:58:02,642] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.47 | optimizer_gradients: 16.23 | optimizer_step: 66.17
[2024-12-03 19:58:02,642] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 203.20 | bwd_microstep: 576.20 | bwd_inner_microstep: 557.23 | bwd_allreduce_microstep: 18.92 | step_microstep: 163.59
[2024-12-03 19:58:02,642] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 203.18 | bwd: 576.22 | bwd_inner: 557.23 | bwd_allreduce: 18.94 | step: 163.59
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:58:02Start of save checkpoint, device rank: 1, time: 2024-12-03 19:58:02

Start of save checkpoint, device rank: 0, time: 2024-12-03 19:58:02
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:58:13,649] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step175 is about to be saved!
[2024-12-03 19:58:13,666] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-175/global_step175/mp_rank_00_model_states.pt
[2024-12-03 19:58:13,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-175/global_step175/mp_rank_00_model_states.pt...
[2024-12-03 19:58:23,997] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-175/global_step175/mp_rank_00_model_states.pt.
[2024-12-03 19:58:24,000] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-175/global_step175/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:58:46,960] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-175/global_step175/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:58:46,964] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-175/global_step175/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:58:46,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step175 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:58:47
End of save checkpoint, device rank: 2, time: 2024-12-03 19:58:47
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:58:52
[2024-12-03 19:58:53,482] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.47 | optimizer_gradients: 16.23 | optimizer_step: 66.29
[2024-12-03 19:58:53,482] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 192.27 | bwd_microstep: 552.96 | bwd_inner_microstep: 533.91 | bwd_allreduce_microstep: 18.99 | step_microstep: 163.92
[2024-12-03 19:58:53,483] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 192.25 | bwd: 552.97 | bwd_inner: 533.91 | bwd_allreduce: 19.01 | step: 163.92
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:58:53
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:58:53
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:58:53
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:59:04,574] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step176 is about to be saved!
[2024-12-03 19:59:04,583] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-176/global_step176/mp_rank_00_model_states.pt
[2024-12-03 19:59:04,583] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-176/global_step176/mp_rank_00_model_states.pt...
[2024-12-03 19:59:14,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-176/global_step176/mp_rank_00_model_states.pt.
[2024-12-03 19:59:14,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-176/global_step176/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 19:59:37,904] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-176/global_step176/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 19:59:37,907] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-176/global_step176/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 19:59:37,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step176 is ready now!
End of save checkpoint, device rank: 1, time: 2024-12-03 19:59:38
End of save checkpoint, device rank: 2, time: 2024-12-03 19:59:38
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 19:59:46
[2024-12-03 19:59:47,562] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.80 | optimizer_gradients: 16.19 | optimizer_step: 66.11
[2024-12-03 19:59:47,563] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 209.48 | bwd_microstep: 577.55 | bwd_inner_microstep: 558.30 | bwd_allreduce_microstep: 19.18 | step_microstep: 164.02
[2024-12-03 19:59:47,563] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 209.46 | bwd: 577.56 | bwd_inner: 558.30 | bwd_allreduce: 19.20 | step: 164.03
Start of save checkpoint, device rank: 2, time: 2024-12-03 19:59:47
Start of save checkpoint, device rank: 1, time: 2024-12-03 19:59:47
Start of save checkpoint, device rank: 0, time: 2024-12-03 19:59:47
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 19:59:58,356] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step177 is about to be saved!
[2024-12-03 19:59:58,365] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-177/global_step177/mp_rank_00_model_states.pt
[2024-12-03 19:59:58,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-177/global_step177/mp_rank_00_model_states.pt...
[2024-12-03 20:00:08,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-177/global_step177/mp_rank_00_model_states.pt.
[2024-12-03 20:00:08,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-177/global_step177/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:00:31,501] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-177/global_step177/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:00:31,504] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-177/global_step177/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:00:31,504] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step177 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 20:00:31
End of save checkpoint, device rank: 1, time: 2024-12-03 20:00:31
should save
End of save checkpoint, device rank: 0, time: 2024-12-03 20:00:37
[2024-12-03 20:00:38,220] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.69 | optimizer_gradients: 16.20 | optimizer_step: 66.23
[2024-12-03 20:00:38,220] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 209.89 | bwd_microstep: 577.21 | bwd_inner_microstep: 557.95 | bwd_allreduce_microstep: 19.19 | step_microstep: 163.84
[2024-12-03 20:00:38,220] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 209.87 | bwd: 577.23 | bwd_inner: 557.95 | bwd_allreduce: 19.22 | step: 163.85
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:00:38
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:00:38
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:00:38
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:00:48,966] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step178 is about to be saved!
[2024-12-03 20:00:48,975] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-178/global_step178/mp_rank_00_model_states.pt
[2024-12-03 20:00:48,975] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-178/global_step178/mp_rank_00_model_states.pt...
[2024-12-03 20:00:59,915] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-178/global_step178/mp_rank_00_model_states.pt.
[2024-12-03 20:00:59,917] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-178/global_step178/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:01:22,701] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-178/global_step178/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:01:22,704] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-178/global_step178/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:01:22,704] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step178 is ready now!
End of save checkpoint, device rank: 2, time: 2024-12-03 20:01:22End of save checkpoint, device rank: 1, time: 2024-12-03 20:01:22

should save
End of save checkpoint, device rank: 0, time: 2024-12-03 20:01:27
[2024-12-03 20:01:29,043] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.83 | optimizer_gradients: 16.19 | optimizer_step: 66.17
[2024-12-03 20:01:29,043] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 217.30 | bwd_microstep: 576.90 | bwd_inner_microstep: 557.81 | bwd_allreduce_microstep: 19.03 | step_microstep: 164.00
[2024-12-03 20:01:29,043] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 217.28 | bwd: 576.91 | bwd_inner: 557.81 | bwd_allreduce: 19.05 | step: 164.00
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:01:29
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:01:29
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:01:29
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:01:38,574] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step179 is about to be saved!
[2024-12-03 20:01:38,586] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-179/global_step179/mp_rank_00_model_states.pt
[2024-12-03 20:01:38,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-179/global_step179/mp_rank_00_model_states.pt...
[2024-12-03 20:01:47,963] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-179/global_step179/mp_rank_00_model_states.pt.
[2024-12-03 20:01:47,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-179/global_step179/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:02:10,077] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-179/global_step179/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:02:10,082] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-179/global_step179/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:02:10,082] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step179 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 20:02:10End of save checkpoint, device rank: 1, time: 2024-12-03 20:02:10

End of save checkpoint, device rank: 0, time: 2024-12-03 20:02:10
[2024-12-03 20:02:11,856] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.73 | optimizer_gradients: 16.20 | optimizer_step: 66.18
[2024-12-03 20:02:11,856] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 423.96 | bwd_microstep: 584.86 | bwd_inner_microstep: 566.77 | bwd_allreduce_microstep: 18.03 | step_microstep: 233.08
[2024-12-03 20:02:11,856] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 423.93 | bwd: 584.87 | bwd_inner: 566.77 | bwd_allreduce: 18.05 | step: 233.09
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:02:11
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:02:11
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:02:11
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:02:21,693] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step180 is about to be saved!
[2024-12-03 20:02:21,797] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-180/global_step180/mp_rank_00_model_states.pt
[2024-12-03 20:02:21,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-180/global_step180/mp_rank_00_model_states.pt...
[2024-12-03 20:02:31,484] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-180/global_step180/mp_rank_00_model_states.pt.
[2024-12-03 20:02:31,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-180/global_step180/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:02:56,194] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-180/global_step180/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:02:56,200] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-180/global_step180/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:02:56,200] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step180 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 20:02:56End of save checkpoint, device rank: 2, time: 2024-12-03 20:02:56

End of save checkpoint, device rank: 0, time: 2024-12-03 20:02:56
[2024-12-03 20:02:57,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.67 | optimizer_gradients: 16.18 | optimizer_step: 66.17
[2024-12-03 20:02:57,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 378.18 | bwd_microstep: 610.84 | bwd_inner_microstep: 572.07 | bwd_allreduce_microstep: 38.72 | step_microstep: 196.65
[2024-12-03 20:02:57,691] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 378.15 | bwd: 610.85 | bwd_inner: 572.07 | bwd_allreduce: 38.73 | step: 196.65
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:02:57
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:02:57
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:02:57
is_deepspeed_enabled
is_deepspeed_enabledis_deepspeed_enabled

isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:03:09,430] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step181 is about to be saved!
[2024-12-03 20:03:09,489] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-181/global_step181/mp_rank_00_model_states.pt
[2024-12-03 20:03:09,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-181/global_step181/mp_rank_00_model_states.pt...
[2024-12-03 20:03:20,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-181/global_step181/mp_rank_00_model_states.pt.
[2024-12-03 20:03:20,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-181/global_step181/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:03:45,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-181/global_step181/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:03:45,613] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-181/global_step181/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:03:45,614] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step181 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 20:03:45
End of save checkpoint, device rank: 2, time: 2024-12-03 20:03:45
End of save checkpoint, device rank: 0, time: 2024-12-03 20:03:45
[2024-12-03 20:03:47,198] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 33.68 | optimizer_gradients: 16.20 | optimizer_step: 66.30
[2024-12-03 20:03:47,198] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 431.12 | bwd_microstep: 577.53 | bwd_inner_microstep: 558.48 | bwd_allreduce_microstep: 19.00 | step_microstep: 186.62
[2024-12-03 20:03:47,198] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 431.10 | bwd: 577.55 | bwd_inner: 558.48 | bwd_allreduce: 19.02 | step: 186.63
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:03:47
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:03:47
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:03:47
is_deepspeed_enabledis_deepspeed_enabled
is_deepspeed_enabled

isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:03:59,083] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step182 is about to be saved!
[2024-12-03 20:03:59,143] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-182/global_step182/mp_rank_00_model_states.pt
[2024-12-03 20:03:59,143] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-182/global_step182/mp_rank_00_model_states.pt...
[2024-12-03 20:04:09,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-182/global_step182/mp_rank_00_model_states.pt.
[2024-12-03 20:04:09,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-182/global_step182/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:04:34,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-182/global_step182/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:04:34,131] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-182/global_step182/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:04:34,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step182 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 20:04:34End of save checkpoint, device rank: 1, time: 2024-12-03 20:04:34

End of save checkpoint, device rank: 0, time: 2024-12-03 20:04:34
[2024-12-03 20:04:35,721] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.87 | optimizer_gradients: 16.18 | optimizer_step: 66.18
[2024-12-03 20:04:35,722] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 322.60 | bwd_microstep: 582.85 | bwd_inner_microstep: 563.80 | bwd_allreduce_microstep: 19.00 | step_microstep: 226.80
[2024-12-03 20:04:35,722] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 322.58 | bwd: 582.86 | bwd_inner: 563.80 | bwd_allreduce: 19.01 | step: 226.80
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:04:35
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:04:35
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:04:35
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:04:48,261] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step183 is about to be saved!
[2024-12-03 20:04:48,339] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-183/global_step183/mp_rank_00_model_states.pt
[2024-12-03 20:04:48,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-183/global_step183/mp_rank_00_model_states.pt...
[2024-12-03 20:04:59,102] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-183/global_step183/mp_rank_00_model_states.pt.
[2024-12-03 20:04:59,161] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-183/global_step183/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:05:22,557] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-183/global_step183/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:05:22,563] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-183/global_step183/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:05:22,566] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step183 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 20:05:24End of save checkpoint, device rank: 1, time: 2024-12-03 20:05:24

End of save checkpoint, device rank: 0, time: 2024-12-03 20:05:24
[2024-12-03 20:05:26,296] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.63 | optimizer_gradients: 16.18 | optimizer_step: 66.24
[2024-12-03 20:05:26,296] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 383.54 | bwd_microstep: 610.81 | bwd_inner_microstep: 576.37 | bwd_allreduce_microstep: 34.39 | step_microstep: 183.32
[2024-12-03 20:05:26,296] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 383.51 | bwd: 610.82 | bwd_inner: 576.37 | bwd_allreduce: 34.40 | step: 183.33
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:05:26
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:05:26
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:05:26
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:05:40,021] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step184 is about to be saved!
[2024-12-03 20:05:40,080] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-184/global_step184/mp_rank_00_model_states.pt
[2024-12-03 20:05:40,080] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-184/global_step184/mp_rank_00_model_states.pt...
[2024-12-03 20:05:51,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-184/global_step184/mp_rank_00_model_states.pt.
[2024-12-03 20:05:51,282] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-184/global_step184/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:06:18,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-184/global_step184/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:06:18,669] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-184/global_step184/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:06:18,669] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step184 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 20:06:18End of save checkpoint, device rank: 2, time: 2024-12-03 20:06:18

End of save checkpoint, device rank: 0, time: 2024-12-03 20:06:18
[2024-12-03 20:06:20,184] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.67 | optimizer_gradients: 16.17 | optimizer_step: 66.24
[2024-12-03 20:06:20,184] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 408.06 | bwd_microstep: 577.28 | bwd_inner_microstep: 559.31 | bwd_allreduce_microstep: 17.91 | step_microstep: 182.68
[2024-12-03 20:06:20,185] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 408.03 | bwd: 577.29 | bwd_inner: 559.31 | bwd_allreduce: 17.93 | step: 182.68
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:06:20
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:06:20
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:06:20
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:06:35,424] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step185 is about to be saved!
[2024-12-03 20:06:35,484] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-185/global_step185/mp_rank_00_model_states.pt
[2024-12-03 20:06:35,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-185/global_step185/mp_rank_00_model_states.pt...
[2024-12-03 20:06:47,300] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-185/global_step185/mp_rank_00_model_states.pt.
[2024-12-03 20:06:47,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-185/global_step185/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:07:14,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-185/global_step185/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:07:14,281] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-185/global_step185/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:07:14,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step185 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 20:07:14
End of save checkpoint, device rank: 1, time: 2024-12-03 20:07:14
End of save checkpoint, device rank: 0, time: 2024-12-03 20:07:14
[2024-12-03 20:07:15,681] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.64 | optimizer_gradients: 16.18 | optimizer_step: 66.15
[2024-12-03 20:07:15,681] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 316.04 | bwd_microstep: 568.77 | bwd_inner_microstep: 539.42 | bwd_allreduce_microstep: 29.29 | step_microstep: 213.25
[2024-12-03 20:07:15,681] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 316.02 | bwd: 568.78 | bwd_inner: 539.42 | bwd_allreduce: 29.31 | step: 213.26
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:07:15
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:07:15
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:07:15
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:07:29,603] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step186 is about to be saved!
[2024-12-03 20:07:29,665] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-186/global_step186/mp_rank_00_model_states.pt
[2024-12-03 20:07:29,665] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-186/global_step186/mp_rank_00_model_states.pt...
[2024-12-03 20:07:42,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-186/global_step186/mp_rank_00_model_states.pt.
[2024-12-03 20:07:42,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-186/global_step186/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:08:08,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-186/global_step186/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:08:08,615] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-186/global_step186/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:08:08,618] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step186 is ready now!
should save
End of save checkpoint, device rank: 2, time: 2024-12-03 20:08:10End of save checkpoint, device rank: 1, time: 2024-12-03 20:08:10

End of save checkpoint, device rank: 0, time: 2024-12-03 20:08:10
[2024-12-03 20:08:13,398] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.77 | optimizer_gradients: 16.17 | optimizer_step: 67.28
[2024-12-03 20:08:13,399] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 1076.15 | bwd_microstep: 1473.85 | bwd_inner_microstep: 1451.22 | bwd_allreduce_microstep: 21.42 | step_microstep: 190.11
[2024-12-03 20:08:13,399] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 1076.12 | bwd: 1472.73 | bwd_inner: 1451.20 | bwd_allreduce: 21.44 | step: 190.12
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:08:13
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:08:13
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:08:13
is_deepspeed_enabledis_deepspeed_enabledis_deepspeed_enabled


isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:08:27,834] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step187 is about to be saved!
[2024-12-03 20:08:27,893] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-187/global_step187/mp_rank_00_model_states.pt
[2024-12-03 20:08:27,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-187/global_step187/mp_rank_00_model_states.pt...
[2024-12-03 20:08:39,562] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-187/global_step187/mp_rank_00_model_states.pt.
[2024-12-03 20:08:39,613] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-187/global_step187/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:09:07,354] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-187/global_step187/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:09:07,360] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-187/global_step187/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:09:07,360] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step187 is ready now!
should save
End of save checkpoint, device rank: 1, time: 2024-12-03 20:09:07End of save checkpoint, device rank: 2, time: 2024-12-03 20:09:07

End of save checkpoint, device rank: 0, time: 2024-12-03 20:09:07
[2024-12-03 20:09:08,849] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | optimizer_allgather: 30.47 | optimizer_gradients: 16.17 | optimizer_step: 66.24
[2024-12-03 20:09:08,850] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd_microstep: 530.86 | bwd_microstep: 670.56 | bwd_inner_microstep: 649.42 | bwd_allreduce_microstep: 21.09 | step_microstep: 210.61
[2024-12-03 20:09:08,850] [INFO] [logging.py:128:log_dist] [Rank 0] time (ms) | fwd: 530.83 | bwd: 670.57 | bwd_inner: 649.42 | bwd_allreduce: 21.10 | step: 210.61
Start of save checkpoint, device rank: 1, time: 2024-12-03 20:09:08
Start of save checkpoint, device rank: 2, time: 2024-12-03 20:09:08
Start of save checkpoint, device rank: 0, time: 2024-12-03 20:09:08
is_deepspeed_enabledis_deepspeed_enabled

is_deepspeed_enabled
isinstance(self.model, supported_classes)
Now save your training arguments together with the trained model
[2024-12-03 20:09:23,378] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step188 is about to be saved!
[2024-12-03 20:09:23,433] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bdof/nkanamarla/checkpoint-188/global_step188/mp_rank_00_model_states.pt
[2024-12-03 20:09:23,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-188/global_step188/mp_rank_00_model_states.pt...
[2024-12-03 20:09:35,301] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-188/global_step188/mp_rank_00_model_states.pt.
[2024-12-03 20:09:35,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bdof/nkanamarla/checkpoint-188/global_step188/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-12-03 20:10:03,301] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bdof/nkanamarla/checkpoint-188/global_step188/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-12-03 20:10:03,307] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /work/nvme/bdof/nkanamarla/checkpoint-188/global_step188/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-12-03 20:10:03,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step188 is ready now!
should saveEnd of save checkpoint, device rank: 1, time: 2024-12-03 20:10:03
End of save checkpoint, device rank: 2, time: 2024-12-03 20:10:03

End of save checkpoint, device rank: 0, time: 2024-12-03 20:10:03
{'eval_loss': 0.8060210943222046, 'eval_runtime': 5.9998, 'eval_samples_per_second': 83.336, 'eval_steps_per_second': 3.5, 'epoch': 1.0}
End of inner train loop, device rank: 2, time: 2024-12-03 20:10:09
End of inner train loop, device rank: 1, time: 2024-12-03 20:10:09
{'train_runtime': 9554.6678, 'train_samples_per_second': 0.471, 'train_steps_per_second': 0.02, 'train_loss': 2.3373245888567986, 'epoch': 1.0}
End of inner train loop, device rank: 0, time: 2024-12-03 20:10:09
[2024-12-03 20:10:32,857] [INFO] [launch.py:351:main] Process 3197434 exits successfully.
[2024-12-03 20:10:32,857] [INFO] [launch.py:351:main] Process 3197435 exits successfully.
[2024-12-03 20:10:32,857] [INFO] [launch.py:351:main] Process 3197436 exits successfully.
